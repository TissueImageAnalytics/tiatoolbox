# ***** BEGIN GPL LICENSE BLOCK *****
#
# This program is free software; you can redistribute it and/or
# modify it under the terms of the GNU General Public License
# as published by the Free Software Foundation; either version 2
# of the License, or (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, write to the Free Software Foundation,
# Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301, USA.
#
# The Original Code is Copyright (C) 2021, TIALab, University of Warwick
# All rights reserved.
# ***** END GPL LICENSE BLOCK *****

"""This module enables patch-level prediction."""

import numpy as np
from abc import abstractmethod

from tiatoolbox.models import abc as tia_model_abc


class IOStateSegmentor(tia_model_abc.IOStateBase):
    """Define a class to hold IO information for patch predictor."""

    # We predefine to follow enforcement, actual initialization in init
    input_resolutions = None
    output_resolutions = None

    def __init__(
            self,
            input_resolutions,
            output_resolutions,
            save_resolution=None,
            **kwargs):

        self.patch_input_shape = None
        self.patch_output_shape = None
        self.stride_shape = None
        self.input_resolutions = input_resolutions
        self.output_resolutions = output_resolutions

        # setting this to overwrite, such that all output head
        # will be saved under a same resolution
        self.save_resolution = save_resolution

        for variable, value in kwargs.items():
            self.__setattr__(variable, value)

    def _validate(self):
        """Validate the data format."""
        pass

    def convert_to_baseline(self):
        """Convert IO resolution to 'baseline'.

        This will permanently alter the object values.
        Actually return scale factor wrt highest resolution initially defined.
        """
        def _to_baseline(resolution_list):
            old_val = [v['resolution'] for v in resolution_list]
            if resolution_list[0]['units'] == 'baseline':
                new_val = old_val
            elif resolution_list[0]['units'] == 'mpp':
                new_val = np.array(old_val) / np.min(old_val)
            elif resolution_list[0]['units'] == 'power':
                new_val = np.array(old_val) / np.max(old_val)
            resolution_list = [
                {'units' : 'baseline', 'resolution' : v}
                for v in new_val]
            return resolution_list
        self.input_resolutions = _to_baseline(self.input_resolutions)
        self.output_resolutions = _to_baseline(self.output_resolutions)


class ModelBase(tia_model_abc.ModelBase):
    """ABC.
    """

    @property
    @abstractmethod
    def pre_proc(self):
        raise NotImplementedError

    @property
    @abstractmethod
    def post_proc(self):
        raise NotImplementedError

    @staticmethod
    def infer_batch(model, batch_data, on_gpu):
        """Run inference on an input batch. Contains logic for
        forward operation as well as i/o aggregation.

        Args:
            model (nn.Module): PyTorch defined model.
            # ! TODO: change this to object or sthg, as this only need
            # ! to be in the same API protocol as loader
            batch_data (ndarray): A batch of data generated by
                torch.utils.data.DataLoader.
            on_gpu (bool): Whether to run inference on a GPU.

        """
        raise NotImplementedError
