# ***** BEGIN GPL LICENSE BLOCK *****
#
# This program is free software; you can redistribute it and/or
# modify it under the terms of the GNU General Public License
# as published by the Free Software Foundation; either version 2
# of the License, or (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, write to the Free Software Foundation,
# Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301, USA.
#
# The Original Code is Copyright (C) 2021, TIALab, University of Warwick
# All rights reserved.
# ***** END GPL LICENSE BLOCK *****

"""This module enables patch-level prediction."""

import numpy as np
from abc import abstractmethod

from tiatoolbox.models import abc as tia_model_abc


class IOStateSegmentor(tia_model_abc.IOStateBase):
    """Define a class to hold IO information for patch predictor."""

    # We predefine to follow enforcement, actual initialization in init
    input_resolutions = None
    output_resolutions = None

    def __init__(
            self,
            input_resolutions,
            output_resolutions,
            save_resolution=None,
            **kwargs):
        """Define IO placement for patch input and output.

        Args:

            input_resolutions: resolution of each input head to model
                inference, must be in the same order as target model.forward().

            output_resolutions: resolution of each output head from model
                inference, must be in the same order as target model.infer_batch().

            save_resolution: resolution to save all output.

        """
        self.patch_input_shape = None
        self.patch_output_shape = None
        self.stride_shape = None
        self.input_resolutions = input_resolutions
        self.output_resolutions = output_resolutions

        self.resolution_unit = input_resolutions[0]['units']
        self.save_resolution = save_resolution

        for variable, value in kwargs.items():
            self.__setattr__(variable, value)

        self._validate()

        if self.resolution_unit == 'mpp':
            self.highest_input_resolution = min(
                self.input_resolutions, key=lambda x: x['resolution'])
        else:
            self.highest_input_resolution = max(
                self.input_resolutions, key=lambda x: x['resolution'])

    def _validate(self):
        """Validate the data format."""
        def validate_units(resolution_list):
            unit_list = [v['units'] for v in resolution_list]
            unit_list = np.unique(unit_list)
            if (len(unit_list) != 1 or unit_list[0] 
                    not in ['power',  'baseline', 'mpp', 'level']):
                raise ValueError('Invalid resolution units.')
        validate_units(self.input_resolutions + self.output_resolutions)

    def convert_to_baseline(self):
        """Convert IO resolution to 'baseline'.

        This will permanently alter the object values.
        Actually return scale factor wrt highest resolution initially defined.
        """
        def _to_baseline(resolution_list):
            old_val = [v['resolution'] for v in resolution_list]
            if self.resolution_unit == 'baseline':
                new_val = old_val
            elif self.resolution_unit == 'mpp':
                new_val = np.min(old_val) / np.array(old_val)
            elif self.resolution_unit == 'power':
                new_val = np.array(old_val) / np.max(old_val)
            resolution_list = [
                {'units' : 'baseline', 'resolution' : v}
                for v in new_val]
            return resolution_list
        self.input_resolutions = _to_baseline(self.input_resolutions)
        self.output_resolutions = _to_baseline(self.output_resolutions)


class ModelBase(tia_model_abc.ModelBase):
    """ABC.
    """

    @property
    @abstractmethod
    def pre_proc(self):
        raise NotImplementedError

    @property
    @abstractmethod
    def post_proc(self):
        raise NotImplementedError

    @staticmethod
    def infer_batch(model, batch_data, on_gpu):
        """Run inference on an input batch. Contains logic for
        forward operation as well as i/o aggregation.

        Args:
            model (nn.Module): PyTorch defined model.
            # ! TODO: change this to object or sthg, as this only need
            # ! to be in the same API protocol as loader
            batch_data (ndarray): A batch of data generated by
                torch.utils.data.DataLoader.
            on_gpu (bool): Whether to run inference on a GPU.

        """
        raise NotImplementedError
