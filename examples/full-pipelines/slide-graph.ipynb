{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9AnzE3yKLzZZ"
   },
   "source": [
    "# Slide Graph Full-pipeline Notebook\n",
    "\n",
    "Click to open in: \\[[GitHub](https://github.com/TissueImageAnalytics/tiatoolbox/blob/develop/examples/full-pipelines/slide-graph.ipynb)\\]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y6GGQ4xzLzZa"
   },
   "source": [
    "## About this notebook\n",
    "\n",
    "This notebook is computationally intensive. We advise users to run this notebook locally on a machine with GPUs.\n",
    "To run the notebook on your local machine, set up your Python environment, as explained in the\n",
    "[README](https://github.com/TIA-Lab/tiatoolbox/blob/master/README.md#install-python-package) file.\n",
    "You can edit the notebook during the session, for example substituting your own image files for the\n",
    "image files used in this demo. Experiment by changing the parameters of functions.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ysm7KUttLzZa"
   },
   "source": [
    "## Introduction\n",
    "\n",
    "This notebook is aimed at advanced users who are interested in using\n",
    "TIAToolbox as part of an experiment or larger project. Here we replicate\n",
    "the method in \"SlideGraph+: Whole Slide Image Level Graphs\n",
    "to Predict HER2Status in Breast Cancer\" by Lu et al. (2021) to generate a\n",
    "graph on the whole slide image level and directly predict a slide level label.\n",
    "Our task is to classify a whole slide image (WSI) as either\n",
    "[HER2](https://en.wikipedia.org/wiki/HER2/neu) negative or positive.\n",
    "For this work, we will use the\n",
    "[TCGA-BRCA](https://portal.gdc.cancer.gov/projects/TCGA-BRCA) dataset.\n",
    "\n",
    "Throughout this notebook we use modules from TIAToolbox to assist with\n",
    "common tasks including:\n",
    "\n",
    "- Patch extraction\n",
    "- Stain normalization\n",
    "- Cell segmentation & classification\n",
    "- Extraction of deep features\n",
    "\n",
    "> **Note**: Although the original paper was evaluated for HER2, the method itself\n",
    "> can be applied to other mutation predictions. We provide a pretrained model\n",
    "> for predicting [ER (Estrogen receptor)](https://en.wikipedia.org/wiki/Estrogen_receptor)\n",
    "> status [here (model weights)](https://tiatoolbox.dcs.warwick.ac.uk/models/slide_graph/deep-features/model.weights.pth)\n",
    "> and [here (model auxiliary)](https://tiatoolbox.dcs.warwick.ac.uk/models/slide_graph/deep-features/model.aux.dat).\n",
    "> You can get the pre-generated graphs [here](https://tiatoolbox.dcs.warwick.ac.uk/models/slide_graph/deep-features/graphs.zip)\n",
    "> and its node preprocessing model [here](https://tiatoolbox.dcs.warwick.ac.uk/models/slide_graph/deep-features/node_scaler.dat).\n",
    "> For predicting ER status, we use deep features coming from ResNet50 rather than\n",
    "> the cellular structure.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "n6ZHYP9nLzZa"
   },
   "source": [
    "%%bash\n",
    "pip install -U numpy\n",
    "pip install umap-learn ujson\n",
    "pip uninstall -y torch-scatter torch-sparse torch-geometric\n",
    "pip uninstall -y torch\n",
    "pip install torch==1.10.0+cu113 torchvision==0.11.1+cu113 torchaudio==0.10.0+cu113 -f https://download.pytorch.org/whl/cu113/torch_stable.html\n",
    "pip install torch-scatter -f https://data.pyg.org/whl/torch-1.10.0+cu113.html\n",
    "pip install torch-sparse -f https://data.pyg.org/whl/torch-1.10.0+cu113.html\n",
    "pip install torch-geometric"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2MMdT5lZLzZb"
   },
   "source": [
    "## Preparation: Imports, Helpers, & Data Split\n",
    "\n",
    "We begin by importing some libraries, defining some helper functions\n",
    "and defining the split of the dataset into train, validation, and test subsets.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cioQgJXeLzZb"
   },
   "source": [
    "### Import Libraries\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rO0dwq0nLzZb"
   },
   "source": [
    "\"\"\"Import modules required to run the Jupyter notebook.\"\"\"\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "# Clear logger to use tiatoolbox.logger\n",
    "import logging\n",
    "\n",
    "if logging.getLogger().hasHandlers():\n",
    "    logging.getLogger().handlers.clear()\n",
    "\n",
    "import copy\n",
    "import os\n",
    "import random\n",
    "import shutil\n",
    "import warnings\n",
    "from collections import OrderedDict\n",
    "from pathlib import Path\n",
    "from typing import TYPE_CHECKING, Callable\n",
    "\n",
    "# Third party imports\n",
    "import joblib\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn.functional as F  # noqa: N812\n",
    "\n",
    "# Use ujson as replacement for default json because it's faster for large JSON\n",
    "import ujson as json\n",
    "from shapely.geometry import box as shapely_box\n",
    "from shapely.strtree import STRtree\n",
    "from skimage.exposure import equalize_hist\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import LogisticRegression as PlattScaling\n",
    "from sklearn.metrics import average_precision_score as auprc_scorer\n",
    "from sklearn.metrics import roc_auc_score as auroc_scorer\n",
    "from sklearn.model_selection import StratifiedKFold, StratifiedShuffleSplit\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from torch import nn\n",
    "from torch.nn import BatchNorm1d, Linear, ReLU\n",
    "from torch.utils.data import Sampler\n",
    "from torch_geometric.data import Batch, Data, Dataset\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.nn import (\n",
    "    EdgeConv,\n",
    "    GINConv,\n",
    "    global_add_pool,\n",
    "    global_max_pool,\n",
    "    global_mean_pool,\n",
    ")\n",
    "from tqdm import tqdm\n",
    "\n",
    "from tiatoolbox import logger\n",
    "from tiatoolbox.data import stain_norm_target\n",
    "from tiatoolbox.models import (\n",
    "    DeepFeatureExtractor,\n",
    "    IOSegmentorConfig,\n",
    "    NucleusInstanceSegmentor,\n",
    ")\n",
    "from tiatoolbox.models.architecture.vanilla import CNNBackbone\n",
    "from tiatoolbox.tools.graph import SlideGraphConstructor\n",
    "from tiatoolbox.tools.patchextraction import PatchExtractor\n",
    "from tiatoolbox.tools.stainnorm import get_normalizer\n",
    "\n",
    "# ! save_yaml, save_as_json => need same name, need to factor out jsonify\n",
    "from tiatoolbox.utils.misc import download_data, save_as_json, select_device\n",
    "from tiatoolbox.utils.visualization import plot_graph\n",
    "from tiatoolbox.wsicore.wsireader import (\n",
    "    OpenSlideWSIReader,\n",
    "    Resolution,\n",
    "    Units,\n",
    "    WSIReader,\n",
    ")\n",
    "\n",
    "if TYPE_CHECKING:  # pragma: no cover\n",
    "    from collections.abc import Iterator\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "mpl.rcParams[\"figure.dpi\"] = 300  # for high resolution figure in notebook"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cy5iHoxILzZb"
   },
   "source": [
    "### GPU or CPU runtime\n",
    "\n",
    "Processes in this notebook can be accelerated by using a GPU. Therefore, whether you are running this notebook on your system or Colab, you need to check and specify if you are using GPU or CPU hardware acceleration. In Colab, you need to make sure that the runtime type is set to GPU in the *\"Runtime→Change runtime type→Hardware accelerator\"*. If you are *not* using GPU, consider changing the `ON_GPU` flag to `Flase` value, otherwise, some errors will be raised when running the following cells.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XsWt6m_8LzZc"
   },
   "source": [
    "ON_GPU = True  # Should be changed to False if no cuda-enabled GPU is available"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8v5aEjGrLzZc"
   },
   "source": [
    "### Helper Functions\n",
    "\n",
    "Here we define some helper functions that will be used throughout the notebook:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kIrg_JtcLzZc"
   },
   "source": [
    "def load_json(path: Path) -> dict | list | int | float | str:\n",
    "    \"\"\"Load JSON from a file path.\"\"\"\n",
    "    with path.open() as fptr:\n",
    "        return json.load(fptr)\n",
    "\n",
    "\n",
    "def rmdir(dir_path: Path) -> None:\n",
    "    \"\"\"Remove a directory.\"\"\"\n",
    "    if dir_path.is_dir():\n",
    "        shutil.rmtree(dir_path)\n",
    "\n",
    "\n",
    "def rm_n_mkdir(dir_path: Path) -> None:\n",
    "    \"\"\"Remove then re-create a directory.\"\"\"\n",
    "    if dir_path.is_dir():\n",
    "        shutil.rmtree(dir_path)\n",
    "    dir_path.mkdir(parents=True)\n",
    "\n",
    "\n",
    "def mkdir(dir_path: Path) -> None:\n",
    "    \"\"\"Create a directory if it does not exist.\"\"\"\n",
    "    if not dir_path.is_dir():\n",
    "        dir_path.mkdir(parents=True)\n",
    "\n",
    "\n",
    "def recur_find_ext(root_dir: Path, exts: list[str]) -> list[str]:\n",
    "    \"\"\"Recursively find files with an extension in `exts`.\n",
    "\n",
    "    This is much faster than glob if the folder\n",
    "    hierachy is complicated and contain > 1000 files.\n",
    "\n",
    "    Args:\n",
    "        root_dir (Path):\n",
    "            Root directory for searching.\n",
    "        exts (list):\n",
    "            List of extensions to match.\n",
    "\n",
    "    Returns:\n",
    "        List of full paths with matched extension in sorted order.\n",
    "\n",
    "    \"\"\"\n",
    "    assert isinstance(exts, list)  # noqa: S101\n",
    "    file_path_list = []\n",
    "    for cur_path, _dir_list, file_list in os.walk(root_dir):\n",
    "        for file_name in file_list:\n",
    "            file_ext = Path(file_name).suffix\n",
    "            if file_ext in exts:\n",
    "                full_path = cur_path / file_name\n",
    "                file_path_list.append(full_path)\n",
    "    file_path_list.sort()\n",
    "    return file_path_list"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1LAE3HarLzZc"
   },
   "source": [
    "### Loading The Dataset\n",
    "\n",
    "For this dataset (TCGA-BRCA), the HER2 status is provided per patient\n",
    "instead of per slide. Therefore, we assign the same label to all WSIs coming\n",
    "from the same patient. WSIs that do not have labels are excluded\n",
    "from subsequent processing.\n",
    "\n",
    "We begin this notebook with loading the data by doing the following:\n",
    "\n",
    "1. Load a list of WSIs and associated tissue masks (file paths).\n",
    "1. Convert the clinical infomation in `.csv` to labels.\n",
    "1. Assign the patient label to each WSI\n",
    "1. Filter out WSIs which do not have a label.\n",
    "\n",
    "We use the following global variables:\n",
    "\n",
    "- `CLINICAL_FILE`: The `.csv` file which contains the patient code and\n",
    "  the associated labels.\n",
    "- `ROOT_OUTPUT_DIR`: Root directory to save output under.\n",
    "- `WSI_DIR`: Directory containing WSIs.\n",
    "- `MSK_DIR`: Directory containing the corresponding WSI mask. If set to `None`,\n",
    "  the subsequent process will use the default method in the toolbox to obtain\n",
    "  the mask (via `WSIReader.tissue_mask`). Each mask file is assumed to be `.png`\n",
    "  and any non-zero pixels within it are considered for processing.\n",
    "\n",
    "By the end of this process, we obtain the following variables for subsequent operations\n",
    "\n",
    "- `wsi_paths`: A list of file paths to WSIs.\n",
    "- `wsi_names`: A list of WSI names in `wsi_paths`.\n",
    "- `msk_paths`: A list of paths pointing to masks of each\n",
    "  WSI in `wsi_paths`.\n",
    "- `label_df`: A panda dataframe containing two columns: `WSI-CODE` and `LABEL`. Each row in the dataframe is a pair, whose first entry is the name of a WSI in the list `wsi_names` and whose second entry is the label of that WSI.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e75ylslhLzZc"
   },
   "source": [
    "SEED = 5\n",
    "random.seed(SEED)\n",
    "rng = np.random.default_rng(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "i7Iq9kSBLzZc"
   },
   "source": [
    "# Set these variables to run next cell either\n",
    "# seperately or with customized parameters\n",
    "ROOT_OUTPUT_DIR = Path(\"PATH/TO/DIR/\")\n",
    "WSI_DIR = Path(\"PATH/TO/DIR/\")\n",
    "MSK_DIR = None\n",
    "CLINICAL_FILE = Path(\"PATH/TO/DIR/\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ua6cHD-1LzZc"
   },
   "source": [
    "# * Query for paths\n",
    "\n",
    "wsi_paths = recur_find_ext(WSI_DIR, [\".svs\", \".ndpi\"])\n",
    "wsi_names = [Path(v).stem for v in wsi_paths]\n",
    "msk_paths = None if MSK_DIR is None else [f\"{MSK_DIR}/{v}.png\" for v in wsi_names]\n",
    "assert len(wsi_paths) > 0, \"No files found.\"  # noqa: S101\n",
    "\n",
    "# * Generate WSI labels\n",
    "clinical_df = pd.read_csv(CLINICAL_FILE)\n",
    "patient_uids = clinical_df[\"PATIENT\"].to_numpy()\n",
    "patient_labels = clinical_df[\"HER2FinalStatus\"].to_numpy()\n",
    "\n",
    "patient_labels_ = np.full_like(patient_labels, -1)\n",
    "patient_labels_[patient_labels == \"Positive\"] = 1\n",
    "patient_labels_[patient_labels == \"Negative\"] = 0\n",
    "sel = patient_labels_ >= 0\n",
    "\n",
    "patient_uids = patient_uids[sel]\n",
    "patient_labels = patient_labels_[sel]\n",
    "assert len(patient_uids) == len(patient_labels)  # noqa: S101\n",
    "clinical_info = OrderedDict(list(zip(patient_uids, patient_labels)))\n",
    "\n",
    "# Retrieve patient code of each WSI, this is based on TCGA barcodes:\n",
    "# https://docs.gdc.cancer.gov/Encyclopedia/pages/TCGA_Barcode/\n",
    "wsi_patient_codes = np.array([\"-\".join(v.split(\"-\")[:3]) for v in wsi_names])\n",
    "wsi_labels = np.array(\n",
    "    [clinical_info.get(v, np.nan) for v in wsi_patient_codes],\n",
    ")\n",
    "\n",
    "# * Filter the WSIs and paths that do not have labels\n",
    "sel = ~np.isnan(wsi_labels)\n",
    "# Simple sanity checks before filtering\n",
    "assert len(wsi_paths) == len(wsi_names)  # noqa: S101\n",
    "assert len(wsi_paths) == len(wsi_labels)  # noqa: S101\n",
    "wsi_paths = np.array(wsi_paths)[sel]\n",
    "wsi_names = np.array(wsi_names)[sel]\n",
    "wsi_labels = np.array(wsi_labels)[sel]\n",
    "\n",
    "label_df = list(zip(wsi_names, wsi_labels))\n",
    "label_df = pd.DataFrame(label_df, columns=[\"WSI-CODE\", \"LABEL\"])"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7zIRsykvLzZc"
   },
   "source": [
    "### Generate the Data Split\n",
    "\n",
    "Now, we split our dataset into disjoint train, validation, and test subsets.\n",
    "\n",
    "To that end, we define a new function called `stratified_split`.\n",
    "It receives:\n",
    "\n",
    "- paired input of the samples and their labels\n",
    "- the train, valid, and test percentages\n",
    "\n",
    "and then returns a number of stratified splits.\n",
    "\n",
    "**Stratification** means that, for each label, the proportion of samples with that label is as similar as possible in each of the three splits. Stratification ensures that any bias that might result from a particular label operates as equally as possible in each split. This is a standard way of avoiding bias due to possible confounding factors â€“ here each label is regarded as a possible confounding factor.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VmXGp1aLLzZc"
   },
   "source": [
    "def stratified_split(\n",
    "    x: list,\n",
    "    y: list,\n",
    "    train: float,\n",
    "    valid: float,\n",
    "    test: float,\n",
    "    num_folds: int,\n",
    "    seed: int = 5,\n",
    ") -> list:\n",
    "    \"\"\"Helper to generate stratified splits.\n",
    "\n",
    "    Split `x` and `y` in to N number of `num_folds` sets\n",
    "    of `train`, `valid`, and `test` set in stratified manner.\n",
    "    `train`, `valid`, and `test` are guaranteed to be mutually\n",
    "    exclusive.\n",
    "\n",
    "    Args:\n",
    "        x (list, np.ndarray):\n",
    "            List of samples.\n",
    "        y (list, np.ndarray):\n",
    "            List of labels, each value is the value\n",
    "            of the sample at the same index in `x`.\n",
    "        train (float):\n",
    "            Percentage to be used for training set.\n",
    "        valid (float):\n",
    "            Percentage to be used for validation set.\n",
    "        test (float):\n",
    "            Percentage to be used for testing set.\n",
    "        num_folds (int):\n",
    "            Number of split generated.\n",
    "        seed (int):\n",
    "            Random seed. Default=5.\n",
    "\n",
    "    Returns:\n",
    "        A list of splits where each is a dictionary of\n",
    "        {\n",
    "            'train': [(sample_A, label_A), (sample_B, label_B), ...],\n",
    "            'valid': [(sample_C, label_C), (sample_D, label_D), ...],\n",
    "            'test' : [(sample_E, label_E), (sample_E, label_E), ...],\n",
    "        }\n",
    "\n",
    "    \"\"\"\n",
    "    assert (  # noqa: S101\n",
    "        train + valid + test - 1.0 < 1.0e-10  # noqa: PLR2004\n",
    "    ), \"Ratios must sum to 1.0 .\"\n",
    "\n",
    "    outer_splitter = StratifiedShuffleSplit(\n",
    "        n_splits=num_folds,\n",
    "        train_size=train + valid,\n",
    "        random_state=seed,\n",
    "    )\n",
    "    inner_splitter = StratifiedShuffleSplit(\n",
    "        n_splits=1,\n",
    "        train_size=train / (train + valid),\n",
    "        random_state=seed,\n",
    "    )\n",
    "\n",
    "    x = np.array(x)\n",
    "    y = np.array(y)\n",
    "    splits = []\n",
    "    for train_valid_idx, test_idx in outer_splitter.split(x, y):\n",
    "        test_x = x[test_idx]\n",
    "        test_y = y[test_idx]\n",
    "\n",
    "        # Holder for train_valid set\n",
    "        x_ = x[train_valid_idx]\n",
    "        y_ = y[train_valid_idx]\n",
    "\n",
    "        # Split train_valid into train and valid set\n",
    "        train_idx, valid_idx = next(iter(inner_splitter.split(x_, y_)))\n",
    "        valid_x = x_[valid_idx]\n",
    "        valid_y = y_[valid_idx]\n",
    "\n",
    "        train_x = x_[train_idx]\n",
    "        train_y = y_[train_idx]\n",
    "\n",
    "        # Integrity check\n",
    "        assert len(set(train_x).intersection(set(valid_x))) == 0  # noqa: S101\n",
    "        assert len(set(valid_x).intersection(set(test_x))) == 0  # noqa: S101\n",
    "        assert len(set(train_x).intersection(set(test_x))) == 0  # noqa: S101\n",
    "\n",
    "        splits.append(\n",
    "            {\n",
    "                \"train\": list(zip(train_x, train_y)),\n",
    "                \"valid\": list(zip(valid_x, valid_y)),\n",
    "                \"test\": list(zip(test_x, test_y)),\n",
    "            },\n",
    "        )\n",
    "    return splits"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yoeuhlTrLzZc"
   },
   "source": [
    "Now, we split the data with given ratio.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ya3jpGRFLzZd"
   },
   "source": [
    "CACHE_PATH = None\n",
    "SPLIT_PATH = ROOT_OUTPUT_DIR / \"splits.dat\"\n",
    "\n",
    "NUM_FOLDS = 5\n",
    "TEST_RATIO = 0.2\n",
    "TRAIN_RATIO = 0.8 * 0.9\n",
    "VALID_RATIO = 0.8 * 0.1\n",
    "\n",
    "if CACHE_PATH and CACHE_PATH.exists():\n",
    "    splits = joblib.load(CACHE_PATH)\n",
    "    SPLIT_PATH = CACHE_PATH\n",
    "else:\n",
    "    x = np.array(label_df[\"WSI-CODE\"].to_list())\n",
    "    y = np.array(label_df[\"LABEL\"].to_list())\n",
    "    splits = stratified_split(x, y, TRAIN_RATIO, VALID_RATIO, TEST_RATIO, NUM_FOLDS)\n",
    "\n",
    "    joblib.dump(splits, SPLIT_PATH)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zVMiXHzqLzZd"
   },
   "source": [
    "## Generating Graphs from WSIs\n",
    "\n",
    "> **Note**: If you do not want to construct the graphs and only want to try out the graph neural\n",
    "> network portion, we provide pre-generated graphs based on cell-composition features extracted by\n",
    "> HoVer-Net at this [link](https://tiatoolbox.dcs.warwick.ac.uk/models/slide_graph/cell-composition/graphs.zip).\n",
    "> After downloading and extracting them, please follow subsequent instructions.\n",
    "\n",
    "Now that we have defined our sources of data, we move on to transforming\n",
    "them into a more usable form. We represent each WSI as a graph. Each node\n",
    "in the graph corresponds to one local region (such as an image patch) within\n",
    "the WSI and is then represented by a set of features. Here, we show\n",
    "two alternative feature representations:\n",
    "\n",
    "- Deep Neural Network features: obtained from the global average\n",
    "  pooling layer after we apply ResNet50 on the patch.\n",
    "- Cellular composition: where we count the number of nuclei of each type within\n",
    "  the patch. A pre-trained model (HoVer-Net trained on Pannuke) from the toolbox\n",
    "  provides the following nucleus types: neoplastic, non-neoplastic epithelial,\n",
    "  inflammatory, connective tissue and necrotic.\n",
    "\n",
    "With these node-level representations (or features), we then perform clustering\n",
    "so that nodes that are close to each other both in feature space and in 2D\n",
    "space (i.e the WSI canvas) are assigned to the same cluster. These clusters are\n",
    "then linked to other clusters within a certain distance, thus giving a WSI graph.\n",
    "\n",
    "**Note**: Features of patches and theirs positions within each WSI\n",
    "will be stored separately in files named `*.features.npy` and `*.position.npy` .\n",
    "The `position` of a feature is, by definition, the patch bounding box\n",
    "`(start_x, start_y, end_x, end_y)` at the highest resolution.\n",
    "Subsequent function definitions will be based on this convention.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BL5h_KkqLzZd"
   },
   "source": [
    "### Deep Feature Extraction\n",
    "\n",
    "We now show how to use the toolbox to extract features. We package\n",
    "it into a small function called `extract_deep_features` for better organization.\n",
    "\n",
    "In this function, we define the config object which defines the shape and magnification of the\n",
    "patch we want to extract. Although the patches are allowed to have arbitrary size and differing\n",
    "resolutions, here we use a patch of size 512x512 with 0.25 microns-per-pixel (`mpp=0.25`).\n",
    "We use ResNet50 trained on ImageNet as a feature extractor. For more\n",
    "detail on how to further customize this, refer to [this notebook](https://github.com/TissueImageAnalytics/tiatoolbox/blob/develop/examples/07_example_modeltechniques.ipynb).\n",
    "\n",
    "We explain how to construct a customized preprocessing function\n",
    "that we would like the `engine` to perform on each input patch.\n",
    "(`engine` is a set of classes defined under `tiatoolbox.models.engine`.\n",
    "Each instance of these classes has multiple properties and abilities,\n",
    "possibly incorporating several functions.) For this notebook, we perform\n",
    "stain-normalization on each image patch. We show how this function can\n",
    "be defined later.\n",
    "\n",
    "By default, the names of output files from the toolbox are\n",
    "changed to sequentially ordered names (`000.*.npy`, `001.*.npy`, etc.) to avoid\n",
    "inadvertent overwriting. A mapping from output path name to input path name is returned by the engine,\n",
    "making the name change easy to manage.\n",
    "\n",
    "In this demo, we use a toolbox model with only one head (output channel).\n",
    "For each input, we will have `*.position.npy` and `*.features.0.npy`.\n",
    "In the case of models having multiple output heads (output channels), the output is\n",
    "`['*.position.npy', '*.features.0.npy', '*.features.1.npy', etc.]` . The positions are always\n",
    "defined as the patch bounding box `(start_x, start_y, end_x, end_y)` at the highest resolution\n",
    "within the list of input resolutions. Refer to the [semantic segmentation notebook](https://github.com/TissueImageAnalytics/tiatoolbox/blob/develop/examples/06_example_semanticsegmentation.ipynb)\n",
    "for details.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EHFqqFXmLzZd"
   },
   "source": [
    "def extract_deep_features(\n",
    "    wsi_paths: list[str],\n",
    "    msk_paths: list[str],\n",
    "    save_dir: str,\n",
    "    preproc_func: Callable | None = None,\n",
    ") -> list:\n",
    "    \"\"\"Helper function to extract deep features.\"\"\"\n",
    "    ioconfig = IOSegmentorConfig(\n",
    "        input_resolutions=[\n",
    "            {\"units\": \"mpp\", \"resolution\": 0.25},\n",
    "        ],\n",
    "        output_resolutions=[\n",
    "            {\"units\": \"mpp\", \"resolution\": 0.25},\n",
    "        ],\n",
    "        patch_input_shape=[512, 512],\n",
    "        patch_output_shape=[512, 512],\n",
    "        stride_shape=[512, 512],\n",
    "        save_resolution={\"units\": \"mpp\", \"resolution\": 8.0},\n",
    "    )\n",
    "    model = CNNBackbone(\"resnet50\")\n",
    "    extractor = DeepFeatureExtractor(batch_size=16, model=model, num_loader_workers=4)\n",
    "    # Injecting customized preprocessing functions,\n",
    "    # check the document or sample code below for API.\n",
    "    extractor.model.preproc_func = preproc_func\n",
    "\n",
    "    rmdir(save_dir)\n",
    "    output_map_list = extractor.predict(\n",
    "        wsi_paths,\n",
    "        msk_paths,\n",
    "        mode=\"wsi\",\n",
    "        ioconfig=ioconfig,\n",
    "        on_gpu=ON_GPU,\n",
    "        crash_on_exception=True,\n",
    "        save_dir=save_dir,\n",
    "    )\n",
    "\n",
    "    # Rename output files\n",
    "    for input_path, output_path in output_map_list:\n",
    "        input_name = Path(input_path).stem\n",
    "\n",
    "        output_parent_dir = Path(output_path).parent\n",
    "\n",
    "        src_path = Path(f\"{output_path}.position.npy\")\n",
    "        new_path = Path(f\"{output_parent_dir}/{input_name}.position.npy\")\n",
    "        src_path.rename(new_path)\n",
    "\n",
    "        src_path = Path(f\"{output_path}.features.0.npy\")\n",
    "        new_path = Path(f\"{output_parent_dir}/{input_name}.features.npy\")\n",
    "        src_path.rename(new_path)\n",
    "\n",
    "    return output_map_list"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-L_u7I-FLzZd"
   },
   "source": [
    "### Cell Composition Extraction\n",
    "\n",
    "In a similar manner, we define the code to extract cell\n",
    "composition in `extract_composition_features`. First, we need to\n",
    "detect all the nuclei in the WSI and their types. This can be\n",
    "easily achieved via the `tiatoolbox.models.NucleusInstanceSegmentor`\n",
    "engine and the HoVer-Net pretrained model, both provided in the toolbox.\n",
    "Once we have the nuclei, we split the WSI into patches\n",
    "and count the nuclei of each type in each patch.\n",
    "We encapsulate this process in the function `get_composition_features`.\n",
    "\n",
    "Unlike the `DeepFeatureExtractor` above, the `NucleusInstanceSegmentor` engine\n",
    "returns a single output file when given a single WSI input. Their corresponding\n",
    "output files are named as `['*/0.dat', '*/1.dat', etc.]` and we need to rename\n",
    "them accordingly. We generate the cell composition features from each\n",
    "of these files. The information related to each file is saved in two files with names\n",
    "`*.features.npy` and `*.position.npy`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b61PdMHiLzZd"
   },
   "source": [
    "def get_cell_compositions(\n",
    "    wsi_path: str,\n",
    "    mask_path: str,\n",
    "    inst_pred_path: str,\n",
    "    save_dir: str,\n",
    "    num_types: int = 6,\n",
    "    patch_input_shape: tuple[int] = (512, 512),\n",
    "    stride_shape: tuple[int] = (512, 512),\n",
    "    resolution: Resolution = 0.25,\n",
    "    units: Units = \"mpp\",\n",
    ") -> None:\n",
    "    \"\"\"Estimates cellular composition.\"\"\"\n",
    "    reader = WSIReader.open(wsi_path)\n",
    "    inst_pred = joblib.load(inst_pred_path)\n",
    "    # Convert to {key: int, value: dict}\n",
    "    inst_pred = {i: v for i, (_, v) in enumerate(inst_pred.items())}\n",
    "\n",
    "    inst_boxes = [v[\"box\"] for v in inst_pred.values()]\n",
    "    inst_boxes = np.array(inst_boxes)\n",
    "\n",
    "    geometries = [shapely_box(*bounds) for bounds in inst_boxes]\n",
    "    spatial_indexer = STRtree(geometries)\n",
    "\n",
    "    # * Generate patch coordinates (in xy format)\n",
    "    wsi_shape = reader.slide_dimensions(resolution=resolution, units=units)\n",
    "\n",
    "    (patch_inputs, _) = PatchExtractor.get_coordinates(\n",
    "        image_shape=wsi_shape,\n",
    "        patch_input_shape=patch_input_shape,\n",
    "        patch_output_shape=patch_input_shape,\n",
    "        stride_shape=stride_shape,\n",
    "    )\n",
    "\n",
    "    # filter out coords which dont lie in mask\n",
    "    selected_coord_indices = PatchExtractor.filter_coordinates(\n",
    "        WSIReader.open(mask_path),\n",
    "        patch_inputs,\n",
    "        wsi_shape=wsi_shape,\n",
    "        min_mask_ratio=0.5,\n",
    "    )\n",
    "    patch_inputs = patch_inputs[selected_coord_indices]\n",
    "\n",
    "    bounds_compositions = []\n",
    "    for bounds in patch_inputs:\n",
    "        bounds_ = shapely_box(*bounds)\n",
    "        indices = [\n",
    "            geo\n",
    "            for geo in spatial_indexer.query(bounds_)\n",
    "            if bounds_.contains(geometries[geo])\n",
    "        ]\n",
    "        insts = [inst_pred[v][\"type\"] for v in indices]\n",
    "        uids, freqs = np.unique(insts, return_counts=True)\n",
    "        # A bound may not contain all types, hence, to sync\n",
    "        # the array and placement across all types, we create\n",
    "        # a holder then fill the count within.\n",
    "        holder = np.zeros(num_types, dtype=np.int16)\n",
    "        holder[uids.astype(int)] = freqs\n",
    "        bounds_compositions.append(holder)\n",
    "    bounds_compositions = np.array(bounds_compositions)\n",
    "\n",
    "    base_name = Path(wsi_path).stem\n",
    "    # Output in the same saving protocol for construct graph\n",
    "    np.save(f\"{save_dir}/{base_name}.position.npy\", patch_inputs)\n",
    "    np.save(f\"{save_dir}/{base_name}.features.npy\", bounds_compositions)\n",
    "\n",
    "\n",
    "def extract_composition_features(\n",
    "    wsi_paths: list[str],\n",
    "    msk_paths: list[str],\n",
    "    save_dir: str,\n",
    "    preproc_func: Callable,\n",
    ") -> list:\n",
    "    \"\"\"Extract cellular composition features.\"\"\"\n",
    "    inst_segmentor = NucleusInstanceSegmentor(\n",
    "        pretrained_model=\"hovernet_fast-pannuke\",\n",
    "        batch_size=16,\n",
    "        num_postproc_workers=4,\n",
    "        num_loader_workers=4,\n",
    "    )\n",
    "    # bigger tile shape for postprocessing performance\n",
    "    inst_segmentor.ioconfig.tile_shape = (4000, 4000)\n",
    "    # Injecting customized preprocessing functions,\n",
    "    # check the document or sample codes below for API\n",
    "    inst_segmentor.model.preproc_func = preproc_func\n",
    "\n",
    "    rmdir(save_dir)\n",
    "    output_map_list = inst_segmentor.predict(\n",
    "        wsi_paths,\n",
    "        msk_paths,\n",
    "        mode=\"wsi\",\n",
    "        on_gpu=ON_GPU,\n",
    "        crash_on_exception=True,\n",
    "        save_dir=save_dir,\n",
    "    )\n",
    "    # Rename output files of toolbox\n",
    "    output_paths = []\n",
    "    for input_path, output_path in output_map_list:\n",
    "        input_name = Path(input_path).stem\n",
    "\n",
    "        output_parent_dir = Path(output_path).parent\n",
    "\n",
    "        src_path = Path(f\"{output_path}.dat\")\n",
    "        new_path = Path(f\"{output_parent_dir}/{input_name}.dat\")\n",
    "        src_path.rename(new_path)\n",
    "        output_paths.append(new_path)\n",
    "\n",
    "    # TODO(TBC): Parallelize this if possible  # noqa: TD003, FIX002\n",
    "    for idx, path in enumerate(output_paths):\n",
    "        get_cell_compositions(wsi_paths[idx], msk_paths[idx], path, save_dir)\n",
    "    return output_paths"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9hGqICegLzZd"
   },
   "source": [
    "### Apply Stain Normalization Across Image Patches\n",
    "\n",
    "Extracting either deep features or cell compositions above requires\n",
    "inference on each patch within the WSI. In histopathology, we often want to\n",
    "normalize the image patch staining to reduce variation as much as possible.\n",
    "\n",
    "Here we define the normalizer and a function to perform normalization later\n",
    "in parallel processing manner. The target image and the normalizer are\n",
    "provided at `tiatoolbox.tools.stainnorm` and `tiatoolbox.data`.\n",
    "\n",
    "We do not perform stain normalization at this point in the program. Instead, we\n",
    "stain-normalize in tandem with other methods in the toolbox during pre-processing.\n",
    "In our case, this will be done by the `engine` object defined above.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5VW-z719LzZd"
   },
   "source": [
    "target_image = stain_norm_target()\n",
    "stain_normalizer = get_normalizer(\"vahadane\")\n",
    "stain_normalizer.fit(target_image)\n",
    "\n",
    "\n",
    "def stain_norm_func(img: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Helper function to perform stain normalization.\"\"\"\n",
    "    return stain_normalizer.transform(img)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OUxanBpLLzZd"
   },
   "source": [
    "Above, we have already defined functions that can perform WSI feature extraction.\n",
    "Now we perform the extraction itself. We avoid computationally expensive re-extraction\n",
    "of WSI features. We distinguish two use cases via the `CACHE_PATH` variable;\n",
    "if `CACHE_PATH = None`, then extraction is performed and the results are saved\n",
    "in `WSI_FEATURE_DIR`. For ease of organization, we set by default\n",
    "`WSI_FEATURE_DIR = f'{ROOT_OUTPUT_DIR}/features/'`. Otherwise, the paths\n",
    "to feature files are queried.\n",
    "\n",
    "The `FEATURE_MODE` variable dictates which patch features will be extracted. Currently,\n",
    "we support two alternatives:\n",
    "\n",
    "- `\"cnn\"` : for the deep neural network features. We use ResNet50 pretrained on ImageNet\n",
    "  as feature extractor. Therefore, there are 2048 features representing each image patch.\n",
    "- `\"composition\"` : for the cell composition features. The features here are\n",
    "  the counts of each nucleus type within the image. We use the HoVer-Net\n",
    "  pretrained on Pannuke data to identify 6 nuclei types: neoplastic epithelial, lymphocytes,\n",
    "  connective tissue, necrosis, and non-neoplastic epithelial.\n",
    "\n",
    "We use an assertion check at the end to ensure that we have\n",
    "the same number of output files as samples.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PBHEFCgpLzZd"
   },
   "source": [
    "NUM_NODE_FEATURES = 2048\n",
    "FEATURE_MODE = \"cnn\"\n",
    "CACHE_PATH = None\n",
    "WSI_FEATURE_DIR = f\"{ROOT_OUTPUT_DIR}/features/\""
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Xs-wS9fQLzZd"
   },
   "source": [
    "# Uncomment and set these variables to run the next cell,\n",
    "# either separately or with customized parameters"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zAgpE82TLzZd"
   },
   "source": [
    "if CACHE_PATH and CACHE_PATH.exists():\n",
    "    output_list = recur_find_ext(f\"{CACHE_PATH}/\", [\".npy\"])\n",
    "elif FEATURE_MODE == \"composition\":\n",
    "    output_list = extract_composition_features(\n",
    "        wsi_paths,\n",
    "        msk_paths,\n",
    "        WSI_FEATURE_DIR,\n",
    "        stain_norm_func,\n",
    "    )\n",
    "else:\n",
    "    output_list = extract_deep_features(\n",
    "        wsi_paths,\n",
    "        msk_paths,\n",
    "        WSI_FEATURE_DIR,\n",
    "        stain_norm_func,\n",
    "    )"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FxY0hS6tLzZd"
   },
   "source": [
    "### Constructing the Graphs\n",
    "\n",
    "Finally, with patches and their features loaded,\n",
    "we construct a graph for each WSI using the function provided in `tiatoolbox.tools.graph`.\n",
    "Again, if the graph has already been constructed, we avoid re-doing the work by\n",
    "setting `CACHE_PATH` appropriately.\n",
    "\n",
    "> **Note**: In this notebook, each node of the graph represents a patch.\n",
    "> However, if you prefer, you can provide your own version of nodes and their features.\n",
    "> You will need to modify the lines\n",
    ">\n",
    "> ```python\n",
    "> positions = np.load(f\"{WSI_FEATURE_DIR}/{wsi_name}.position.npy\")\n",
    "> features = np.load(f\"{WSI_FEATURE_DIR}/{wsi_name}.features.npy\")\n",
    "> ```\n",
    ">\n",
    "> within `construct_graph` to fit with your objectives.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6j-Sx361LzZd"
   },
   "source": [
    "CACHE_PATH = None\n",
    "GRAPH_DIR = ROOT_OUTPUT_DIR / \"graph\""
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MKubj2ONLzZd"
   },
   "source": [
    "# Uncomment and set these variables to run the next cell,\n",
    "# either separately or with customized parameters"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ibdDDkcjLzZd"
   },
   "source": [
    "def construct_graph(wsi_name: str, save_path: Path) -> None:\n",
    "    \"\"\"Construct graph for one WSI and save to file.\"\"\"\n",
    "    positions = np.load(f\"{WSI_FEATURE_DIR}/{wsi_name}.position.npy\")\n",
    "    features = np.load(f\"{WSI_FEATURE_DIR}/{wsi_name}.features.npy\")\n",
    "    graph_dict = SlideGraphConstructor.build(\n",
    "        positions[:, :2],\n",
    "        features,\n",
    "        feature_range_thresh=None,\n",
    "    )\n",
    "\n",
    "    # Write a graph to a JSON file\n",
    "    with save_path.open(\"w\") as handle:\n",
    "        graph_dict = {k: v.tolist() for k, v in graph_dict.items()}\n",
    "        json.dump(graph_dict, handle)\n",
    "\n",
    "\n",
    "if CACHE_PATH and CACHE_PATH.exists():\n",
    "    GRAPH_DIR = CACHE_PATH  # assignment for follow up loading\n",
    "    graph_paths = recur_find_ext(f\"{CACHE_PATH}/\", [\".json\"])\n",
    "else:\n",
    "    rm_n_mkdir(GRAPH_DIR)\n",
    "    graph_paths = [construct_graph(v, f\"{GRAPH_DIR}/{v}.json\") for v in wsi_names]\n",
    "# ! put the assertion back later"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RcRGO6zILzZd"
   },
   "source": [
    "### Visualize a Sample Graph\n",
    "\n",
    "It is always a good practice to visually validate data or any results.\n",
    "Here, we plot one sample graph upon its WSI thumbnail. For illustration purpose,\n",
    "by default we download and plot a sample WSI and its previously generated graph.\n",
    "In order to plot your own WSI or a graph, obtained by running the graph construction code above,\n",
    "you need to comment and uncomment some specific cells below. For more instruction, please read the first\n",
    "comment within each cell.\n",
    "\n",
    "Aside from that, most of the time the nodes within the graph will be at different resolutions\n",
    "from the resolution at which we want to visualize them. Hence, before plotting, we scale their coordinates\n",
    "to the target resolution. We provide `NODE_RESOLUTION` and `PLOT_RESOLUTION` variables\n",
    "respectively as the resolution of the node and the resolution at which to plot the graph.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UU_cxgNyLzZe",
    "outputId": "1167ab59-6c1d-4b4e-8b21-65c04af1c721"
   },
   "source": [
    "# By default, we download then visualize a sample WSI and its graph\n",
    "DOWNLOAD_DIR = \"local/dump/\"\n",
    "wsi_path = f\"{DOWNLOAD_DIR}/sample.svs\"\n",
    "graph_path = f\"{DOWNLOAD_DIR}/graph.json\"\n",
    "mkdir(DOWNLOAD_DIR)\n",
    "\n",
    "# Downloading sample image tile\n",
    "URL_HOME = \"https://tiatoolbox.dcs.warwick.ac.uk/models/slide_graph/cell-composition\"\n",
    "download_data(\n",
    "    f\"{URL_HOME}/TCGA-C8-A278-01Z-00-DX1.188B3FE0-7B20-401A-A6B7-8F1798018162.svs\",\n",
    "    wsi_path,\n",
    ")\n",
    "download_data(\n",
    "    f\"{URL_HOME}/TCGA-C8-A278-01Z-00-DX1.188B3FE0-7B20-401A-A6B7-8F1798018162.json\",\n",
    "    graph_path,\n",
    ")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yah2WCd6LzZe"
   },
   "source": [
    "# Uncomment to run later cells to visualize the first WSI within the dataset"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EcYOSaohLzZe"
   },
   "source": [
    "# Uncomment and set these variables to run the next cell,\n",
    "# either separately or with customized parameters\n",
    "# wsi_path = 'PATH"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IcKifok6LzZe"
   },
   "source": [
    "NODE_SIZE = 24\n",
    "NODE_RESOLUTION = {\"resolution\": 0.25, \"units\": \"mpp\"}\n",
    "PLOT_RESOLUTION = {\"resolution\": 4.0, \"units\": \"mpp\"}"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1v9E0mzvLzZh",
    "outputId": "f9a529eb-dc1a-4424-d0aa-65ceb4cb685b"
   },
   "source": [
    "graph_dict = load_json(graph_path)\n",
    "graph_dict = {k: np.array(v) for k, v in graph_dict.items()}\n",
    "graph = Data(**graph_dict)\n",
    "\n",
    "# deriving node colors via projecting n-d features down to 3-d\n",
    "graph.x = StandardScaler().fit_transform(graph.x)\n",
    "# .c for node colors\n",
    "node_colors = PCA(n_components=3).fit_transform(graph.x)[:, [1, 0, 2]]\n",
    "for channel in range(node_colors.shape[-1]):\n",
    "    node_colors[:, channel] = 1 - equalize_hist(node_colors[:, channel]) ** 2\n",
    "node_colors = (node_colors * 255).astype(np.uint8)\n",
    "\n",
    "reader = WSIReader.open(wsi_path)\n",
    "thumb = reader.slide_thumbnail(4.0, \"mpp\")\n",
    "\n",
    "node_resolution = reader.slide_dimensions(**NODE_RESOLUTION)\n",
    "plot_resolution = reader.slide_dimensions(**PLOT_RESOLUTION)\n",
    "fx = np.array(node_resolution) / np.array(plot_resolution)\n",
    "\n",
    "node_coordinates = np.array(graph.coordinates) / fx\n",
    "edges = graph.edge_index.T\n",
    "\n",
    "thumb = reader.slide_thumbnail(**PLOT_RESOLUTION)\n",
    "thumb_overlaid = plot_graph(\n",
    "    thumb.copy(),\n",
    "    node_coordinates,\n",
    "    edges,\n",
    "    node_colors=node_colors,\n",
    "    node_size=NODE_SIZE,\n",
    ")\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(thumb)\n",
    "plt.axis(\"off\")\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.imshow(thumb_overlaid)\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eUFNdva_LzZh"
   },
   "source": [
    "## The Graph Neural Network\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qSCeekI6LzZh"
   },
   "source": [
    "### The Dataset Loader\n",
    "\n",
    "At the time of writing this, graph datasets were not yet supported by TIAToolbox.\n",
    "We therefore defined here their loading and IO conversion. The goal of this dataset\n",
    "class is to support loading the input concurrently, and separately from the running\n",
    "GPU process. The class performs data conversion and other preprocessing if necessary.\n",
    "The `preproc` argument below is available to specify the function that normalizes node\n",
    "features.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mh2hiSJELzZh"
   },
   "source": [
    "class SlideGraphDataset(Dataset):\n",
    "    \"\"\"Handling loading graph data from disk.\n",
    "\n",
    "    Args:\n",
    "        info_list (list): In case of `train` or `valid` is in `mode`,\n",
    "            this is expected to be a list of `[uid, label]` . Otherwise,\n",
    "            it is a list of `uid`. Here, `uid` is used to construct\n",
    "            `f\"{GRAPH_DIR}/{wsi_code}.json\"` which is a path points to\n",
    "            a `.json` file containing the graph structure. By `label`, we mean\n",
    "            the label of the graph. The format within the `.json` file comes\n",
    "            from `tiatoolbox.tools.graph`.\n",
    "        mode (str): This denotes which data mode the `info_list` is in.\n",
    "        preproc (callable): The prerocessing function for each node\n",
    "            within the graph.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self: Dataset,\n",
    "        info_list: list,\n",
    "        mode: str = \"train\",\n",
    "        preproc: Callable | None = None,\n",
    "    ) -> None:\n",
    "        \"\"\"Initialize SlideGraphDataset.\"\"\"\n",
    "        self.info_list = info_list\n",
    "        self.mode = mode\n",
    "        self.preproc = preproc\n",
    "\n",
    "    def __getitem__(self: Dataset, idx: int) -> Dataset:\n",
    "        \"\"\"Get an element from SlideGraphDataset.\"\"\"\n",
    "        info = self.info_list[idx]\n",
    "        if any(v in self.mode for v in [\"train\", \"valid\"]):\n",
    "            wsi_code, label = info\n",
    "            # torch.Tensor will create 1-d vector not scalar\n",
    "            label = torch.tensor(label)\n",
    "        else:\n",
    "            wsi_code = info\n",
    "\n",
    "        with (GRAPH_DIR / str(wsi_code) + \".json\").open() as fptr:\n",
    "            graph_dict = json.load(fptr)\n",
    "        graph_dict = {k: np.array(v) for k, v in graph_dict.items()}\n",
    "\n",
    "        if self.preproc is not None:\n",
    "            graph_dict[\"x\"] = self.preproc(graph_dict[\"x\"])\n",
    "\n",
    "        graph_dict = {k: torch.tensor(v) for k, v in graph_dict.items()}\n",
    "        graph = Data(**graph_dict)\n",
    "\n",
    "        if any(v in self.mode for v in [\"train\", \"valid\"]):\n",
    "            return {\"graph\": graph, \"label\": label}\n",
    "        return {\"graph\": graph}\n",
    "\n",
    "    def __len__(self: Dataset) -> int:\n",
    "        \"\"\"Length of SlideGraphDataset.\"\"\"\n",
    "        return len(self.info_list)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SiAXnaduLzZi"
   },
   "source": [
    "### Entire Dataset Feature Normalization\n",
    "\n",
    "We define the feature normalizer, following the approach used for the stain normalizer.\n",
    "Since this normalization is derived from the entire dataset population, we first load\n",
    "all the node features from all the graphs within our dataset in order to train the normalizer.\n",
    "\n",
    "To avoid redundancy, we can skip this training step and use an existing normalizer by\n",
    "setting `CACHE_PATH` to a valid path. By default, the normalizer is trained and saved\n",
    "to `SCALER_PATH`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3pG6PA8rLzZi"
   },
   "source": [
    "CACHE_PATH = None\n",
    "SCALER_PATH = f\"{ROOT_OUTPUT_DIR}/node_scaler.dat\""
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "X40QGRNRLzZi"
   },
   "source": [
    "# Uncomment and set these variables to run next cell either\n",
    "# seperately or with customized parameters\n",
    "# GRAPH_DIR = 'PATH"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wEdgheRtLzZi",
    "outputId": "0e86b083-76eb-46d9-afe1-597fac49bae8"
   },
   "source": [
    "if CACHE_PATH and CACHE_PATH.exists():\n",
    "    SCALER_PATH = CACHE_PATH  # assignment for follow up loading\n",
    "    node_scaler = joblib.load(SCALER_PATH)\n",
    "else:\n",
    "    # ! we need a better way of doing this, will have OOM problem\n",
    "    loader = SlideGraphDataset(wsi_names, mode=\"infer\")\n",
    "    loader = DataLoader(\n",
    "        loader,\n",
    "        num_workers=8,\n",
    "        batch_size=1,\n",
    "        shuffle=False,\n",
    "        drop_last=False,\n",
    "    )\n",
    "    node_features = [v[\"graph\"].x.numpy() for idx, v in enumerate(tqdm(loader))]\n",
    "    node_features = np.concatenate(node_features, axis=0)\n",
    "    node_scaler = StandardScaler(copy=False)\n",
    "    node_scaler.fit(node_features)\n",
    "    joblib.dump(node_scaler, SCALER_PATH)\n",
    "\n",
    "\n",
    "# we must define the function after training/loading\n",
    "def nodes_preproc_func(node_features: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Pre-processing function for nodes.\"\"\"\n",
    "    return node_scaler.transform(node_features)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yTS2MvQZLzZi"
   },
   "source": [
    "### GNN Architecture Definition\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lWNB9f_jLzZi"
   },
   "source": [
    "class SlideGraphArch(nn.Module):\n",
    "    \"\"\"Define SlideGraph architecture.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self: nn.Module,\n",
    "        dim_features: int,\n",
    "        dim_target: int,\n",
    "        layers: list[int, int] | None = None,\n",
    "        pooling: str = \"max\",\n",
    "        dropout: float = 0.0,\n",
    "        conv: str = \"GINConv\",\n",
    "        *,\n",
    "        gembed: bool = False,\n",
    "        **kwargs: dict,\n",
    "    ) -> None:\n",
    "        \"\"\"Initialize SlideGraphArch.\"\"\"\n",
    "        super().__init__()\n",
    "        if layers is None:\n",
    "            layers = [6, 6]\n",
    "        self.dropout = dropout\n",
    "        self.embeddings_dim = layers\n",
    "        self.num_layers = len(self.embeddings_dim)\n",
    "        self.nns = []\n",
    "        self.convs = []\n",
    "        self.linears = []\n",
    "        self.pooling = {\n",
    "            \"max\": global_max_pool,\n",
    "            \"mean\": global_mean_pool,\n",
    "            \"add\": global_add_pool,\n",
    "        }[pooling]\n",
    "        # If True then learn a graph embedding for final classification\n",
    "        # (classify pooled node features), otherwise pool node decision scores.\n",
    "        self.gembed = gembed\n",
    "\n",
    "        conv_dict = {\"GINConv\": [GINConv, 1], \"EdgeConv\": [EdgeConv, 2]}\n",
    "        if conv not in conv_dict:\n",
    "            msg = f'Not support `conv=\"{conv}\".'\n",
    "            raise ValueError(msg)\n",
    "\n",
    "        def create_linear(in_dims: int, out_dims: int) -> Linear:\n",
    "            return nn.Sequential(\n",
    "                Linear(in_dims, out_dims),\n",
    "                BatchNorm1d(out_dims),\n",
    "                ReLU(),\n",
    "            )\n",
    "\n",
    "        input_emb_dim = dim_features\n",
    "        out_emb_dim = self.embeddings_dim[0]\n",
    "        self.first_h = create_linear(input_emb_dim, out_emb_dim)\n",
    "        self.linears.append(Linear(out_emb_dim, dim_target))\n",
    "\n",
    "        input_emb_dim = out_emb_dim\n",
    "        for out_emb_dim in self.embeddings_dim[1:]:\n",
    "            conv_class, alpha = conv_dict[conv]\n",
    "            subnet = create_linear(alpha * input_emb_dim, out_emb_dim)\n",
    "            # ! this variable should be removed after training integrity checking\n",
    "            self.nns.append(subnet)  # <--| as it already within ConvClass\n",
    "            self.convs.append(conv_class(self.nns[-1], **kwargs))\n",
    "            self.linears.append(Linear(out_emb_dim, dim_target))\n",
    "            input_emb_dim = out_emb_dim\n",
    "\n",
    "        self.nns = torch.nn.ModuleList(self.nns)\n",
    "        self.convs = torch.nn.ModuleList(self.convs)\n",
    "        # Has got one more for initial input, what does this mean\n",
    "        self.linears = torch.nn.ModuleList(self.linears)\n",
    "\n",
    "        # Auxilary holder for external model, these are saved separately from torch.save\n",
    "        # as they can be sklearn model etc.\n",
    "        self.aux_model = {}\n",
    "\n",
    "    def save(self: nn.Module, path: str | Path, aux_path: str | Path) -> None:\n",
    "        \"\"\"Save torch model.\"\"\"\n",
    "        state_dict = self.state_dict()\n",
    "        torch.save(state_dict, path)\n",
    "        joblib.dump(self.aux_model, aux_path)\n",
    "\n",
    "    def load(self: nn.Module, path: str | Path, aux_path: str | Path) -> None:\n",
    "        \"\"\"Load torch model.\"\"\"\n",
    "        state_dict = torch.load(path)\n",
    "        self.load_state_dict(state_dict)\n",
    "        self.aux_model = joblib.load(aux_path)\n",
    "\n",
    "    def forward(self: nn.Module, data: np.ndarray | torch.Tensor) -> tuple:\n",
    "        \"\"\"Torch model forward function.\"\"\"\n",
    "        feature, edge_index, batch = data.x, data.edge_index, data.batch\n",
    "\n",
    "        wsi_prediction = 0\n",
    "        pooling = self.pooling\n",
    "        node_prediction = 0\n",
    "\n",
    "        feature = self.first_h(feature)\n",
    "        for layer in range(self.num_layers):\n",
    "            if layer == 0:\n",
    "                node_prediction_sub = self.linears[layer](feature)\n",
    "                node_prediction += node_prediction_sub\n",
    "                node_pooled = pooling(node_prediction_sub, batch)\n",
    "                wsi_prediction_sub = F.dropout(\n",
    "                    node_pooled,\n",
    "                    p=self.dropout,\n",
    "                    training=self.training,\n",
    "                )\n",
    "                wsi_prediction += wsi_prediction_sub\n",
    "            else:\n",
    "                feature = self.convs[layer - 1](feature, edge_index)\n",
    "                if not self.gembed:\n",
    "                    node_prediction_sub = self.linears[layer](feature)\n",
    "                    node_prediction += node_prediction_sub\n",
    "                    node_pooled = pooling(node_prediction_sub, batch)\n",
    "                    wsi_prediction_sub = F.dropout(\n",
    "                        node_pooled,\n",
    "                        p=self.dropout,\n",
    "                        training=self.training,\n",
    "                    )\n",
    "                else:\n",
    "                    node_pooled = pooling(feature, batch)\n",
    "                    node_prediction_sub = self.linears[layer](node_pooled)\n",
    "                    wsi_prediction_sub = F.dropout(\n",
    "                        node_prediction_sub,\n",
    "                        p=self.dropout,\n",
    "                        training=self.training,\n",
    "                    )\n",
    "                wsi_prediction += wsi_prediction_sub\n",
    "        return wsi_prediction, node_prediction\n",
    "\n",
    "    # Run one single step\n",
    "    @staticmethod\n",
    "    def train_batch(\n",
    "        model: nn.Module,\n",
    "        batch_data: np.ndarray | torch.Tensor,\n",
    "        optimizer: torch.optim.Optimizer,\n",
    "        *,\n",
    "        on_gpu: bool,\n",
    "    ) -> list:\n",
    "        \"\"\"Helper function for model training.\"\"\"\n",
    "        device = select_device(on_gpu=on_gpu)\n",
    "        wsi_graphs = batch_data[\"graph\"].to(device)\n",
    "        wsi_labels = batch_data[\"label\"].to(device)\n",
    "        model = model.to(device)\n",
    "\n",
    "        # Data type conversion\n",
    "        wsi_graphs.x = wsi_graphs.x.type(torch.float32)\n",
    "\n",
    "        # Not an RNN so does not accumulate\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        wsi_output, _ = model(wsi_graphs)\n",
    "\n",
    "        # Both are expected to be Nx1\n",
    "        wsi_labels_ = wsi_labels[:, None]\n",
    "        wsi_labels_ = wsi_labels_ - wsi_labels_.T\n",
    "        wsi_output_ = wsi_output - wsi_output.T\n",
    "        diff = wsi_output_[wsi_labels_ > 0]\n",
    "        loss = torch.mean(F.relu(1.0 - diff))\n",
    "        # Backprop and update\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        #\n",
    "        loss = loss.detach().cpu().numpy()\n",
    "        assert not np.isnan(loss)  # noqa: S101\n",
    "        wsi_labels = wsi_labels.cpu().numpy()\n",
    "        return [loss, wsi_output, wsi_labels]\n",
    "\n",
    "    # Run one inference step\n",
    "    @staticmethod\n",
    "    def infer_batch(\n",
    "        model: nn.Module,\n",
    "        batch_data: torch.Tensor,\n",
    "        *,\n",
    "        on_gpu: bool,\n",
    "    ) -> list:\n",
    "        \"\"\"Model inference.\"\"\"\n",
    "        device = select_device(on_gpu=on_gpu)\n",
    "        wsi_graphs = batch_data[\"graph\"].to(device)\n",
    "        model = model.to(device)\n",
    "\n",
    "        # Data type conversion\n",
    "        wsi_graphs.x = wsi_graphs.x.type(torch.float32)\n",
    "\n",
    "        # Inference mode\n",
    "        model.eval()\n",
    "        # Do not compute the gradient (not training)\n",
    "        with torch.inference_mode():\n",
    "            wsi_output, _ = model(wsi_graphs)\n",
    "\n",
    "        wsi_output = wsi_output.cpu().numpy()\n",
    "        # Output should be a single tensor or scalar\n",
    "        if \"label\" in batch_data:\n",
    "            wsi_labels = batch_data[\"label\"]\n",
    "            wsi_labels = wsi_labels.cpu().numpy()\n",
    "            return wsi_output, wsi_labels\n",
    "        return [wsi_output]"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5alc4yjmLzZi"
   },
   "source": [
    "To test that our architecture works, at least superficially,\n",
    "we perform a brief inference with some random graph data and print\n",
    "out the output predictions.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "p3-iX2J9LzZi"
   },
   "source": [
    "# Uncomment and set these variables to run next cell either\n",
    "# seperately or with customized parameters"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "S2A6INHTLzZi",
    "outputId": "e42374f4-8bcb-4690-8490-832c01f9dc12"
   },
   "source": [
    "dummy_ds = SlideGraphDataset(wsi_names, mode=\"infer\")\n",
    "loader = DataLoader(\n",
    "    dummy_ds,\n",
    "    num_workers=0,\n",
    "    batch_size=8,\n",
    "    shuffle=False,\n",
    ")\n",
    "iterator = iter(loader)\n",
    "batch_data = iterator.__next__()\n",
    "\n",
    "# Data type conversion\n",
    "wsi_graphs = batch_data[\"graph\"]\n",
    "wsi_graphs.x = wsi_graphs.x.type(torch.float32)\n",
    "\n",
    "# Define model object\n",
    "arch_kwargs = {\n",
    "    \"dim_features\": NUM_NODE_FEATURES,\n",
    "    \"dim_target\": 1,\n",
    "    \"layers\": [16, 16, 8],\n",
    "    \"dropout\": 0.5,\n",
    "    \"pooling\": \"mean\",\n",
    "    \"conv\": \"EdgeConv\",\n",
    "    \"aggr\": \"max\",\n",
    "}\n",
    "model = SlideGraphArch(**arch_kwargs)\n",
    "\n",
    "# Inference section\n",
    "model.eval()\n",
    "with torch.inference_mode():\n",
    "    output, _ = model(wsi_graphs)\n",
    "    output = output.cpu().numpy()\n",
    "logger.info(\n",
    "    \"Output [%f, %f, %f, %f, %f, %f, %f, %f]\",\n",
    "    output[0][0],\n",
    "    output[0][1],\n",
    "    output[0][2],\n",
    "    output[0][3],\n",
    "    output[0][4],\n",
    "    output[0][5],\n",
    "    output[0][6],\n",
    "    output[0][7],\n",
    ")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iUILmNQqLzZi"
   },
   "source": [
    "Notice that the output values do not lie in the interval \\[0,1\\]. Later we will turn the above\n",
    "values into probabilities using [Platt Scaling](https://en.wikipedia.org/wiki/Platt_scaling).\n",
    "The scaler will be defined and trained during the training process defined below.\n",
    "After training is complete, the scaler can be accessed with:\n",
    "\n",
    "```python\n",
    "model = SlideGraphArch(**arch_kwargs)\n",
    "model.aux_model  # will hold the trained Platt Scaler\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ou0YRUVjLzZi"
   },
   "source": [
    "### Batch Sampler\n",
    "\n",
    "Now that we have ensured that the model can run, let's take a step back and look at\n",
    "the model definition again, in preparation for training and inference handling.\n",
    "\n",
    "The `infer_batch` is straightforward here: it handles inferencing of the input batch\n",
    "data and organizes the output content. Likewise, `train_batch` defines training, such as\n",
    "calculating the loss and so on. The loss defined here is not straightforward or standardized\n",
    "like cross-entropy. There is a pitfall lurking in the above code that could crash the training.\n",
    "Consider the lines:\n",
    "\n",
    "```python\n",
    "wsi_labels_ = wsi_labels[:, None]\n",
    "wsi_labels_ = wsi_labels_ - wsi_labels_.T\n",
    "wsi_output_ = wsi_output - wsi_output.T\n",
    "diff = wsi_output_[wsi_labels_ > 0]\n",
    "loss = torch.mean(F.relu(1.0 - diff))\n",
    "```\n",
    "\n",
    "Specifically, we need to take care of `diff = wsi_output_[wsi_labels_ > 0]` where\n",
    "we want to calculate the loss using only positive samples. When a batch contains\n",
    "no positive samples at all, especially for a skewed dataset, there will no samples to\n",
    "calculate the loss and we will have `NaN` loss. To resolve this, we define a sampler\n",
    "specifically for the training process, such that its resulting batch always contains\n",
    "positive samples.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "B-vzqct7LzZi"
   },
   "source": [
    "class StratifiedSampler(Sampler):\n",
    "    \"\"\"Sampling the dataset such that the batch contains stratified samples.\n",
    "\n",
    "    Args:\n",
    "        labels (list): List of labels, must be in the same ordering as input\n",
    "            samples provided to the `SlideGraphDataset` object.\n",
    "        batch_size (int): Size of the batch.\n",
    "\n",
    "    Returns:\n",
    "        List of indices to query from the `SlideGraphDataset` object.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self: Sampler, labels: list, batch_size: int = 10) -> None:\n",
    "        \"\"\"Initialize StratifiedSampler.\"\"\"\n",
    "        self.batch_size = batch_size\n",
    "        self.num_splits = int(len(labels) / self.batch_size)\n",
    "        self.labels = labels\n",
    "        self.num_steps = self.num_splits\n",
    "\n",
    "    def _sampling(self: Sampler) -> list:\n",
    "        \"\"\"Do we want to control randomness here.\"\"\"\n",
    "        skf = StratifiedKFold(n_splits=self.num_splits, shuffle=True)\n",
    "        indices = np.arange(len(self.labels))  # idx holder\n",
    "        # return array of arrays of indices in each batch\n",
    "        return [tidx for _, tidx in skf.split(indices, self.labels)]\n",
    "\n",
    "    def __iter__(self: Sampler) -> Iterator:\n",
    "        \"\"\"Define Iterator.\"\"\"\n",
    "        return iter(self._sampling())\n",
    "\n",
    "    def __len__(self: Sampler) -> int:\n",
    "        \"\"\"The length of the sampler.\n",
    "\n",
    "        This value actually corresponds to the number of steps to query\n",
    "        sampled batch indices. Thus, to maintain epoch and steps hierarchy,\n",
    "        this should be equal to the number of expected steps as in usual\n",
    "        sampling: `steps=dataset_size / batch_size`.\n",
    "\n",
    "        \"\"\"\n",
    "        return self.num_steps"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZYN5X1_1LzZj"
   },
   "source": [
    "### The Training Loop\n",
    "\n",
    "Training and running a neural network at the current time involves plugging\n",
    "several parts together so that they work in tandem. In simplified terms, training\n",
    "consists of the following steps:\n",
    "\n",
    "1. Define a network object (`torch.nn.module`) for a particular architecture.\n",
    "1. Define a loader object to handle loading data concurrently.\n",
    "1. Define an optimizer(s) and scheduler to update the network weights.\n",
    "1. Define callback functions for several stages (starting of epoch, end of step, etc.)\n",
    "   to aggregate results, save the models, refresh data, and much more.\n",
    "\n",
    "For inference, #3 is not necessary.\n",
    "\n",
    "At the moment, the wiring of these operations is handled mostly by various `engine`\n",
    "classes within the toolbox. However, they focus mostly on the inference portion.\n",
    "For the SlideGraph case and this notebook, we also require the `engine` to handle\n",
    "the training portion. Hence, we define below a very simplified version of what an\n",
    "`engine` usually does for both `training` and `inference`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v__WATVvLzZj"
   },
   "source": [
    "#### Helper Functions & Classes\n",
    "\n",
    "The function create_pbar simplifies the process of creating a progress bar for tracking the running loop. We also define a class to calculate the exponential moving average (EMA) of the training loss for each step.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "16mVfiE3LzZj"
   },
   "source": [
    "def create_pbar(subset_name: str, num_steps: int) -> tqdm:\n",
    "    \"\"\"Create a nice progress bar.\"\"\"\n",
    "    pbar_format = (\n",
    "        \"Processing: |{bar}| {n_fmt}/{total_fmt}[{elapsed}<{remaining},{rate_fmt}]\"\n",
    "    )\n",
    "    pbar = tqdm(total=num_steps, leave=True, bar_format=pbar_format, ascii=True)\n",
    "    if subset_name == \"train\":\n",
    "        pbar_format += \"step={postfix[1][step]:0.5f}|EMA={postfix[1][EMA]:0.5f}\"\n",
    "        # * Changing print char may break the bar so avoid it\n",
    "        pbar = tqdm(\n",
    "            total=num_steps,\n",
    "            leave=True,\n",
    "            initial=0,\n",
    "            bar_format=pbar_format,\n",
    "            ascii=True,\n",
    "            postfix=[\"\", {\"step\": float(\"NaN\"), \"EMA\": float(\"NaN\")}],\n",
    "        )\n",
    "    return pbar\n",
    "\n",
    "\n",
    "class ScalarMovingAverage:\n",
    "    \"\"\"Class to calculate running average.\"\"\"\n",
    "\n",
    "    def __init__(self: ScalarMovingAverage, alpha: float = 0.95) -> None:\n",
    "        \"\"\"Initialize ScalarMovingAverage.\"\"\"\n",
    "        super().__init__()\n",
    "        self.alpha = alpha\n",
    "        self.tracking_dict = {}\n",
    "\n",
    "    def __call__(self: ScalarMovingAverage, step_output: dict) -> None:\n",
    "        \"\"\"ScalarMovingAverage instances behave and can be called like a function.\"\"\"\n",
    "        for key, current_value in step_output.items():\n",
    "            if key in self.tracking_dict:\n",
    "                old_ema_value = self.tracking_dict[key]\n",
    "                # Calculate the exponential moving average\n",
    "                new_ema_value = (\n",
    "                    old_ema_value * self.alpha + (1.0 - self.alpha) * current_value\n",
    "                )\n",
    "                self.tracking_dict[key] = new_ema_value\n",
    "            else:  # Init for variable which appear for the first time\n",
    "                new_ema_value = current_value\n",
    "                self.tracking_dict[key] = new_ema_value"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QpdPQSmfLzZj"
   },
   "source": [
    "#### Defining The Loop\n",
    "\n",
    "Finally, we define the function to manage the running loop, or the simplified\n",
    "engine so to speak. The running loop contains of several important events\n",
    "that require special definition and handling of the dataset, the model, etc.\n",
    "\n",
    "- **EPOCH_START**: The start of each epoch. Depending on the task, it may be\n",
    "  necessary to clean up and refresh the data accumulated over the previous epoch\n",
    "  (such as clearing previous validation results).\n",
    "- **STEP_START**: The start of each step. The loader is asked for data. The data\n",
    "  is passed on and training or model inference step is triggered.\n",
    "- **STEP_STOP**: The end of each step. The loss is computed, console output is logged,\n",
    "  and the training or inference results are collated.\n",
    "- **EPOCH_COMPLETE**: The end of each epoch. This often involves saving the model,\n",
    "  or in our case, starting the training of the Platt Scaler.\n",
    "\n",
    "Often, each of these events has its own set of callbacks that will be invoked.\n",
    "Furthermore, these callbacks may also vary with dataset or running mode (such as\n",
    "metric calculations, saving mode, etc.). As this is a simplified version,\n",
    "we include all handling of these within `run_once`. In practice, they are\n",
    "usually factored out into a set of classes and hooks.\n",
    "\n",
    "The `run_once` function is provided with a dictionary of datasets.\n",
    "Within this dictionary, `train` is the dataset used for training which\n",
    "includes the sampler that ensures a positive sample in each batch.\n",
    "Additionally, `*infer-valid*` and `*infer-train*` are the datasets used for\n",
    "validation of the model and training of the Platt scaling respectively.\n",
    "These two datasets do not make use of the sampler ensuring a positive\n",
    "sample in each batch. Any other dataset in the dictionary which\n",
    "matches the pattern `*infer*` is assumed to be used for testing.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9rhLStLiLzZj"
   },
   "source": [
    "def run_once(  # noqa: C901, PLR0912, PLR0915\n",
    "    dataset_dict: dict,\n",
    "    num_epochs: int,\n",
    "    save_dir: str | Path,\n",
    "    pretrained: str | None = None,\n",
    "    loader_kwargs: dict | None = None,\n",
    "    arch_kwargs: dict | None = None,\n",
    "    optim_kwargs: dict | None = None,\n",
    "    *,\n",
    "    on_gpu: bool = True,\n",
    ") -> list:\n",
    "    \"\"\"Running the inference or training loop once.\n",
    "\n",
    "    The actual running mode is defined via the code name of the dataset\n",
    "    within `dataset_dict`. Here, `train` is specifically preserved for\n",
    "    the dataset used for training. `.*infer-valid.*` and `.*infer-train*`\n",
    "    are reserved for datasets containing the corresponding labels.\n",
    "    Otherwise, the dataset is assumed to be for the inference run.\n",
    "\n",
    "    \"\"\"\n",
    "    if loader_kwargs is None:\n",
    "        loader_kwargs = {}\n",
    "\n",
    "    if arch_kwargs is None:\n",
    "        arch_kwargs = {}\n",
    "\n",
    "    if optim_kwargs is None:\n",
    "        optim_kwargs = {}\n",
    "\n",
    "    model = SlideGraphArch(**arch_kwargs)\n",
    "    if pretrained is not None:\n",
    "        model.load(*pretrained)\n",
    "    model = model.to(\"cuda\")\n",
    "    optimizer = torch.optim.Adam(model.parameters(), **optim_kwargs)\n",
    "\n",
    "    # Create the graph dataset holder for each subset info then\n",
    "    # pipe them through torch/torch geometric specific loader\n",
    "    # for loading in multi-thread.\n",
    "    loader_dict = {}\n",
    "    for subset_name, subset in dataset_dict.items():\n",
    "        _loader_kwargs = copy.deepcopy(loader_kwargs)\n",
    "        batch_sampler = None\n",
    "        if subset_name == \"train\":\n",
    "            _loader_kwargs = {}\n",
    "            batch_sampler = StratifiedSampler(\n",
    "                labels=[v[1] for v in subset],\n",
    "                batch_size=loader_kwargs[\"batch_size\"],\n",
    "            )\n",
    "\n",
    "        ds = SlideGraphDataset(subset, mode=subset_name, preproc=nodes_preproc_func)\n",
    "        loader_dict[subset_name] = DataLoader(\n",
    "            ds,\n",
    "            batch_sampler=batch_sampler,\n",
    "            drop_last=subset_name == \"train\" and batch_sampler is None,\n",
    "            shuffle=subset_name == \"train\" and batch_sampler is None,\n",
    "            **_loader_kwargs,\n",
    "        )\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        logger.info(\"EPOCH: %03d\", epoch)\n",
    "        for loader_name, loader in loader_dict.items():\n",
    "            # * EPOCH START\n",
    "            step_output = []\n",
    "            ema = ScalarMovingAverage()\n",
    "            pbar = create_pbar(loader_name, len(loader))\n",
    "            for _step, batch_data in enumerate(loader):\n",
    "                # * STEP COMPLETE CALLBACKS\n",
    "                if loader_name == \"train\":\n",
    "                    output = model.train_batch(model, batch_data, on_gpu, optimizer)\n",
    "                    # check the output for agreement\n",
    "                    ema({\"loss\": output[0]})\n",
    "                    pbar.postfix[1][\"step\"] = output[0]\n",
    "                    pbar.postfix[1][\"EMA\"] = ema.tracking_dict[\"loss\"]\n",
    "                else:\n",
    "                    output = model.infer_batch(model, batch_data, on_gpu)\n",
    "\n",
    "                    batch_size = batch_data[\"graph\"].num_graphs\n",
    "                    # Iterate over output head and retrieve\n",
    "                    # each as N x item, each item may be of\n",
    "                    # arbitrary dimensions\n",
    "                    output = [np.split(v, batch_size, axis=0) for v in output]\n",
    "                    # pairing such that it will be\n",
    "                    # N batch size x H head list\n",
    "                    output = list(zip(*output))\n",
    "                    step_output.extend(output)\n",
    "                pbar.update()\n",
    "            pbar.close()\n",
    "\n",
    "            # * EPOCH COMPLETE\n",
    "\n",
    "            # Callbacks to process output\n",
    "            logging_dict = {}\n",
    "            if loader_name == \"train\":\n",
    "                for val_name, val in ema.tracking_dict.items():\n",
    "                    logging_dict[f\"train-EMA-{val_name}\"] = val\n",
    "            elif \"infer\" in loader_name and any(\n",
    "                v in loader_name for v in [\"train\", \"valid\"]\n",
    "            ):\n",
    "                # Expand the list of N dataset size x H heads\n",
    "                # back to a list of H Head each with N samples.\n",
    "                output = list(zip(*step_output))\n",
    "                logit, true = output\n",
    "                logit = np.squeeze(np.array(logit))\n",
    "                true = np.squeeze(np.array(true))\n",
    "\n",
    "                if \"train\" in loader_name:\n",
    "                    scaler = PlattScaling()\n",
    "                    scaler.fit(np.array(logit, ndmin=2).T, true)\n",
    "                    model.aux_model[\"scaler\"] = scaler\n",
    "                scaler = model.aux_model[\"scaler\"]\n",
    "                prob = scaler.predict_proba(np.array(logit, ndmin=2).T)[:, 0]\n",
    "\n",
    "                val = auroc_scorer(true, prob)\n",
    "                logging_dict[f\"{loader_name}-auroc\"] = val\n",
    "                val = auprc_scorer(true, prob)\n",
    "                logging_dict[f\"{loader_name}-auprc\"] = val\n",
    "\n",
    "                logging_dict[f\"{loader_name}-raw-logit\"] = logit\n",
    "                logging_dict[f\"{loader_name}-raw-true\"] = true\n",
    "\n",
    "            # Callbacks for logging and saving\n",
    "            for val_name, val in logging_dict.items():\n",
    "                if \"raw\" not in val_name:\n",
    "                    logging.info(\"%s: %d:\", val_name, val)\n",
    "            if \"train\" not in loader_dict:\n",
    "                continue\n",
    "\n",
    "            # Track the statistics\n",
    "            new_stats = {}\n",
    "            if (save_dir / \"stats.json\").exists():\n",
    "                old_stats = load_json(f\"{save_dir}/stats.json\")\n",
    "                # Save a backup first\n",
    "                save_as_json(old_stats, f\"{save_dir}/stats.old.json\", exist_ok=False)\n",
    "                new_stats = copy.deepcopy(old_stats)\n",
    "                new_stats = {int(k): v for k, v in new_stats.items()}\n",
    "\n",
    "            old_epoch_stats = {}\n",
    "            if epoch in new_stats:\n",
    "                old_epoch_stats = new_stats[epoch]\n",
    "            old_epoch_stats.update(logging_dict)\n",
    "            new_stats[epoch] = old_epoch_stats\n",
    "            save_as_json(new_stats, f\"{save_dir}/stats.json\", exist_ok=False)\n",
    "\n",
    "            # Save the pytorch model\n",
    "            model.save(\n",
    "                f\"{save_dir}/epoch={epoch:03d}.weights.pth\",\n",
    "                f\"{save_dir}/epoch={epoch:03d}.aux.dat\",\n",
    "            )\n",
    "    return step_output"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nyIN0eAgLzZj"
   },
   "source": [
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    ")\n",
    "\n",
    "\n",
    "def reset_logging(save_path: str | Path) -> None:\n",
    "    \"\"\"Reset logger handler.\"\"\"\n",
    "    log_formatter = logging.Formatter(\n",
    "        \"|%(asctime)s.%(msecs)03d| [%(levelname)s] %(message)s\",\n",
    "        datefmt=\"%Y-%m-%d|%H:%M:%S\",\n",
    "    )\n",
    "    log = logging.getLogger()  # Root logger\n",
    "    for hdlr in log.handlers[:]:  # Remove all old handlers\n",
    "        log.removeHandler(hdlr)\n",
    "    new_hdlr_list = [\n",
    "        logging.FileHandler(f\"{save_path}/debug.log\"),\n",
    "        logging.StreamHandler(),\n",
    "    ]\n",
    "    for hdlr in new_hdlr_list:\n",
    "        hdlr.setFormatter(log_formatter)\n",
    "        log.addHandler(hdlr)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "96OsdzSJLzZj"
   },
   "source": [
    "### Training\n",
    "\n",
    "With the `engine` above, we can now start our training loop with\n",
    "a set of parameters:\n",
    "\n",
    "- `MODEL_DIR`: the location where we save the model weights\n",
    "  and associated information every epoch. Under it, we have\n",
    "  - `epoch=[X].weights.pth`: the graph neural network weights after\n",
    "    the X-th training epoch.\n",
    "  - `epoch=[X].weights.aux.dat`: the associated sklearn model trained\n",
    "    for the X-th epoch. In our case, it contains the Platt Scaling.\n",
    "  - `stats.json`: the file contains accumulated statistic of the entire\n",
    "    training run for the X-th epoch.\n",
    "  - `stats.old.json`: the backup file of `stats.json` of the previous epoch.\n",
    "- `NUM_EPOCHS`: the number of epoch for training.\n",
    "\n",
    "To avoid accidentally over-writing training results, we will\n",
    "skip training if `MODEL_DIR` already exists.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZVc9GeKrLzZj"
   },
   "source": [
    "# Default parameters\n",
    "NUM_EPOCHS = 100\n",
    "NUM_NODE_FEATURES = 4\n",
    "SCALER_PATH = f\"{ROOT_OUTPUT_DIR}/node_scaler.dat\"\n",
    "MODEL_DIR = f\"{ROOT_OUTPUT_DIR}/model/\""
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZE_HhOJoLzZj"
   },
   "source": [
    "# Uncomment and set these variables to run next cell either\n",
    "# seperately or with customized parameters"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3wL1wsI7LzZj"
   },
   "source": [
    "splits = joblib.load(SPLIT_PATH)\n",
    "node_scaler = joblib.load(SCALER_PATH)\n",
    "loader_kwargs = {\n",
    "    \"num_workers\": 8,\n",
    "    \"batch_size\": 16,\n",
    "}\n",
    "arch_kwargs = {\n",
    "    \"dim_features\": NUM_NODE_FEATURES,\n",
    "    \"dim_target\": 1,\n",
    "    \"layers\": [16, 16, 8],\n",
    "    \"dropout\": 0.5,\n",
    "    \"pooling\": \"mean\",\n",
    "    \"conv\": \"EdgeConv\",\n",
    "    \"aggr\": \"max\",\n",
    "}\n",
    "optim_kwargs = {\n",
    "    \"lr\": 1.0e-3,\n",
    "    \"weight_decay\": 1.0e-4,\n",
    "}\n",
    "\n",
    "\n",
    "if not MODEL_DIR.exists():\n",
    "    for split_idx, split in enumerate(splits):\n",
    "        new_split = {\n",
    "            \"train\": split[\"train\"],\n",
    "            \"infer-train\": split[\"train\"],\n",
    "            \"infer-valid-A\": split[\"valid\"],\n",
    "            \"infer-valid-B\": split[\"test\"],\n",
    "        }\n",
    "        split_save_dir = f\"{MODEL_DIR}/{split_idx:02d}/\"\n",
    "        rm_n_mkdir(split_save_dir)\n",
    "        reset_logging(split_save_dir)\n",
    "        run_once(\n",
    "            new_split,\n",
    "            NUM_EPOCHS,\n",
    "            save_dir=split_save_dir,\n",
    "            arch_kwargs=arch_kwargs,\n",
    "            loader_kwargs=loader_kwargs,\n",
    "            optim_kwargs=optim_kwargs,\n",
    "        )"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0ST7V5OqLzZj"
   },
   "source": [
    "### Inference\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sgT3YxhFLzZj"
   },
   "source": [
    "#### Model Selections\n",
    "\n",
    "According to our engine running loop defined above, we will have the following metrics saved for each epoch:\n",
    "\n",
    "- \"infer-train-auroc\"\n",
    "- \"infer-train-auprc\"\n",
    "- \"infer-valid-auroc\"\n",
    "- \"infer-valid-auprc\"\n",
    "\n",
    "With these metrics, we can pick the most promising model weights for inference\n",
    "on an independent dataset. We encapsulate this selection within the `select_checkpoints`\n",
    "function.\n",
    "\n",
    "> **Note**: For the metrics we defined here (`auroc`, `auprc`), a larger value\n",
    "> is better. If you want to add your own metrics, remember to change\n",
    "> the comparison operators within `select_checkpoints` function accordingly.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1cy3cQp8LzZk"
   },
   "source": [
    "def select_checkpoints(\n",
    "    stat_file_path: str,\n",
    "    top_k: int = 2,\n",
    "    metric: str = \"infer-valid-auprc\",\n",
    "    epoch_range: tuple[int] | None = None,\n",
    ") -> tuple[list, list]:\n",
    "    \"\"\"Select checkpoints basing on training statistics.\n",
    "\n",
    "    Args:\n",
    "        stat_file_path (str): Path pointing to the .json\n",
    "            which contains the statistics.\n",
    "        top_k (int): Number of top checkpoints to be selected.\n",
    "        metric (str): The metric name saved within .json to perform\n",
    "            selection.\n",
    "        epoch_range (list): The range of epochs for checking, denoted\n",
    "            as [start, end] . Epoch x that is `start <= x <= end` is\n",
    "            kept for further selection.\n",
    "\n",
    "    Returns:\n",
    "        paths (list): List of paths or info tuple where each point\n",
    "            to the correspond check point saving location.\n",
    "        stats (list): List of corresponding statistics.\n",
    "\n",
    "    \"\"\"\n",
    "    if epoch_range is None:\n",
    "        epoch_range = [0, 1000]\n",
    "    stats_dict = load_json(stat_file_path)\n",
    "    # k is the epoch counter in this case\n",
    "    stats_dict = {\n",
    "        k: v\n",
    "        for k, v in stats_dict.items()\n",
    "        if int(k) >= epoch_range[0] and int(k) <= epoch_range[1]\n",
    "    }\n",
    "    stats = [[int(k), v[metric], v] for k, v in stats_dict.items()]\n",
    "    # sort epoch ranking from largest to smallest\n",
    "    stats = sorted(stats, key=lambda v: v[1], reverse=True)\n",
    "    chkpt_stats_list = stats[:top_k]  # select top_k\n",
    "\n",
    "    model_dir = Path(stat_file_path).parent\n",
    "    epochs = [v[0] for v in chkpt_stats_list]\n",
    "    paths = [\n",
    "        (\n",
    "            f\"{model_dir}/epoch={epoch:03d}.weights.pth\",\n",
    "            f\"{model_dir}/epoch={epoch:03d}.aux.dat\",\n",
    "        )\n",
    "        for epoch in epochs\n",
    "    ]\n",
    "    chkpt_stats_list = [[v[0], v[2]] for v in chkpt_stats_list]\n",
    "    print(paths)  # noqa: T201\n",
    "    return paths, chkpt_stats_list"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e0AujxZ4LzZk"
   },
   "source": [
    "#### Bulk Inference & Ensemble Results\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AqdSSmVuLzZk"
   },
   "source": [
    "# default parameters\n",
    "TOP_K = 1\n",
    "metric_name = \"infer-valid-B-auroc\"\n",
    "PRETRAINED_DIR = f\"{ROOT_OUTPUT_DIR}/model/\"\n",
    "SCALER_PATH = f\"{ROOT_OUTPUT_DIR}/node_scaler.dat\""
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ulXBenzkLzZk"
   },
   "source": [
    "# Uncomment and set these variables to run the next cell,\n",
    "# either seperately or with customized parameters"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fdcKPBn_LzZk",
    "outputId": "095eeff7-2b59-46da-faab-700593eba511"
   },
   "source": [
    "splits = joblib.load(SPLIT_PATH)\n",
    "node_scaler = joblib.load(SCALER_PATH)\n",
    "loader_kwargs = {\n",
    "    \"num_workers\": 8,\n",
    "    \"batch_size\": 16,\n",
    "}\n",
    "arch_kwargs = {\n",
    "    \"dim_features\": NUM_NODE_FEATURES,\n",
    "    \"dim_target\": 1,\n",
    "    \"layers\": [16, 16, 8],\n",
    "    \"dropout\": 0.5,\n",
    "    \"pooling\": \"mean\",\n",
    "    \"conv\": \"EdgeConv\",\n",
    "    \"aggr\": \"max\",\n",
    "}\n",
    "\n",
    "cum_stats = []\n",
    "for split_idx, split in enumerate(splits):\n",
    "    new_split = {\"infer\": [v[0] for v in split[\"test\"]]}\n",
    "\n",
    "    stat_files = recur_find_ext(f\"{PRETRAINED_DIR}/{split_idx:02d}/\", [\".json\"])\n",
    "    stat_files = [v for v in stat_files if \".old.json\" not in v]\n",
    "    assert len(stat_files) == 1  # noqa: S101\n",
    "    chkpts, chkpt_stats_list = select_checkpoints(\n",
    "        stat_files[0],\n",
    "        top_k=TOP_K,\n",
    "        metric=metric_name,\n",
    "    )\n",
    "\n",
    "    # Perform ensembling by averaging probabilities\n",
    "    # across checkpoint predictions\n",
    "    cum_results = []\n",
    "    for chkpt_info in chkpts:\n",
    "        chkpt_results = run_once(\n",
    "            new_split,\n",
    "            num_epochs=1,\n",
    "            save_dir=None,\n",
    "            pretrained=chkpt_info,\n",
    "            arch_kwargs=arch_kwargs,\n",
    "            loader_kwargs=loader_kwargs,\n",
    "        )\n",
    "        # * re-calibrate logit to probabilities\n",
    "        model = SlideGraphArch(**arch_kwargs)\n",
    "        model.load(*chkpt_info)\n",
    "        scaler = model.aux_model[\"scaler\"]\n",
    "        chkpt_results = np.array(chkpt_results)\n",
    "        chkpt_results = np.squeeze(chkpt_results)\n",
    "        chkpt_results = scaler.transform(chkpt_results)\n",
    "\n",
    "        cum_results.append(chkpt_results)\n",
    "    cum_results = np.array(cum_results)\n",
    "    cum_results = np.squeeze(cum_results)\n",
    "\n",
    "    prob = cum_results\n",
    "    if len(cum_results.shape) == 2:  # noqa: PLR2004\n",
    "        prob = np.mean(cum_results, axis=0)\n",
    "\n",
    "    # * Calculate split statistics\n",
    "    true = [v[1] for v in split[\"test\"]]\n",
    "    true = np.array(true)\n",
    "\n",
    "    cum_stats.append(\n",
    "        {\"auroc\": auroc_scorer(true, prob), \"auprc\": auprc_scorer(true, prob)},\n",
    "    )"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yUcSJOxLLzZk"
   },
   "source": [
    "Now we print out the results.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YrYd72PLLzZk",
    "outputId": "319df11d-69b9-40ae-deea-40d7fc8b5cf6"
   },
   "source": [
    "stat_df = pd.DataFrame(cum_stats)\n",
    "for metric in stat_df.columns:\n",
    "    vals = stat_df[metric]\n",
    "    mu = np.mean(vals)\n",
    "    va = np.std(vals)\n",
    "    logger.info(\" %s: %0.4fÂ±%0.4f\", metric, mu, va)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y47n0usRLzZk"
   },
   "source": [
    "#### Visualizing Node Activation of the Graph Neural Network\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cnjxkq6mLzZk"
   },
   "source": [
    "Visualizing the activations of each node within the graph is sometimes necessary to either debug or\n",
    "verify the predictions of the graph neural network. Here, we demonstrate\n",
    "\n",
    "1. Loading a pretrained model and running inference on one single sample graph.\n",
    "1. Retrieving the node activations and plot them on the original WSI.\n",
    "\n",
    "By default, notice that node activations are output when running the `mode.forward(input)` (Or\n",
    "simply `model(input)` in pytorch).\n",
    "\n",
    "By default, we download the pretrained model as well as samples from the tiatoolbox server to\n",
    "`DOWNLOAD_DIR`. However, if you want to use your own set of input, you can comment out the next cell\n",
    "and provide your own data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Iu7mBbviLzZk",
    "outputId": "9cb281c8-1e0b-4b78-b032-2207cf91c644"
   },
   "source": [
    "# ! If you want to run your own set of input, comment out this cell\n",
    "# ! and uncomment the next cell\n",
    "DOWNLOAD_DIR = \"local/dump/\"\n",
    "WSI_PATH = f\"{DOWNLOAD_DIR}/sample.svs\"\n",
    "GRAPH_PATH = f\"{DOWNLOAD_DIR}/graph.json\"\n",
    "SCALER_PATH = f\"{DOWNLOAD_DIR}/node_scaler.dat\"\n",
    "MODEL_WEIGHTS_PATH = f\"{DOWNLOAD_DIR}/model.weigths.pth\"\n",
    "MODEL_AUX_PATH = f\"{DOWNLOAD_DIR}/model.aux.dat\"\n",
    "mkdir(DOWNLOAD_DIR)\n",
    "\n",
    "# Downloading sample image tile\n",
    "URL_HOME = \"https://tiatoolbox.dcs.warwick.ac.uk/models/slide_graph/cell-composition\"\n",
    "download_data(\n",
    "    f\"{URL_HOME}/TCGA-C8-A278-01Z-00-DX1.188B3FE0-7B20-401A-A6B7-8F1798018162.svs\",\n",
    "    WSI_PATH,\n",
    ")\n",
    "download_data(\n",
    "    f\"{URL_HOME}/TCGA-C8-A278-01Z-00-DX1.188B3FE0-7B20-401A-A6B7-8F1798018162.json\",\n",
    "    GRAPH_PATH,\n",
    ")\n",
    "download_data(f\"{URL_HOME}/node_scaler.dat\", SCALER_PATH)\n",
    "download_data(f\"{URL_HOME}/model.aux.dat\", MODEL_AUX_PATH)\n",
    "download_data(f\"{URL_HOME}/model.weights.pth\", MODEL_WEIGHTS_PATH)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "g93Qe5t9LzZk"
   },
   "source": [
    "# If you want to run your own set of input,\n",
    "# uncomment these lines and then set variables to run next cell"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5maTP90mLzZk"
   },
   "source": [
    "Most of the time the nodes within the graph will be at different resolutions\n",
    "from the resolution at which we want to visualize them. Before plotting, we scale their coordinates\n",
    "to the target resolution. We provide `NODE_RESOLUTION` and `PLOT_RESOLUTION` variables\n",
    "respectively as the resolution of the node and the resolution at which to plot the graph.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9c2IIT8yLzZl",
    "outputId": "cadbd9c3-822c-4d80-c639-ba2d41ba4e56"
   },
   "source": [
    "NODE_SIZE = 25\n",
    "NUM_NODE_FEATURES = 4\n",
    "NODE_RESOLUTION = {\"resolution\": 0.25, \"units\": \"mpp\"}\n",
    "PLOT_RESOLUTION = {\"resolution\": 4.0, \"units\": \"mpp\"}\n",
    "\n",
    "node_scaler = joblib.load(SCALER_PATH)\n",
    "loader_kwargs = {\n",
    "    \"num_workers\": 8,\n",
    "    \"batch_size\": 16,\n",
    "}\n",
    "arch_kwargs = {\n",
    "    \"dim_features\": NUM_NODE_FEATURES,\n",
    "    \"dim_target\": 1,\n",
    "    \"layers\": [16, 16, 8],\n",
    "    \"dropout\": 0.5,\n",
    "    \"pooling\": \"mean\",\n",
    "    \"conv\": \"EdgeConv\",\n",
    "    \"aggr\": \"max\",\n",
    "}\n",
    "\n",
    "\n",
    "with GRAPH_PATH.open() as fptr:\n",
    "    graph_dict = json.load(fptr)\n",
    "graph_dict = {k: np.array(v) for k, v in graph_dict.items()}\n",
    "graph_dict[\"x\"] = node_scaler.transform(graph_dict[\"x\"])\n",
    "graph_dict = {k: torch.tensor(v) for k, v in graph_dict.items()}\n",
    "graph = Data(**graph_dict)\n",
    "batch = Batch.from_data_list([graph])\n",
    "\n",
    "model = SlideGraphArch(**arch_kwargs)\n",
    "model.load(MODEL_WEIGHTS_PATH, MODEL_AUX_PATH)\n",
    "model = model.to(\"cuda\")\n",
    "\n",
    "# Data type conversion\n",
    "batch = batch.to(\"cuda\")\n",
    "batch.x = batch.x.type(torch.float32)\n",
    "predictions, node_activations = model(batch)\n",
    "node_activations = node_activations.detach().cpu().numpy()\n",
    "\n",
    "reader = OpenSlideWSIReader(WSI_PATH)\n",
    "node_resolution = reader.slide_dimensions(**NODE_RESOLUTION)\n",
    "plot_resolution = reader.slide_dimensions(**PLOT_RESOLUTION)\n",
    "fx = np.array(node_resolution) / np.array(plot_resolution)\n",
    "\n",
    "cmap = plt.get_cmap(\"inferno\")\n",
    "graph = graph.to(\"cpu\")\n",
    "\n",
    "node_coordinates = np.array(graph.coordinates) / fx\n",
    "node_colors = (cmap(np.squeeze(node_activations))[..., :3] * 255).astype(np.uint8)\n",
    "edges = graph.edge_index.T\n",
    "\n",
    "thumb = reader.slide_thumbnail(**PLOT_RESOLUTION)\n",
    "thumb_overlaid = plot_graph(\n",
    "    thumb.copy(),\n",
    "    node_coordinates,\n",
    "    edges,\n",
    "    node_colors=node_colors,\n",
    "    node_size=NODE_SIZE,\n",
    ")\n",
    "\n",
    "ax = plt.subplot(1, 1, 1)\n",
    "plt.imshow(thumb_overlaid)\n",
    "plt.axis(\"off\")\n",
    "# Add minorticks on the colorbar to make it easy to read the\n",
    "# values off the colorbar.\n",
    "fig = plt.gcf()\n",
    "norm = mpl.colors.Normalize(\n",
    "    vmin=np.min(node_activations),\n",
    "    vmax=np.max(node_activations),\n",
    ")\n",
    "sm = plt.cm.ScalarMappable(cmap=cmap, norm=norm)\n",
    "cbar = fig.colorbar(sm, ax=ax, extend=\"both\")\n",
    "cbar.minorticks_on()\n",
    "plt.show()"
   ],
   "outputs": []
  }
 ],
 "metadata": {
  "celltoolbar": "Edit Metadata",
  "colab": {
   "provenance": []
  },
  "interpreter": {
   "hash": "d7dbdbe2455f46b5db4d84207edcb1b9a28bd63f9214791348f746cdd6bc0c58"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
