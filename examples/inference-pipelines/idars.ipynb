{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8Kag0_9eJLPd"
   },
   "source": [
    "# Prediction of Molecular Pathways and Key Mutations\n",
    "\n",
    "Click to open in: [[GitHub](https://github.com/TissueImageAnalytics/tiatoolbox/tree/master/examples/inference-pipelines/idars.ipynb)][[Colab](https://colab.research.google.com/github/TissueImageAnalytics/tiatoolbox/blob/master/examples/inference-pipelines/idars.ipynb)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbsphinx": "hidden"
   },
   "source": [
    "## About this notebook\n",
    "\n",
    "This jupyter notebook can be run on any computer with a standard browser and no prior installation of any programming language is required. It can run remotely over the Internet, free of charge, thanks to Google Colaboratory. To connect with Colab, click on one of the two blue checkboxes above. Check that \"colab\" appears in the address bar. You can right-click on \"Open in Colab\" and select \"Open in new tab\" if the left click does not work for you. Familiarize yourself with the drop-down menus near the top of the window. You can edit the notebook during the session, for example substituting your own image files for the image files used in this demo. Experiment by changing the parameters of functions. It is not possible for an ordinary user to permanently change this version of the notebook on GitHub or Colab, so you cannot inadvertently mess it up. Use the notebook's File Menu if you wish to save your own (changed) notebook.\n",
    "\n",
    "To run the notebook on any platform, except for Colab, set up your Python environment, as explained in the\n",
    "[README](https://github.com/TIA-Lab/tiatoolbox/blob/master/README.md#install-python-package) file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p3l08vukKAWo"
   },
   "source": [
    "## About this demo\n",
    "\n",
    "Prediction of molecular pathways and key mutations directly from Haematoxylin and Eosin stained histology images can help bypass additional genetic (e.g., polymerase chain reaction or PCR) or immunohistochemistry (IHC) testing, which can therefore save both money and time.\n",
    "\n",
    "In this example notebook, we show how you can use pretrained models to reproduce the inference results obtained by [Bilal et al.](https://www.thelancet.com/journals/landig/article/PIIS2589-7500&#40;2100180-1/fulltext)\n",
    "\n",
    "In this paper, an Iterative Draw and Rank Sampling (IDaRS) approach is proposed, which consists of a two-stage approach:\n",
    "\n",
    "1. Patch-level tumour classification\n",
    "2. Patch-level WSI classification\n",
    "\n",
    "In stage 1, we use a pretrained tumour segmentation model to identify potentially diagnositc areas. In stage 2, we make a task-specific prediction for each tumour patch. For stage 2, the model is trained utilising only the slide-level label and hence the model does not require any detailed annotations at the cell or region level. Here, representative tiles from each WSI are iteratively sampled, which gives justification for the name IDaRS.\n",
    "\n",
    "In TIAToolbox, we include models that are capable of predicting:\n",
    "\n",
    "- Microsatellite instability (MSI)\n",
    "- Hypermutation density\n",
    "- Chromosomal instability\n",
    "- CpG island methylator phenotype (CIMP)-high prediction\n",
    "- BRAF mutation\n",
    "- TP53 mutation\n",
    "\n",
    "## Available models\n",
    "\n",
    "In line with the above description, in TIAToolbox, we provide the following pretrained models used as part of the [original publication](https://www.thelancet.com/journals/landig/article/PIIS2589-7500&#40;2100180-1/fulltext).\n",
    "\n",
    "- Tumour segmentation\n",
    "    - `resnet18-idars-tumour`\n",
    "- Task specific prediction\n",
    "    - MSI: `resnet34-idars-msi`\n",
    "    - Hypermutation density: `resnet34-idars-hm`\n",
    "    - Chromosomal instability: `resnet34-idars-cin`\n",
    "    - CpG island methylator phenotype (CIMP)-high prediction: `resnet34-idars-cimp`\n",
    "    - BRAF mutation: `resnet34-idars-braf`\n",
    "    - TP53 mutation: `resnet34-idars-tp53`\n",
    "\n",
    "The provided models are trained on the first fold used in the original paper."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BkeTbaX_mo7Z",
    "nbsphinx": "hidden"
   },
   "source": [
    "## Setting up the environment\n",
    "\n",
    "### TIAToolbox and dependencies installation\n",
    "\n",
    "You can skip the following cell if 1) you are not using the Colab plaform or 2) you are using Colab and this is not your first run of the notebook in the current runtime session. If you nevertheless run the cell, you may get an error message, but no harm will be done. On Colab the cell installs `tiatoolbox`, and other prerequisite software. Harmless error messages should be ignored. Outside Colab , the notebook expects `tiatoolbox` to already be installed. (See the instructions in [README](https://github.com/TIA-Lab/tiatoolbox/blob/master/README.md#install-python-package).)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oCOSzUCUXnfh",
    "nbsphinx": "hidden",
    "outputId": "4f06aa6a-69c7-4fb9-b124-d1f10b1527ce"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: apt-get: command not found\n",
      "Requirement already satisfied: six>=1.5 in /Users/dbae/opt/miniconda3/envs/tiatoolbox-dev/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib>==3.5.1->tiatoolbox) (1.16.0)\n",
      "Installation is done.\n"
     ]
    }
   ],
   "source": [
    "!apt-get -y install libopenjp2-7-dev libopenjp2-tools openslide-tools | tail -n 1\n",
    "!pip install tiatoolbox | tail -n 1\n",
    "\n",
    "print(\"Installation is done.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**IMPORTANT**: If you are using Colab and you run the cell above for the first time, please note that you need to restart the runtime before proceeding through (menu) *Runtime&#8594;Restart runtime* . This is needed to load the latest versions of prerequisite packages installed with TIAToolbox. Doing so, you should be able to run all the remaining cells altogether (*Runtime&#8594;Run after* from the next cell) or one by one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPU or CPU runtime\n",
    "\n",
    "Processes in this notebook can be accelerated by using a GPU. Therefore, whether you are running this notebook on your system or Colab, you need to check and specify if you are using GPU or CPU hardware acceleration. In Colab, you need to make sure that the runtime type is set to GPU in the *\"Runtime&#8594;Change runtime type&#8594;Hardware accelerator\"*. If you are *not* using GPU, consider changing the `ON_GPU` flag to `Flase` value, otherwise, some errors will be raised when running the following cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "ON_GPU = False\n",
    "\"\"\"ON_GPU should be True if cuda-enabled GPU is \n",
    "available and False otherwise \"\"\"\n",
    "WORKERS = 0\n",
    "if ON_GPU:\n",
    "    WORKERS = 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing leftovers from previous runs\n",
    "\n",
    "The cell below removes a tumour mask that may have been generated with a previous run. This cell can be skipped if you are running this notebook for the first time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf tumour_mask.png"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hmJxBFzDJLPj"
   },
   "source": [
    "### Importing related libraries\n",
    "\n",
    "We import some standard Python modules, and also the Python module `wsireader` (see [details](https://github.com/TIA-Lab/tiatoolbox/blob/master/tiatoolbox/wsicore/wsireader.py)) written by the TIA Centre team."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "UEIfjUTaJLPj"
   },
   "outputs": [],
   "source": [
    "from tiatoolbox.models.engine.patch_predictor import PatchPredictor\n",
    "from tiatoolbox.utils.misc import imwrite\n",
    "from tiatoolbox.wsicore.wsireader import WSIReader\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "\n",
    "mpl.rcParams[\"figure.dpi\"] = 140  # for high resolution figure in notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kA9ti0jNXniD"
   },
   "source": [
    "### Downloading the required files\n",
    "\n",
    "We download, over the internet, image files used for the purpose of this notebook. In particular, we download a whole slide image of cancerous colon tissue to highlight how the pipeline works.\n",
    "\n",
    "In Colab, if you click the files icon (see below) in the vertical toolbar on the left hand side then you can see all the files which the code in this notebook can access. The data will appear here when it is downloaded.\n",
    "\n",
    "![image.png](data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACQAAAAlCAYAAAAqXEs9AAAAwElEQVRYhe3WMQ6DMAyFYa7q1Yfw7Dl3ICusZM0hzJpDMLtTGSoFNy2UVPIvvf3DYsignTXcDXjNQVYOsnKQlYOsDkHjOCoiKgBUl3P+DWhZlkPIVagqaJqmt0EAoDFGnefZXEpJt227HtQyZv4chIjKzKeMiHZU7Uom6OhrWhORHSQiDnKQg/oChRD6AjGzg/4L9PyHiEjXdT1lKaUdVEppA7W8h1qHiNUrfv1ibB0RVa9jgu7IQVYOsnKQVXegB/ZWYoL8lUCBAAAAAElFTkSuQmCC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Rjw14DaJlO_o",
    "outputId": "d0d4f6bc-abc5-4003-953c-6e0465fb31c5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Download has started. Please wait...\n",
      "Download is complete.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "wsi_file_name = \"sample_wsi.svs\"\n",
    "print(\"Download has started. Please wait...\")\n",
    "\n",
    "# Downloading sample TCGA whole-slide image\n",
    "r = requests.get(\n",
    "    \"https://tiatoolbox.dcs.warwick.ac.uk/sample_wsis/TCGA-AD-6964-01Z-00-DX1.83AF88B9-C59B-48C6-A739-85ACB8F8ECA9.svs\"\n",
    ")\n",
    "with open(wsi_file_name, \"wb\") as f:\n",
    "    f.write(r.content)\n",
    "\n",
    "print(\"Download is complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fKterRqUKHnW"
   },
   "source": [
    "## Tumour segmentation using TIAToolbox pretrained models\n",
    "\n",
    "In this section, we will display patch-level tumour segmentation results using a pretrained model used in the original paper by Bilal _et al_. In particular, this model is a ResNet model with 18 layers (resnet18). A prediction is made for each input patch, which denotes the probability of being tumour.\n",
    "\n",
    "> More information on the model and the dataset used for training can be found [here](https://www.thelancet.com/journals/landig/article/PIIS2589-7500&#40;2100180-1/fulltext).\n",
    "\n",
    "In line with the patch prediction model provided in `tiatoolbox`, the tumour segmentation model can be applied to input patches, large images tiles or whole-slide images. In order to replicate the original pipeline, we choose to process a sample whole-slide image. It can be seen that we can perform inference on a WSI with minimal effort. First we create the `PatchPredictor` object, which denotes the pretrained model that we will use, along with other arguments, such as the batch size and number of workers. Then, we call the `predict` method to process the slide and return the results. More information on using the `PatchPredictor` functionality can be seen in the dedicated patch prediction notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 243,
     "referenced_widgets": [
      "5c737719ec54471199c42d7732df0f26",
      "df29a2b570704ad2a96b776f3a352f16",
      "501c86f107be49cbb10aa522ad7e3b0f",
      "1ce0a19aecb5468290e21172ceacf2ca",
      "841bef2fbe374fd980e37b945ebe421a",
      "20e3921b29b845ec9448f0ee7f556e6d",
      "f2ff287229d246d197eac72ede439582",
      "c031de5cba4d4e00883b5781cb3244e2",
      "513665800fc6407b81722fee8e37dd1f",
      "d621c61ed1844246ba9c0e67ac0160f6",
      "8cc718e270864e38a9c601cc43e8362a"
     ]
    },
    "id": "irBRlF2_JLPj",
    "outputId": "4ac1da86-edd3-4859-b568-322c7488281f"
   },
   "outputs": [],
   "source": [
    "wsi_file_list = [\n",
    "    wsi_file_name\n",
    "]  # the list of WSIs to process- in this example we just use a single WSI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 243,
     "referenced_widgets": [
      "5c737719ec54471199c42d7732df0f26",
      "df29a2b570704ad2a96b776f3a352f16",
      "501c86f107be49cbb10aa522ad7e3b0f",
      "1ce0a19aecb5468290e21172ceacf2ca",
      "841bef2fbe374fd980e37b945ebe421a",
      "20e3921b29b845ec9448f0ee7f556e6d",
      "f2ff287229d246d197eac72ede439582",
      "c031de5cba4d4e00883b5781cb3244e2",
      "513665800fc6407b81722fee8e37dd1f",
      "d621c61ed1844246ba9c0e67ac0160f6",
      "8cc718e270864e38a9c601cc43e8362a"
     ]
    },
    "id": "irBRlF2_JLPj",
    "outputId": "4ac1da86-edd3-4859-b568-322c7488281f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Download from https://tiatoolbox.dcs.warwick.ac.uk/models/idars/resnet18-idars-tumour.pth\n",
      "Save to /Users/dbae/.tiatoolbox/models/resnet18-idars-tumour.pth\n"
     ]
    }
   ],
   "source": [
    "tumour_predictor = PatchPredictor(\n",
    "    pretrained_model=\"resnet18-idars-tumour\", batch_size=64,\n",
    "        num_loader_workers=WORKERS\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 243,
     "referenced_widgets": [
      "5c737719ec54471199c42d7732df0f26",
      "df29a2b570704ad2a96b776f3a352f16",
      "501c86f107be49cbb10aa522ad7e3b0f",
      "1ce0a19aecb5468290e21172ceacf2ca",
      "841bef2fbe374fd980e37b945ebe421a",
      "20e3921b29b845ec9448f0ee7f556e6d",
      "f2ff287229d246d197eac72ede439582",
      "c031de5cba4d4e00883b5781cb3244e2",
      "513665800fc6407b81722fee8e37dd1f",
      "d621c61ed1844246ba9c0e67ac0160f6",
      "8cc718e270864e38a9c601cc43e8362a"
     ]
    },
    "id": "irBRlF2_JLPj",
    "outputId": "4ac1da86-edd3-4859-b568-322c7488281f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "|2022-07-20|13:15:11.025| [WARNING] /Users/dbae/opt/miniconda3/envs/tiatoolbox-dev/lib/python3.10/site-packages/tiatoolbox/wsicore/wsireader.py:298: UserWarning: Read: Scale > 1.This means that the desired resolution is higher than the WSI baseline (maximum encoded resolution). Interpolation of read regions may occur.\n",
      "  warnings.warn(\n",
      "\n",
      " 67%|#########################3            | 16/24 [2:32:12<1:04:13, 481.72s/it]"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [17]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m tumour_output \u001b[38;5;241m=\u001b[39m \u001b[43mtumour_predictor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mimgs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwsi_file_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mwsi\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_probabilities\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mon_gpu\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mON_GPU\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/tiatoolbox-dev/lib/python3.10/site-packages/tiatoolbox/models/engine/patch_predictor.py:683\u001b[0m, in \u001b[0;36mPatchPredictor.predict\u001b[0;34m(self, imgs, masks, labels, mode, return_probabilities, return_labels, on_gpu, ioconfig, patch_input_shape, stride_shape, resolution, units, merge_predictions, save_dir, save_output)\u001b[0m\n\u001b[1;32m    672\u001b[0m img_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m masks \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m masks[idx]\n\u001b[1;32m    674\u001b[0m dataset \u001b[38;5;241m=\u001b[39m WSIPatchDataset(\n\u001b[1;32m    675\u001b[0m     img_path,\n\u001b[1;32m    676\u001b[0m     mode\u001b[38;5;241m=\u001b[39mmode,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    681\u001b[0m     units\u001b[38;5;241m=\u001b[39mioconfig\u001b[38;5;241m.\u001b[39minput_resolutions[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124munits\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m    682\u001b[0m )\n\u001b[0;32m--> 683\u001b[0m output_model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_predict_engine\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    684\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    685\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_labels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    686\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_probabilities\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_probabilities\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    687\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_coordinates\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_coordinates\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    688\u001b[0m \u001b[43m    \u001b[49m\u001b[43mon_gpu\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mon_gpu\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    689\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    690\u001b[0m output_model[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m img_label\n\u001b[1;32m    691\u001b[0m \u001b[38;5;66;03m# add extra information useful for downstream analysis\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/tiatoolbox-dev/lib/python3.10/site-packages/tiatoolbox/models/engine/patch_predictor.py:423\u001b[0m, in \u001b[0;36mPatchPredictor._predict_engine\u001b[0;34m(self, dataset, return_probabilities, return_labels, return_coordinates, on_gpu)\u001b[0m\n\u001b[1;32m    415\u001b[0m cum_output \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    416\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprobabilities\u001b[39m\u001b[38;5;124m\"\u001b[39m: [],\n\u001b[1;32m    417\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpredictions\u001b[39m\u001b[38;5;124m\"\u001b[39m: [],\n\u001b[1;32m    418\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcoordinates\u001b[39m\u001b[38;5;124m\"\u001b[39m: [],\n\u001b[1;32m    419\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m\"\u001b[39m: [],\n\u001b[1;32m    420\u001b[0m }\n\u001b[1;32m    421\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _, batch_data \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(dataloader):\n\u001b[0;32m--> 423\u001b[0m     batch_output_probabilities \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minfer_batch\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    424\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_data\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mimage\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mon_gpu\u001b[49m\n\u001b[1;32m    425\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    426\u001b[0m     \u001b[38;5;66;03m# We get the index of the class with the maximum probability\u001b[39;00m\n\u001b[1;32m    427\u001b[0m     batch_output_predictions \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mpostproc_func(\n\u001b[1;32m    428\u001b[0m         batch_output_probabilities\n\u001b[1;32m    429\u001b[0m     )\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/tiatoolbox-dev/lib/python3.10/site-packages/tiatoolbox/models/architecture/vanilla.py:152\u001b[0m, in \u001b[0;36mCNNModel.infer_batch\u001b[0;34m(model, batch_data, on_gpu)\u001b[0m\n\u001b[1;32m    150\u001b[0m \u001b[38;5;66;03m# Do not compute the gradient (not training)\u001b[39;00m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39minference_mode():\n\u001b[0;32m--> 152\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg_patches_device\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    153\u001b[0m \u001b[38;5;66;03m# Output should be a single tensor or scalar\u001b[39;00m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/tiatoolbox-dev/lib/python3.10/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/tiatoolbox-dev/lib/python3.10/site-packages/tiatoolbox/models/architecture/vanilla.py:110\u001b[0m, in \u001b[0;36mCNNModel.forward\u001b[0;34m(self, imgs)\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, imgs):\n\u001b[1;32m    103\u001b[0m     \u001b[38;5;124;03m\"\"\"Pass input data through the model.\u001b[39;00m\n\u001b[1;32m    104\u001b[0m \n\u001b[1;32m    105\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    108\u001b[0m \n\u001b[1;32m    109\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 110\u001b[0m     feat \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeat_extract\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimgs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    111\u001b[0m     gap_feat \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpool(feat)\n\u001b[1;32m    112\u001b[0m     gap_feat \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mflatten(gap_feat, \u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/tiatoolbox-dev/lib/python3.10/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/tiatoolbox-dev/lib/python3.10/site-packages/torch/nn/modules/container.py:139\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    137\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    138\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 139\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    140\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/tiatoolbox-dev/lib/python3.10/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/tiatoolbox-dev/lib/python3.10/site-packages/torch/nn/modules/container.py:139\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    137\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    138\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 139\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    140\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/tiatoolbox-dev/lib/python3.10/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/tiatoolbox-dev/lib/python3.10/site-packages/torchvision/models/resnet.py:92\u001b[0m, in \u001b[0;36mBasicBlock.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m     90\u001b[0m     identity \u001b[38;5;241m=\u001b[39m x\n\u001b[0;32m---> 92\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     93\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbn1(out)\n\u001b[1;32m     94\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelu(out)\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/tiatoolbox-dev/lib/python3.10/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/tiatoolbox-dev/lib/python3.10/site-packages/torch/nn/modules/conv.py:457\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    456\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 457\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/tiatoolbox-dev/lib/python3.10/site-packages/torch/nn/modules/conv.py:453\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    449\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    450\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(F\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode),\n\u001b[1;32m    451\u001b[0m                     weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[1;32m    452\u001b[0m                     _pair(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n\u001b[0;32m--> 453\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    454\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "tumour_output = tumour_predictor.predict(\n",
    "    imgs=wsi_file_list, mode=\"wsi\", return_probabilities=True, on_gpu=ON_GPU\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Eoedk3ryW6V4"
   },
   "source": [
    "As can be seen above, with just a few lines of code we are able to perform tumour segmentation on whole-slide images. If running the prediction like we did above, then the default parameters for `patch_shape`, `stride_shape` and `resolution` will be used. Here, `patch_shape`, and `resolution` are in line with what was used for training the model, whereas `stride_shape` is set to be equal to the `patch_shape` (no overlap). In particular, the input patch size is 512x512 and the processing resolution is 0.5 microns per pixel (~20x objective magnification).\n",
    "\n",
    "If you want to change the default parameters, you will need to define this using `IOPatchPredictorConfig`. For example, you may want to run with 50% overlap between neighbouring patches. Therefore, you would define:\n",
    "\n",
    "```\n",
    "wsi_ioconfig = IOPatchPredictorConfig(\n",
    "    input_resolutions=[{'units': 'mpp', 'resolution': 0.5}],\n",
    "    patch_input_shape=[512, 512],\n",
    "    stride_shape=[256, 256],\n",
    ")\n",
    "```\n",
    "\n",
    "Then, you would add this in the `predict` method as follows:\n",
    "\n",
    "```\n",
    "tumour_output = tumour_predictor.predict(\n",
    "    imgs=[wsi_file_name],\n",
    "    mode='wsi',\n",
    "    return_probabilities=True,\n",
    "    on_gpu=True\n",
    "    ioconfig=wsi_ioconfig)\n",
    "```\n",
    "\n",
    "Below, we show how to merge the output predictions to form a 2-dimensional prediction map, denoting areas predicted as tumour. This prediction map is used in the second step of the pipeline, where we use only patches containing tumour."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "smm4ZIlMW6V5",
    "outputId": "08dc3b84-f7a0-40ae-f225-cbec7899fa6f"
   },
   "outputs": [],
   "source": [
    "overview_resolution = 1.25  # the resolution in which we desire to merge and visualize the patch predictions\n",
    "overview_unit = \"power\"  # the unit of the `resolution` parameter. Can be \"power\", \"level\", \"mpp\", or \"baseline\"\n",
    "\n",
    "# merge predictions to form a 2-dimensional output at the desired resolution\n",
    "tumour_mask = tumour_predictor.merge_predictions(\n",
    "    wsi_file_name, tumour_output[0], resolution=overview_resolution, units=overview_unit\n",
    ")\n",
    "\n",
    "# the output map will contain values from 0 to 2.\n",
    "# 0: background that is not processed, 1: non-tumour prediction and 2 is tumour predictions\n",
    "tumour_mask = tumour_mask == 2  # binarise the output\n",
    "\n",
    "# let's save the tumour mask, so that we can use it in stage 2!\n",
    "imwrite(\"tumour_mask.png\", tumour_mask.astype(\"uint8\") * 255)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ubz9041RW6V5"
   },
   "source": [
    "Now that we have merged the prediction, let's visualise the results!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 539
    },
    "id": "5Khdr4T1W6V5",
    "outputId": "12a9e4fe-bad1-471f-d291-d3f2495f9070"
   },
   "outputs": [],
   "source": [
    "from tiatoolbox.utils.visualization import overlay_prediction_mask\n",
    "\n",
    "# first read the WSI at a low-resolution. Here, we use the same resolution that was used when merging the patch-level results.\n",
    "wsi = WSIReader.open(wsi_file_name)\n",
    "wsi_overview = wsi.slide_thumbnail(resolution=overview_resolution, units=overview_unit)\n",
    "\n",
    "# [Overlay map creation]\n",
    "# creating label-color dictionary to be fed into `overlay_prediction_mask` function to help generate a color legend\n",
    "label_dict = {\"Non-Tumour\": 0, \"Tumour\": 1}\n",
    "label_color_dict = {}\n",
    "colors = [[255, 255, 255], [255, 0, 0]]  # defining colours for overlay (white and red)\n",
    "for class_name, label in label_dict.items():\n",
    "    label_color_dict[label] = (class_name, np.array(colors[label]))\n",
    "\n",
    "overlay = overlay_prediction_mask(\n",
    "    wsi_overview, tumour_mask, alpha=0.5, label_info=label_color_dict\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e7JkXBtAETIn"
   },
   "source": [
    "## WSI prediction using TIAToolbox pretrained models\n",
    "\n",
    "Next, we show how one can use a second CNN that takes as input the results obtained from part 1 and gives a prediction for each tumour patch in the input WSI. In the original paper, 4 fold cross validation was used. In the toolbox, we choose to provide models trained on the first fold of the data, which enables us to cross check the results and ensure that they are in line with the original work. All models in the second step of the pipeline are ResNet34.\n",
    "\n",
    "To speed up this step, we choose to retrain the models without stain normalization (as oppoed to the original IDaRS) and instead with colour jitter. We report the difference in results at the end of this notebook.\n",
    "\n",
    "Near the beginning of the notebook, we mention which prediction tasks we consider. Next, we will run inference for all of these prediction tasks and visualise the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 451,
     "referenced_widgets": [
      "72859345f3b14dc8b2a3494cb6b3a94b",
      "0472aa7dd7ac420984db44f487c70461",
      "7627208469ad49ea8935cc7771d1d822",
      "2a85925d02bc490e985e9450564df66d",
      "72c36c9497b940bc845a16475c0770e7",
      "94521a8a76bb42169bcc2780316ee33a",
      "0a447af25e094ed694f9380edde29c0d",
      "b6a5f1ce8f944ffc988bbe9aaabb9cd5",
      "982c738965a44dc3accd6e65d64f29e9",
      "14f354f25fb34296892d667d331812bb",
      "afc7df50726948bb8b4486b7d5912da9"
     ]
    },
    "id": "hk-FzJmDASHo",
    "outputId": "a471558f-ef5c-4e62-97e3-6c8c2a2888a1"
   },
   "outputs": [],
   "source": [
    "# Run inference for each of the 6 WSI prediction tasks\n",
    "prediction_tasks = [\"msi\", \"braf\", \"cimp\", \"cin\", \"hm\", \"tp53\"]\n",
    "\n",
    "# iterate over each of the prediction tasks and add the results to a dictionary\n",
    "wsi_output_dict = {}\n",
    "for task in prediction_tasks:\n",
    "    wsi_predictor = PatchPredictor(\n",
    "        pretrained_model=\"resnet34-idars-%s\" % task, batch_size=64, num_loader_workers=WORKERS\n",
    "    )\n",
    "\n",
    "    # we include the obtained tumour mask from stage 1 as an argument so that we only process patches from those regions.\n",
    "    wsi_output_dict[task] = wsi_predictor.predict(\n",
    "        imgs=wsi_file_list,\n",
    "        masks=[\"tumour_mask.png\"],\n",
    "        mode=\"wsi\",\n",
    "        return_probabilities=True,\n",
    "        on_gpu=ON_GPU,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6mpjnja9W6V6"
   },
   "source": [
    "Now that we have performed patch-level classification for each tumour patch, let's merge together the results to get the 2D prediction map. Here, we return the raw probability map, rather than the class predictions. We do this because the final step in the IDaRS pipeline uses the raw patch-level probabilities to classify each WSI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lXi9EFaIW6V6",
    "outputId": "d2b2515a-da68-49d2-adfd-e1364060c557"
   },
   "outputs": [],
   "source": [
    "overview_resolution = 1.25  # the resolution in which we desire to merge and visualize the patch predictions\n",
    "overview_unit = \"power\"  # the unit of the `resolution` parameter. Can be \"power\", \"level\", \"mpp\", or \"baseline\"\n",
    "\n",
    "# merge predictions to form a 2-dimensional output at the desired resolution\n",
    "merged_output_dict = {}\n",
    "for task in prediction_tasks:\n",
    "    merged_output = PatchPredictor.merge_predictions(\n",
    "        wsi_file_name,\n",
    "        wsi_output_dict[task][0],\n",
    "        resolution=overview_resolution,\n",
    "        units=overview_unit,\n",
    "        return_raw=True,\n",
    "    )\n",
    "    merged_output_dict[task] = merged_output[..., 1]  # consider only the positive class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6yrlQAZpW6V7"
   },
   "source": [
    "Now that we have merged the predictions to obtain the probability map, let's visualise the results. This is very similar to the `overlay_patch_prediction` function, but `label_info` does not need to be provided because we only have one class. Also, `min_val` can be provided. This is a number between 0 and 1 which ensures that the probability map is only shown for values greater than `min_val`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 858
    },
    "id": "vuvLh0edW6V7",
    "nbsphinx-thumbnail": {
     "output-index": -1
    },
    "outputId": "8216c92f-5c13-4304-8f62-ed11898c7345"
   },
   "outputs": [],
   "source": [
    "from tiatoolbox.utils.visualization import overlay_probability_map\n",
    "\n",
    "# [Overlay map creation]\n",
    "# iterate over the tasks and generate the probability map\n",
    "overlay_list = []\n",
    "for task in prediction_tasks:\n",
    "    # only show the probability map when it is greater than `min_val` (0.1 is used here)\n",
    "    overlay_prob_map = overlay_probability_map(\n",
    "        wsi_overview, merged_output_dict[task], alpha=0.5, min_val=0.1, return_ax=False\n",
    "    )\n",
    "    overlay_list.append(overlay_prob_map)\n",
    "\n",
    "# visualise the overlays in a single plot\n",
    "fig = plt.figure(figsize=(14, 7))\n",
    "for i in range(6):\n",
    "    ax = plt.subplot(2, 3, i + 1)\n",
    "    plt.imshow(overlay_list[i])\n",
    "    plt.axis(\"off\")\n",
    "    plt.title(prediction_tasks[i].upper())\n",
    "fig.suptitle(\"Probability Map Overlay for Each Task\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KO6L_ZsOW6V7"
   },
   "source": [
    "These probability maps can be used to increase the interpretability of results and help identify regions contributing to the overall prediction. To get a smoother output, you can increase the overlap by modifying `IOPatchPredictorConfig`.\n",
    "\n",
    "Let's visualise some patches that have either a high probability or a low probability of being MSI positive. For this, we randomly sample 4 patches with probability of being MSI > 0.95 and 4 patches with probability of being MSI < 0.05."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Rr-op06RW6V7"
   },
   "outputs": [],
   "source": [
    "# get the probabilities of each processed patch being MSI\n",
    "msi_probabilities = wsi_output_dict[\"msi\"][0][\"probabilities\"]\n",
    "msi_probabilities = np.array(msi_probabilities)[..., 1]\n",
    "\n",
    "# get the coordinates of each processed patch\n",
    "msi_coordinates = wsi_output_dict[\"msi\"][0][\"coordinates\"]\n",
    "msi_coordinates = np.array(msi_coordinates)\n",
    "\n",
    "# subset where MSI probability is greater than 0.95\n",
    "msi_probabilities_subset_pos = msi_probabilities[msi_probabilities > 0.95]\n",
    "msi_coordinates_subset_pos = msi_coordinates[msi_probabilities > 0.95]\n",
    "\n",
    "# subset where MSI probability is less than than 0.05\n",
    "msi_probabilities_subset_neg = msi_probabilities[msi_probabilities > 0.05]\n",
    "msi_coordinates_subset_neg = msi_coordinates[msi_probabilities > 0.05]\n",
    "msi_coordinates_subset = list(msi_coordinates_subset_pos) + list(\n",
    "    msi_coordinates_subset_neg\n",
    ")\n",
    "\n",
    "# randomly sample 4 positive and 4 negative\n",
    "random_pos_idx = np.random.randint(0, msi_probabilities_subset_pos.shape[0], size=4)\n",
    "random_neg_idx = np.random.randint(0, msi_probabilities_subset_neg.shape[0], size=4)\n",
    "random_idx = list(random_pos_idx) + list(random_neg_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9ObSUui4W6V8"
   },
   "source": [
    "Now that we have obtained our random sample, let's plot the patches! First, we use TIAToolbox's `WSIReader` to read the original WSI and extract patches at defined locations. For detailed information on this, see the relevant example notebook!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 446
    },
    "id": "2XUW0sI2W6V8",
    "outputId": "6d019d1a-2f1b-41e7-de09-b78eac123876"
   },
   "outputs": [],
   "source": [
    "# read the WSI and get the resolution used during processing.\n",
    "wsi_reader = WSIReader.open(input_img=wsi_file_name)\n",
    "resolution = wsi_output_dict[\"msi\"][0][\"resolution\"]\n",
    "units = wsi_output_dict[\"msi\"][0][\"units\"]\n",
    "\n",
    "# visualise the overlays in a single plot\n",
    "fig = plt.figure()\n",
    "for i in range(8):\n",
    "    ax = plt.subplot(2, 4, i + 1)\n",
    "    coords = msi_coordinates_subset[random_idx[i]]\n",
    "    size = (\n",
    "        coords[2] - coords[0],\n",
    "        coords[3] - coords[1],\n",
    "    )  # determine the size of patch from the coordinates\n",
    "    # get the patch\n",
    "    patch = wsi_reader.read_rect(\n",
    "        (coords[0] * 4, coords[1] * 4), size, resolution=resolution, units=units\n",
    "    )\n",
    "    plt.imshow(patch)\n",
    "    plt.axis(\"off\")\n",
    "    if i < 4:\n",
    "        plt.title(\"MSI\")\n",
    "    else:\n",
    "        plt.title(\"MSS\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "itcxcQiZW6V8"
   },
   "source": [
    "The probability maps are not directly needed to obtain the WSI-level score. Below, we will demonstrate how one can obtain the WSI score utilising the output from the `PatchPredictor` in step 2. For the purpose of this example, we will show how to use mean and max aggregation to obtain a slide-level prediction score.\n",
    "\n",
    "First, let's examine the format of the dictionary output by `PatchPredictor`. We look only at the MSI output as an example, but the output from each task is similar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Tx5-ApPVW6V8",
    "outputId": "fed388f0-181a-4761-bd28-619b04d94f29"
   },
   "outputs": [],
   "source": [
    "# get the keys of the dictionary returned at the output of the MSI CNNPatchPredictor\n",
    "print(\"Output of PatchPredictor:\")\n",
    "print(list(wsi_output_dict[\"msi\"][0].keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hdJKoPmrW6V9"
   },
   "source": [
    "As we can see, the probabilities are returned as output. Now, let's compute, for each task, the average and maximum over all probabilities of tumour tiles being positive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0RgnSFwVW6V9",
    "outputId": "f6867d6d-03fa-4a69-b468-2e759748d6be"
   },
   "outputs": [],
   "source": [
    "slide_score_mean = {}\n",
    "slide_score_max = {}\n",
    "# iterate over the tasks\n",
    "for task in prediction_tasks:\n",
    "    probabilities = np.array(wsi_output_dict[task][0][\"probabilities\"])[\n",
    "        ..., 1\n",
    "    ]  # only consider positive class\n",
    "    slide_score_mean[task] = np.mean(\n",
    "        probabilities\n",
    "    )  # get the average probability over all tumour tiles\n",
    "    slide_score_max[task] = np.max(\n",
    "        probabilities\n",
    "    )  # get the maximum probability over all tumour tiles\n",
    "\n",
    "\n",
    "# print the scores\n",
    "print(\"MEAN AGGREGATION\")\n",
    "for task, value in slide_score_mean.items():\n",
    "    print(task, \":\", value)\n",
    "\n",
    "print(\"-\" * 30)\n",
    "\n",
    "print(\"MAX AGGREGATION\")\n",
    "for task, value in slide_score_max.items():\n",
    "    print(task, \":\", value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3_sFqMptW6V9"
   },
   "source": [
    "As can be seen, it is straight forward to go from the WSI output to the WSI-level prediction score. In a similar way, different aggregation methods can be used, such as top k probabilities. To highlight how easy it is to obtain the slide level score using TIAToolbox, we provide below the code required to go from input to output.\n",
    "\n",
    "```\n",
    "# TUMOUR DETECTION\n",
    "tumour_predictor = PatchPredictor(pretrained_model='resnet18-idars-tumour', batch_size=64)\n",
    "\n",
    "tumour_output = tumour_predictor.predict(\n",
    "    imgs=[wsi_file_name],\n",
    "    mode='wsi',\n",
    "    return_probabilities=True,\n",
    "    on_gpu=True)\n",
    "\n",
    "tumour_mask = tumour_predictor.merge_predictions(wsi_file_name, tumour_output[0], resolution=overview_resolution, units=overview_unit)\n",
    "tumour_mask = tumour_mask == 2 # binarise the output\n",
    "imwrite('tumour_mask.png', tumour_mask.astype('uint8') * 255)\n",
    "\n",
    "# WSI PREDICTION\n",
    "msi_predictor = PatchPredictor(pretrained_model='resnet34-idars-msi', batch_size=64)\n",
    "\n",
    "msi_output = msi_predictor.predict(\n",
    "    imgs=[wsi_file_name],\n",
    "    masks=['tumour_mask.png'],\n",
    "    mode='wsi',\n",
    "    return_probabilities=True,\n",
    "    on_gpu=True)\n",
    "\n",
    "# SLIDE-LEVEL SCORE\n",
    "msi_probabilities = np.array(msi_output[0]['probabilities'])[...,1] # only consider MSI class\n",
    "average_msi_probability = np.mean(msi_probabilities) # get the average over all tumour tiles\n",
    "```\n",
    "\n",
    "We encourage you to play around with the models provided by TIAToolbox for prediction of molecular pathways and key mutations. To make a different WSI level prediction, all you need to do is use a different task-specific pretrained weights in the second `PatchPredictor`.\n",
    "\n",
    "Below, we report the results obtained with our retrained model compared to the original results. For the purpose of this, we only consider mean aggregation for each task. All results show the area under the receiver operating characteristic curve (AUROC).\n",
    "\n",
    "Task |Original | TIAToolbox  |\n",
    "-----|---------|-------------|\n",
    "MSI  | 0.83    | 0.87        |\n",
    "TP53 | 0.76    | 0.75        |\n",
    "BRAF | 0.81    | 0.75        |\n",
    "CIMP | 0.85    | 0.75        |\n",
    "CIN  | 0.85    | 0.81        |\n",
    "HM   | 0.86    | 0.79        |\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "celltoolbar": "Edit Metadata",
  "colab": {
   "collapsed_sections": [],
   "name": "inference-pipelines-idars.ipynb",
   "provenance": []
  },
  "interpreter": {
   "hash": "20f080c2d3cdb15ba1f46d1d9775b5885435ab326e3af81a061f3ec6f66f8478"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "0472aa7dd7ac420984db44f487c70461": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "0a447af25e094ed694f9380edde29c0d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "14f354f25fb34296892d667d331812bb": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "1ce0a19aecb5468290e21172ceacf2ca": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_513665800fc6407b81722fee8e37dd1f",
      "max": 46830571,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_c031de5cba4d4e00883b5781cb3244e2",
      "value": 46830571
     }
    },
    "20e3921b29b845ec9448f0ee7f556e6d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "2a85925d02bc490e985e9450564df66d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_982c738965a44dc3accd6e65d64f29e9",
      "max": 87319819,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_b6a5f1ce8f944ffc988bbe9aaabb9cd5",
      "value": 87319819
     }
    },
    "501c86f107be49cbb10aa522ad7e3b0f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_f2ff287229d246d197eac72ede439582",
      "placeholder": "",
      "style": "IPY_MODEL_20e3921b29b845ec9448f0ee7f556e6d",
      "value": "100%"
     }
    },
    "513665800fc6407b81722fee8e37dd1f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5c737719ec54471199c42d7732df0f26": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_501c86f107be49cbb10aa522ad7e3b0f",
       "IPY_MODEL_1ce0a19aecb5468290e21172ceacf2ca",
       "IPY_MODEL_841bef2fbe374fd980e37b945ebe421a"
      ],
      "layout": "IPY_MODEL_df29a2b570704ad2a96b776f3a352f16"
     }
    },
    "72859345f3b14dc8b2a3494cb6b3a94b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_7627208469ad49ea8935cc7771d1d822",
       "IPY_MODEL_2a85925d02bc490e985e9450564df66d",
       "IPY_MODEL_72c36c9497b940bc845a16475c0770e7"
      ],
      "layout": "IPY_MODEL_0472aa7dd7ac420984db44f487c70461"
     }
    },
    "72c36c9497b940bc845a16475c0770e7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_afc7df50726948bb8b4486b7d5912da9",
      "placeholder": "",
      "style": "IPY_MODEL_14f354f25fb34296892d667d331812bb",
      "value": " 83.3M/83.3M [00:00&lt;00:00, 112MB/s]"
     }
    },
    "7627208469ad49ea8935cc7771d1d822": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_0a447af25e094ed694f9380edde29c0d",
      "placeholder": "",
      "style": "IPY_MODEL_94521a8a76bb42169bcc2780316ee33a",
      "value": "100%"
     }
    },
    "841bef2fbe374fd980e37b945ebe421a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_8cc718e270864e38a9c601cc43e8362a",
      "placeholder": "",
      "style": "IPY_MODEL_d621c61ed1844246ba9c0e67ac0160f6",
      "value": " 44.7M/44.7M [00:00&lt;00:00, 108MB/s]"
     }
    },
    "8cc718e270864e38a9c601cc43e8362a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "94521a8a76bb42169bcc2780316ee33a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "982c738965a44dc3accd6e65d64f29e9": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "afc7df50726948bb8b4486b7d5912da9": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b6a5f1ce8f944ffc988bbe9aaabb9cd5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "c031de5cba4d4e00883b5781cb3244e2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "d621c61ed1844246ba9c0e67ac0160f6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "df29a2b570704ad2a96b776f3a352f16": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f2ff287229d246d197eac72ede439582": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
