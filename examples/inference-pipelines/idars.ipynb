{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8Kag0_9eJLPd"
   },
   "source": [
    "Prediction of Molecular Pathways and Key Mutations\n",
    "==================================================================\n",
    "\n",
    "Click to open in: [[GitHub](https://github.com/TissueImageAnalytics/tiatoolbox/tree/master/examples/inference-pipelines/idars.ipynb)][[Colab](https://colab.research.google.com/github/TissueImageAnalytics/tiatoolbox/blob/master/examples/inference-pipelines/idars.ipynb)][[Kaggle](https://kaggle.com/kernels/welcome?src=https://github.com/TissueImageAnalytics/tiatoolbox/blob/master/examples/inference-pipelines/idars.ipynb)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbsphinx": "hidden"
   },
   "source": [
    "*In order to run this notebook on a Kaggle platform, 1) click on the blue checkbox* \"[Kaggle]\" *above, 2) click on Settings on the right of the Kaggle screen, 3) log in to your Kaggle account, 4) tick the* \"Internet\" *checkbox under Settings, to enable necessary downloads.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbsphinx": "hidden"
   },
   "source": [
    "About this notebook\n",
    "------------------------------------\n",
    "This jupyter notebook can be run on any computer with a standard browser and no prior installation of any programming language is required. It can run remotely over the Internet, free of charge, thanks to Google Colaboratory or Kaggle. To connect with Colab or Kaggle, click on one of the two blue checkboxes above. Check that \"colab\" or \"kaggle\", as appropriate, appears in the address bar. If the left click does not work, you can right-click on the blue checkbox \"[Colab]\" above and select \"Open in new tab\". Familiarize yourself with the drop-down menus near the top of the window. You can edit the notebook during the session, for example substituting your own image files for the image files used in this demo. Experiment by changing the parameters of functions. It is not possible for an ordinary user to permanently change this version of the notebook on Github, Colab or Kaggle, so you cannot inadvertently mess it up. Use the notebook's File Menu if you wish to save your own (changed) notebook.\n",
    "\n",
    "To run the notebook on any platform, except for Colab or Kaggle, set up your Python environment, as explained in the\n",
    "[README](https://github.com/TIA-Lab/tiatoolbox/blob/master/README.md#install-python-package) file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p3l08vukKAWo"
   },
   "source": [
    "About this demo\n",
    "-------------------------------------\n",
    "Prediction of molecular pathways and key mutations directly from Haematoxylin and Eosin stained histology images may help bypass additional genetic (e.g., polymerase chain reaction or PCR) or immunohistochemistry (IHC) testing, which may therefore save both money and time.\n",
    "\n",
    "In this example notebook, we show how you can use pretrained models to reproduce inference results such as those described \n",
    "[here](https://www.thelancet.com/journals/landig/article/PIIS2589-7500&#40;2100180-1/fulltext)  (**Bilal et al.**, *\"Development and validation of a weakly supervised deep learning framework to predict the status of molecular pathways and key mutations in colorectal cancer from routine histology images: a retrospective study\"*).  The site also carries important *Supplementary Materials*. These sources give details of the models we use.\n",
    "\n",
    "This paper used a novel Iterative Draw and Rank Sampling (IDaRS) approach, consisting of two-stages:\n",
    "\n",
    "1. Patch-level tumour classification\n",
    "2. Patch-level WSI classification\n",
    "\n",
    "In stage 1, we use a pretrained tumour segmentation model to identify potentially diagnositc areas. In stage 2, we make a task-specific prediction for each tumour patch. For stage 2, the model is trained utilising only the slide-level label and hence the model does not require any detailed annotations at the cell or regional level. Here, representative tiles from each WSI are iteratively sampled, explaining the name IDaRS. A simple account of the theory, justifying IDaRS as an algorithm, is discussed in this [link](https://github.com/TissueImageAnalytics/tiatoolbox/blob/doc-idars-algorithm/examples/inference-pipelines/theory_idars.rst)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p3l08vukKAWo"
   },
   "source": [
    "Available models\n",
    "---------------------------------\n",
    "In TIAToolbox, we include models that are capable of predicting:\n",
    "\n",
    "    * Microsatellite instability (MSI)  \n",
    "    * Hypermutation density  \n",
    "    * Chromosomal instability  \n",
    "    * CpG island methylator phenotype (CIMP)-high\n",
    "      prediction  \n",
    "    * BRAF mutation  \n",
    "    * TP53 mutation  \n",
    "\n",
    "In line with the above description, in TIAToolbox, we provide the following pretrained models used in the [original publication](https://www.thelancet.com/journals/landig/article/PIIS2589-7500&#40;2100180-1/fulltext):\n",
    "\n",
    "    * Tumour segmentation  \n",
    "        * `resnet18-idars-tumour`  \n",
    "    * Task specific prediction  \n",
    "        * MSI: `resnet34-idars-msi`  \n",
    "        * Hypermutation density: `resnet34-idars-hm`  \n",
    "        * Chromosomal instability: `resnet34-idars-cin`    \n",
    "        * CpG island methylator phenotype (CIMP)-high\n",
    "          prediction: `resnet34-idars-cimp`    \n",
    "        * BRAF mutation: `resnet34-idars-braf`    \n",
    "        * TP53 mutation: `resnet34-idars-tp53`    \n",
    "\n",
    "The provided models are trained on the first fold used in the original paper."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Jrx0GDVdXnfg",
    "nbsphinx": "hidden"
   },
   "source": [
    "Setting up the environment\n",
    "--------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MxofyzydW6Vx",
    "nbsphinx": "hidden"
   },
   "source": [
    "If on Colab or Kaggle or any other platform with Cuda-enabled GPU, you can substantially speed up execution by using the`torch` package and a GPU. This is the default, with `ON_GPU==True`. If you can't or do not want to do this, set `ON_GPU=False`, in which case, the notebook will probably take several hours to complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZA_cK8VFW6Vy",
    "nbsphinx": "hidden"
   },
   "outputs": [],
   "source": [
    "ON_GPU = True # change to False to run (slowly) without GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BkeTbaX_mo7Z",
    "nbsphinx": "hidden"
   },
   "source": [
    "You can skip the following cell if 1) you are not using the Colab or Kaggle plaforms or 2) you are using Colab or Kaggle and this is not your first run of the notebook in the current session. If you nevertheless run the cell, you may get an error message, but no harm will be done. On Colab or Kaggle the cell installs `tiatoolbox`, and other prerequisite software. Harmless error messages should be ignored. Outside Colab or Kaggle, the notebook expects `tiatoolbox` to already be installed. (See the instructions in [README](https://github.com/TIA-Lab/tiatoolbox/blob/master/README.md#install-python-package).)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oCOSzUCUXnfh",
    "nbsphinx": "hidden",
    "outputId": "4f06aa6a-69c7-4fb9-b124-d1f10b1527ce"
   },
   "outputs": [],
   "source": [
    "!apt-get -y install libopenjp2-7-dev libopenjp2-tools openslide-tools | tail --line 1\n",
    "!pip install tiatoolbox | tail --line 1\n",
    "print('Installation is done.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If `ON_GPU` is `True` install the torch package. Disregard errors reported by `pip`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KiNe6kd5W7QL",
    "nbsphinx": "hidden",
    "outputId": "5fff1e91-10ef-4cdd-bb6a-224213c4a221"
   },
   "outputs": [],
   "source": [
    "if ON_GPU:\n",
    "    !pip install torch==1.9.1+cu102 torchvision==0.10.1+cu102 -f https://download.pytorch.org/whl/cu102/torch_stable.html --user"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cell below removes the directory `tmp` if it existsâ€”a previous run may have created it.\n",
    "It creates a new tmp directory, for storing downloaded or newly created files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "![ -d tmp ] && ( echo \"deleting old tmp directory\"; rm -rf tmp )\n",
    "!( echo \"creating new tmp directory\"; mkdir tmp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hmJxBFzDJLPj"
   },
   "source": [
    "Importing related libraries\n",
    "------------------------------------------\n",
    "\n",
    "We import some standard Python modules, and also the Python module `wsireader` (see [details](https://github.com/TIA-Lab/tiatoolbox/blob/master/tiatoolbox/wsicore/wsireader.py)) written by the TIA Centre team."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UEIfjUTaJLPj"
   },
   "outputs": [],
   "source": [
    "from tiatoolbox.models.engine.patch_predictor import PatchPredictor\n",
    "from tiatoolbox.utils.misc import imwrite\n",
    "from tiatoolbox.wsicore.wsireader import get_wsireader\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "\n",
    "mpl.rcParams['figure.dpi'] = 140 # for high resolution figure in notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kA9ti0jNXniD"
   },
   "source": [
    "Downloading the required files\n",
    "--------------------------------------------------\n",
    "We download, over the internet, image files used for the purpose of this notebook. In particular, we download a whole slide image of cancerous colon tissue to highlight how the pipeline works.\n",
    "> In Colab, if you click the files icon (see below) in the vertical toolbar on the left hand side then you can see all the files which the code in this notebook can access. The data will appear here when it is downloaded.\n",
    ">\n",
    "> ![image.png](data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACQAAAAlCAYAAAAqXEs9AAAAwElEQVRYhe3WMQ6DMAyFYa7q1Yfw7Dl3ICusZM0hzJpDMLtTGSoFNy2UVPIvvf3DYsignTXcDXjNQVYOsnKQlYOsDkHjOCoiKgBUl3P+DWhZlkPIVagqaJqmt0EAoDFGnefZXEpJt227HtQyZv4chIjKzKeMiHZU7Uom6OhrWhORHSQiDnKQg/oChRD6AjGzg/4L9PyHiEjXdT1lKaUdVEppA7W8h1qHiNUrfv1ibB0RVa9jgu7IQVYOsnKQVXegB/ZWYoL8lUCBAAAAAElFTkSuQmCC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Rjw14DaJlO_o",
    "outputId": "d0d4f6bc-abc5-4003-953c-6e0465fb31c5"
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "wsi_file_name = \"sample_wsi.svs\"\n",
    "print('Download has started. Please wait...')\n",
    "\n",
    "# Downloading sample TCGA whole-slide image\n",
    "r = requests.get(\"https://tiatoolbox.dcs.warwick.ac.uk/sample_wsis/TCGA-AD-6964-01Z-00-DX1.83AF88B9-C59B-48C6-A739-85ACB8F8ECA9.svs\")\n",
    "with open(wsi_file_name, \"wb\") as f:\n",
    "    f.write(r.content)\n",
    "\n",
    "print('Download is complete.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fKterRqUKHnW"
   },
   "source": [
    "Tumour segmentation using TIAToolbox pretrained models\n",
    "--------------------------------------------------------------\n",
    "In this section, we will display patch-level tumour segmentation results using a pretrained model used in the original paper by Bilal _et al_. In particular, this model is a ResNet model with 18 layers (resnet18). A prediction is made for each input patch, which denotes the probability of being tumour.\n",
    "\n",
    "> More information on the model and the dataset used for training can be found <a href=\"https://www.thelancet.com/journals/landig/article/PIIS2589-7500(2100180-1/fulltext\">here</a> (_Bilal et al., \"Development and validation of a weakly supervised deep learning framework to predict the status of molecular pathways and key mutations in colorectal cancer from routine histology images: a retrospective study\"_)\n",
    "\n",
    "In line with the patch prediction model provided in `tiatoolbox`, the tumour segmentation model can be applied to input patches, large images tiles or whole-slide images. In order to replicate the original pipeline, we choose to process a sample whole-slide image. It can be seen that we can perform inference on a WSI with minimal effort. First we create the `PatchPredictor` object, which denotes the pretrained model that we will use, along with other arguments, such as the batch size and number of workers. Then, we call the `predict` method to process the slide and return the results. More information on using the `PatchPredictor` functionality can be seen in the dedicated patch prediction notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 243,
     "referenced_widgets": [
      "5c737719ec54471199c42d7732df0f26",
      "df29a2b570704ad2a96b776f3a352f16",
      "501c86f107be49cbb10aa522ad7e3b0f",
      "1ce0a19aecb5468290e21172ceacf2ca",
      "841bef2fbe374fd980e37b945ebe421a",
      "20e3921b29b845ec9448f0ee7f556e6d",
      "f2ff287229d246d197eac72ede439582",
      "c031de5cba4d4e00883b5781cb3244e2",
      "513665800fc6407b81722fee8e37dd1f",
      "d621c61ed1844246ba9c0e67ac0160f6",
      "8cc718e270864e38a9c601cc43e8362a"
     ]
    },
    "id": "irBRlF2_JLPj",
    "outputId": "4ac1da86-edd3-4859-b568-322c7488281f"
   },
   "outputs": [],
   "source": [
    "wsi_file_list = [wsi_file_name] # the list of WSIs to process- in this example we just use a single WSI.\n",
    "\n",
    "tumour_predictor = PatchPredictor(\n",
    "    pretrained_model='resnet18-idars-tumour',\n",
    "    batch_size=64,\n",
    "    num_loader_workers=8)\n",
    "\n",
    "tumour_output = tumour_predictor.predict(\n",
    "    imgs=wsi_file_list,\n",
    "    mode='wsi',\n",
    "    return_probabilities=True,\n",
    "    on_gpu=ON_GPU)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Eoedk3ryW6V4"
   },
   "source": [
    "As can be seen above, with just a few lines of code we are able to perform tumour segmentation on whole-slide images. If running the prediction like we did above, then the default parameters for `patch_shape`, `stride_shape` and `resolution` will be used. Here, `patch_shape`, and `resolution` are in line with what was used for training the model, whereas `stride_shape` is set to be equal to the `patch_shape` (no overlap). In particular, the input patch size is 512x512 and the processing resolution is 0.5 microns per pixel (~20x objective magnification).\n",
    "\n",
    "If you want to change the default parameters, you will need to define this using `IOPatchPredictorConfig`. For example, you may want to run with 50% overlap between neighbouring patches. Therefore, you would define:\n",
    "\n",
    "```\n",
    "wsi_ioconfig = IOPatchPredictorConfig(\n",
    "    input_resolutions=[{'units': 'mpp', 'resolution': 0.5}],\n",
    "    patch_input_shape=[512, 512],\n",
    "    stride_shape=[256, 256],\n",
    ")\n",
    "```\n",
    "\n",
    "Then, you would add this in the `predict` method as follows:\n",
    "\n",
    "```\n",
    "tumour_output = tumour_predictor.predict(\n",
    "    imgs=[wsi_file_name],\n",
    "    mode='wsi',\n",
    "    return_probabilities=True,\n",
    "    on_gpu=True\n",
    "    ioconfig=wsi_ioconfig)\n",
    "```\n",
    "\n",
    "Below, we show how to merge the output predictions to form a 2-dimensional prediction map, denoting areas predicted as tumour. This prediction map is used in the second step of the pipeline, where we use only patches containing tumour."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "smm4ZIlMW6V5",
    "outputId": "08dc3b84-f7a0-40ae-f225-cbec7899fa6f"
   },
   "outputs": [],
   "source": [
    "\n",
    "overview_resolution = 1.25 # the resolution in which we desire to merge and visualize the patch predictions\n",
    "overview_unit = \"power\" # the unit of the `resolution` parameter. Can be \"power\", \"level\", \"mpp\", or \"baseline\"\n",
    "\n",
    "# merge predictions to form a 2-dimensional output at the desired resolution\n",
    "tumour_mask = tumour_predictor.merge_predictions(\n",
    "    wsi_file_name, tumour_output[0],\n",
    "    resolution=overview_resolution,\n",
    "    units=overview_unit)\n",
    "\n",
    "# the output map will contain values from 0 to 2.\n",
    "# 0: background that is not processed, 1: non-tumour prediction and 2 is tumour predictions\n",
    "tumour_mask = tumour_mask == 2 # binarise the output\n",
    "\n",
    "# let's save the tumour mask, so that we can use it in stage 2!\n",
    "imwrite('tumour_mask.png', tumour_mask.astype('uint8') * 255)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ubz9041RW6V5"
   },
   "source": [
    "Now that we have merged the prediction, let's visualise the results!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 539
    },
    "id": "5Khdr4T1W6V5",
    "outputId": "12a9e4fe-bad1-471f-d291-d3f2495f9070"
   },
   "outputs": [],
   "source": [
    "from tiatoolbox.utils.visualization import overlay_prediction_mask\n",
    "\n",
    "# first read the WSI at a low-resolution. Here, we use the same resolution that was used when merging the patch-level results.\n",
    "wsi = get_wsireader(wsi_file_name)\n",
    "wsi_overview = wsi.slide_thumbnail(resolution=overview_resolution, units=overview_unit)\n",
    "\n",
    "# [Overlay map creation]\n",
    "# creating label-color dictionary to be fed into `overlay_prediction_mask` function to help generate a color legend\n",
    "label_dict = {'Non-Tumour': 0, 'Tumour': 1}\n",
    "label_color_dict = {}\n",
    "colors = [[255, 255, 255], [255, 0, 0]] # defining colours for overlay (white and red)\n",
    "for class_name, label in label_dict.items():\n",
    "    label_color_dict[label] = (class_name, np.array(colors[label]))\n",
    "\n",
    "overlay = overlay_prediction_mask(wsi_overview, tumour_mask, alpha=0.5, label_info=label_color_dict)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e7JkXBtAETIn"
   },
   "source": [
    "WSI prediction using TIAToolbox pretrained models\n",
    "------------------------------------------------------------------\n",
    "Next, we show how one can use a second CNN that takes as input the results obtained from part 1 and gives a prediction for each tumour patch in the input WSI. In the original paper, 4 fold cross validation was used. In the toolbox, we choose to provide models trained on the first fold of the data, which enables us to cross check the results and ensure that they are in line with the original work. All models in the second step of the pipeline are ResNet34.\n",
    "\n",
    "To speed up this step, we choose to retrain the models without stain normalization (as oppoed to the original IDaRS) and instead with colour jitter. We report the difference in results at the end of this notebook.\n",
    "\n",
    "Near the beginning of the notebook, we mention which prediction tasks we consider. Next, we will run inference for all of these prediction tasks and visualise the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 451,
     "referenced_widgets": [
      "72859345f3b14dc8b2a3494cb6b3a94b",
      "0472aa7dd7ac420984db44f487c70461",
      "7627208469ad49ea8935cc7771d1d822",
      "2a85925d02bc490e985e9450564df66d",
      "72c36c9497b940bc845a16475c0770e7",
      "94521a8a76bb42169bcc2780316ee33a",
      "0a447af25e094ed694f9380edde29c0d",
      "b6a5f1ce8f944ffc988bbe9aaabb9cd5",
      "982c738965a44dc3accd6e65d64f29e9",
      "14f354f25fb34296892d667d331812bb",
      "afc7df50726948bb8b4486b7d5912da9"
     ]
    },
    "id": "hk-FzJmDASHo",
    "outputId": "a471558f-ef5c-4e62-97e3-6c8c2a2888a1"
   },
   "outputs": [],
   "source": [
    "# Run inference for each of the 6 WSI prediction tasks\n",
    "prediction_tasks = ['msi', 'braf', 'cimp', 'cin', 'hm', 'tp53']\n",
    "\n",
    "# iterate over each of the prediction tasks and add the results to a dictionary\n",
    "wsi_output_dict = {}\n",
    "for task in prediction_tasks:\n",
    "    wsi_predictor = PatchPredictor(\n",
    "        pretrained_model='resnet34-idars-%s' % task,\n",
    "        batch_size=64,\n",
    "        num_loader_workers=8)\n",
    "\n",
    "    # we include the obtained tumour mask from stage 1 as an argument so that we only process patches from those regions.\n",
    "    wsi_output_dict[task] = wsi_predictor.predict(\n",
    "        imgs=wsi_file_list,\n",
    "        masks=['tumour_mask.png'],\n",
    "        mode='wsi',\n",
    "        return_probabilities=True,\n",
    "        on_gpu=ON_GPU)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6mpjnja9W6V6"
   },
   "source": [
    "Now that we have performed patch-level classification for each tumour patch, let's merge together the results to get the 2D prediction map. Here, we return the raw probability map, rather than the class predictions. We do this because the final step in the IDaRS pipeline uses the raw patch-level probabilities to classify each WSI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lXi9EFaIW6V6",
    "outputId": "d2b2515a-da68-49d2-adfd-e1364060c557"
   },
   "outputs": [],
   "source": [
    "overview_resolution = 1.25 # the resolution in which we desire to merge and visualize the patch predictions\n",
    "overview_unit = \"power\" # the unit of the `resolution` parameter. Can be \"power\", \"level\", \"mpp\", or \"baseline\"\n",
    "\n",
    "# merge predictions to form a 2-dimensional output at the desired resolution\n",
    "merged_output_dict = {}\n",
    "for task in prediction_tasks:\n",
    "    merged_output = PatchPredictor.merge_predictions(wsi_file_name, wsi_output_dict[task][0], resolution=overview_resolution, units=overview_unit, return_raw=True)\n",
    "    merged_output_dict[task] = merged_output[..., 1] # consider only the positive class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6yrlQAZpW6V7"
   },
   "source": [
    "Now that we have merged the predictions to obtain the probability map, let's visualise the results. This is very similar to the `overlay_patch_prediction` function, but `label_info` does not need to be provided because we only have one class. Also, `min_val` can be provided. This is a number between 0 and 1 which ensures that the probability map is only shown for values greater than `min_val`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 858
    },
    "id": "vuvLh0edW6V7",
    "nbsphinx-thumbnail": {
     "output-index": -1
    },
    "outputId": "8216c92f-5c13-4304-8f62-ed11898c7345"
   },
   "outputs": [],
   "source": [
    "from tiatoolbox.utils.visualization import overlay_probability_map\n",
    "\n",
    "# [Overlay map creation]\n",
    "# iterate over the tasks and generate the probability map\n",
    "overlay_list = []\n",
    "for task in prediction_tasks:\n",
    "    # only show the probability map when it is greater than `min_val` (0.1 is used here)\n",
    "  overlay_prob_map = overlay_probability_map(wsi_overview, merged_output_dict[task], alpha=0.5, min_val=0.1, return_ax=False)\n",
    "  overlay_list.append(overlay_prob_map)\n",
    "\n",
    "# visualise the overlays in a single plot\n",
    "fig = plt.figure(figsize=(14,7))\n",
    "for i in range(6):\n",
    "  ax = plt.subplot(2,3,i+1)\n",
    "  plt.imshow(overlay_list[i])\n",
    "  plt.axis('off')\n",
    "  plt.title(prediction_tasks[i].upper())\n",
    "fig.suptitle('Probability Map Overlay for Each Task')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KO6L_ZsOW6V7"
   },
   "source": [
    "These probability maps can be used to increase the interpretability of results and help identify regions contributing to the overall prediction. To get a smoother output, you can increase the overlap by modifying `IOPatchPredictorConfig`.\n",
    "\n",
    "Let's visualise some patches that have either a high probability or a low probability of being MSI positive. For this, we randomly sample 4 patches with probability of being MSI > 0.95 and 4 patches with probability of being MSI < 0.05."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Rr-op06RW6V7"
   },
   "outputs": [],
   "source": [
    "# get the probabilities of each processed patch being MSI\n",
    "msi_probabilities = wsi_output_dict['msi'][0]['probabilities']\n",
    "msi_probabilities = np.array(msi_probabilities)[..., 1]\n",
    "\n",
    "# get the coordinates of each processed patch\n",
    "msi_coordinates = wsi_output_dict['msi'][0]['coordinates']\n",
    "msi_coordinates = np.array(msi_coordinates)\n",
    "\n",
    "# subset where MSI probability is greater than 0.95\n",
    "msi_probabilities_subset_pos = msi_probabilities[msi_probabilities > 0.95]\n",
    "msi_coordinates_subset_pos = msi_coordinates[msi_probabilities > 0.95]\n",
    "\n",
    "# subset where MSI probability is less than than 0.05\n",
    "msi_probabilities_subset_neg = msi_probabilities[msi_probabilities > 0.05]\n",
    "msi_coordinates_subset_neg = msi_coordinates[msi_probabilities > 0.05]\n",
    "msi_coordinates_subset = list(msi_coordinates_subset_pos) + list(msi_coordinates_subset_neg)\n",
    "\n",
    "# randomly sample 4 positive and 4 negative\n",
    "random_pos_idx = np.random.randint(0, msi_probabilities_subset_pos.shape[0], size=4)\n",
    "random_neg_idx = np.random.randint(0, msi_probabilities_subset_neg.shape[0], size=4)\n",
    "random_idx = list(random_pos_idx) + list(random_neg_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9ObSUui4W6V8"
   },
   "source": [
    "Now that we have obtained our random sample, let's plot the patches! First, we use TIAToolbox's `WSIReader` to read the original WSI and extract patches at defined locations. For detailed information on this, see the relevant example notebook!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 446
    },
    "id": "2XUW0sI2W6V8",
    "outputId": "6d019d1a-2f1b-41e7-de09-b78eac123876"
   },
   "outputs": [],
   "source": [
    "# read the WSI and get the resolution used during processing.\n",
    "wsi_reader = get_wsireader(input_img=wsi_file_name)\n",
    "resolution = wsi_output_dict['msi'][0]['resolution']\n",
    "units = wsi_output_dict['msi'][0]['units']\n",
    "\n",
    "# visualise the overlays in a single plot\n",
    "fig = plt.figure()\n",
    "for i in range(8):\n",
    "  ax = plt.subplot(2,4,i+1)\n",
    "  coords = msi_coordinates_subset[random_idx[i]]\n",
    "  size = (coords[2]-coords[0], coords[3]-coords[1]) # determine the size of patch from the coordinates\n",
    "  # get the patch\n",
    "  patch = wsi_reader.read_rect((coords[0]*4, coords[1]*4), size, resolution=resolution, units=units)\n",
    "  plt.imshow(patch)\n",
    "  plt.axis('off')\n",
    "  if i < 4:\n",
    "    plt.title('MSI')\n",
    "  else:\n",
    "    plt.title('MSS')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "itcxcQiZW6V8"
   },
   "source": [
    "The probability maps are not directly needed to obtain the WSI-level score. Below, we will demonstrate how one can obtain the WSI score utilising the output from the `PatchPredictor` in step 2. For the purpose of this example, we will show how to use mean and max aggregation to obtain a slide-level prediction score.\n",
    "\n",
    "First, let's examine the format of the dictionary output by `PatchPredictor`. We look only at the MSI output as an example, but the output from each task is similar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Tx5-ApPVW6V8",
    "outputId": "fed388f0-181a-4761-bd28-619b04d94f29"
   },
   "outputs": [],
   "source": [
    "# get the keys of the dictionary returned at the output of the MSI CNNPatchPredictor\n",
    "print('Output of PatchPredictor:')\n",
    "print(list(wsi_output_dict['msi'][0].keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hdJKoPmrW6V9"
   },
   "source": [
    "As we can see, the probabilities are returned as output. Now, let's compute, for each task, the average and maximum over all probabilities of tumour tiles being positive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0RgnSFwVW6V9",
    "outputId": "f6867d6d-03fa-4a69-b468-2e759748d6be"
   },
   "outputs": [],
   "source": [
    "slide_score_mean = {}\n",
    "slide_score_max = {}\n",
    "# iterate over the tasks\n",
    "for task in prediction_tasks:\n",
    "    probabilities = np.array(wsi_output_dict[task][0]['probabilities'])[...,1] # only consider positive class\n",
    "    slide_score_mean[task] = np.mean(probabilities) # get the average probability over all tumour tiles\n",
    "    slide_score_max[task] = np.max(probabilities) # get the maximum probability over all tumour tiles\n",
    "\n",
    "\n",
    "# print the scores\n",
    "print('MEAN AGGREGATION')\n",
    "for task, value in slide_score_mean.items():\n",
    "    print(task, ':', value)\n",
    "\n",
    "print('-'*30)\n",
    "\n",
    "print('MAX AGGREGATION')\n",
    "for task, value in slide_score_max.items():\n",
    "    print(task, ':', value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3_sFqMptW6V9"
   },
   "source": [
    "As can be seen, it is straight forward to go from the WSI output to the WSI-level prediction score. In a similar way, different aggregation methods can be used, such as top k probabilities. To highlight how easy it is to obtain the slide level score using TIAToolbox, we provide below the code required to go from input to output.\n",
    "\n",
    "```\n",
    "# TUMOUR DETECTION\n",
    "tumour_predictor = PatchPredictor(pretrained_model='resnet18-idars-tumour', batch_size=64)\n",
    "\n",
    "tumour_output = tumour_predictor.predict(\n",
    "    imgs=[wsi_file_name],\n",
    "    mode='wsi',\n",
    "    return_probabilities=True,\n",
    "    on_gpu=True)\n",
    "\n",
    "tumour_mask = tumour_predictor.merge_predictions(wsi_file_name, tumour_output[0], resolution=overview_resolution, units=overview_unit)\n",
    "tumour_mask = tumour_mask == 2 # binarise the output\n",
    "imwrite('tumour_mask.png', tumour_mask.astype('uint8') * 255)\n",
    "\n",
    "# WSI PREDICTION\n",
    "msi_predictor = PatchPredictor(pretrained_model='resnet34-idars-msi', batch_size=64)\n",
    "\n",
    "msi_output = msi_predictor.predict(\n",
    "    imgs=[wsi_file_name],\n",
    "    masks=['tumour_mask.png'],\n",
    "    mode='wsi',\n",
    "    return_probabilities=True,\n",
    "    on_gpu=True)\n",
    "\n",
    "# SLIDE-LEVEL SCORE\n",
    "msi_probabilities = np.array(msi_output[0]['probabilities'])[...,1] # only consider MSI class\n",
    "average_msi_probability = np.mean(msi_probabilities) # get the average over all tumour tiles\n",
    "```\n",
    "\n",
    "We encourage you to play around with the models provided by TIAToolbox for prediction of molecular pathways and key mutations. To make a different WSI level prediction, all you need to do is use a different task-specific pretrained weights in the second `PatchPredictor`.\n",
    "\n",
    "Below, we report the results obtained with our retrained model compared to the original results. For the purpose of this, we only consider mean aggregation for each task. All results show the area under the receiver operating characteristic curve (AUROC).\n",
    "\n",
    "Task |Original | TIAToolbox  |\n",
    "-----|---------|-------------|\n",
    "MSI  | 0.83    | 0.87        |\n",
    "TP53 | 0.76    | 0.75        |\n",
    "BRAF | 0.81    | 0.75        |\n",
    "CIMP | 0.85    | 0.75        |\n",
    "CIN  | 0.85    | 0.81        |\n",
    "HM   | 0.86    | 0.79        |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c3Rbp-mhW6V9"
   },
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "celltoolbar": "Edit Metadata",
  "colab": {
   "collapsed_sections": [],
   "name": "inference-pipelines-idars.ipynb",
   "provenance": []
  },
  "interpreter": {
   "hash": "20f080c2d3cdb15ba1f46d1d9775b5885435ab326e3af81a061f3ec6f66f8478"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "0472aa7dd7ac420984db44f487c70461": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "0a447af25e094ed694f9380edde29c0d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "14f354f25fb34296892d667d331812bb": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "1ce0a19aecb5468290e21172ceacf2ca": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_513665800fc6407b81722fee8e37dd1f",
      "max": 46830571,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_c031de5cba4d4e00883b5781cb3244e2",
      "value": 46830571
     }
    },
    "20e3921b29b845ec9448f0ee7f556e6d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "2a85925d02bc490e985e9450564df66d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_982c738965a44dc3accd6e65d64f29e9",
      "max": 87319819,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_b6a5f1ce8f944ffc988bbe9aaabb9cd5",
      "value": 87319819
     }
    },
    "501c86f107be49cbb10aa522ad7e3b0f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_f2ff287229d246d197eac72ede439582",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_20e3921b29b845ec9448f0ee7f556e6d",
      "value": "100%"
     }
    },
    "513665800fc6407b81722fee8e37dd1f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5c737719ec54471199c42d7732df0f26": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_501c86f107be49cbb10aa522ad7e3b0f",
       "IPY_MODEL_1ce0a19aecb5468290e21172ceacf2ca",
       "IPY_MODEL_841bef2fbe374fd980e37b945ebe421a"
      ],
      "layout": "IPY_MODEL_df29a2b570704ad2a96b776f3a352f16"
     }
    },
    "72859345f3b14dc8b2a3494cb6b3a94b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_7627208469ad49ea8935cc7771d1d822",
       "IPY_MODEL_2a85925d02bc490e985e9450564df66d",
       "IPY_MODEL_72c36c9497b940bc845a16475c0770e7"
      ],
      "layout": "IPY_MODEL_0472aa7dd7ac420984db44f487c70461"
     }
    },
    "72c36c9497b940bc845a16475c0770e7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_afc7df50726948bb8b4486b7d5912da9",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_14f354f25fb34296892d667d331812bb",
      "value": " 83.3M/83.3M [00:00&lt;00:00, 112MB/s]"
     }
    },
    "7627208469ad49ea8935cc7771d1d822": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_0a447af25e094ed694f9380edde29c0d",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_94521a8a76bb42169bcc2780316ee33a",
      "value": "100%"
     }
    },
    "841bef2fbe374fd980e37b945ebe421a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_8cc718e270864e38a9c601cc43e8362a",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_d621c61ed1844246ba9c0e67ac0160f6",
      "value": " 44.7M/44.7M [00:00&lt;00:00, 108MB/s]"
     }
    },
    "8cc718e270864e38a9c601cc43e8362a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "94521a8a76bb42169bcc2780316ee33a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "982c738965a44dc3accd6e65d64f29e9": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "afc7df50726948bb8b4486b7d5912da9": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b6a5f1ce8f944ffc988bbe9aaabb9cd5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "c031de5cba4d4e00883b5781cb3244e2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "d621c61ed1844246ba9c0e67ac0160f6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "df29a2b570704ad2a96b776f3a352f16": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f2ff287229d246d197eac72ede439582": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
