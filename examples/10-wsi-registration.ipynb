{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Whole Slide Image Registration\n",
    "\n",
    "Click to open in: \\[[GitHub](https://github.com/ruqayya/Whole-Slide-Image-Registration/blob/main/WSI_Registration.ipynb)\\]\\[[Colab](https://colab.research.google.com/github/ruqayya/Whole-Slide-Image-Registration/blob/main/WSI_Registration.ipynb)\\]\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## About this notebook\n",
    "\n",
    "This Jupyter notebook can be run on any computer with a standard browser and no prior installation of any programming language is required. It can run remotely over the Internet, free of charge, thanks to Google Colaboratory. To connect with Colab, click on one of the two blue checkboxes above. Check that \"colab\" appears in the address bar. You can right-click on \"Open in Colab\" and select \"Open in new tab\" if the left click does not work for you. Familiarize yourself with the drop-down menus near the top of the window. You can edit the notebook during the session, for example substituting your own image files for the image files used in this demo. Experiment by changing the parameters of functions. It is not possible for an ordinary user to permanently change this version of the notebook on GitHub or Colab, so you cannot inadvertently mess it up. Use the notebook's File Menu if you wish to save your own (changed) notebook.\n",
    "\n",
    "To run the notebook on any platform, except for Colab, set up your Python environment, as explained in the [README](https://github.com/TIA-Lab/tiatoolbox/blob/master/README.md#install-python-package) file.\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### About this demo\n",
    "\n",
    "In this example, we will show how to use TIAToolbox for registration of an image pair using [Deep Feature Based Registration](https://arxiv.org/pdf/2202.09971.pdf) (DFBR) \\[1\\], followed by non-rigid alignment using [SimpleITK](https://simpleitk.readthedocs.io/en/master/registrationOverview.html). The registration tool in the TIAToolbox also comprises a pre-alignment step, pre-requisite to DFBR. In particular, we will introduce the use of our registration tool `wsi_registration`.\n",
    "\n",
    "\\[1\\] Awan, Ruqayya, et al. \"Deep Feature based Cross-slide Registration.\" arXiv preprint arXiv:2202.09971 (2022).\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "tags": [
     "remove-cell"
    ],
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Setting up the environment\n",
    "\n",
    "### TIAToolbox and dependencies installation\n",
    "\n",
    "You can skip the following cell if 1) you are not using the Colab plaform or 2) you are using Colab and this is not your first run of the notebook in the current runtime session. If you nevertheless run the cell, you may get an error message, but no harm will be done. On Colab the cell installs `tiatoolbox`, and other prerequisite software. Harmless error messages should be ignored. Outside Colab , the notebook expects `tiatoolbox` to already be installed. (See the instructions in [README](https://github.com/TIA-Lab/tiatoolbox/blob/master/README.md#install-python-package).)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "tags": [
     "remove-cell"
    ],
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'apt-get' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Installation is done.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'tail' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    }
   ],
   "source": [
    "!apt-get -y install libopenjp2-7-dev libopenjp2-tools openslide-tools | tail -n 1\n",
    "!pip install git+https://github.com/TissueImageAnalytics/tiatoolbox.git@develope | tail --line 1\n",
    "\n",
    "print(\"Installation is done.\")"
   ],
   "metadata": {
    "collapsed": false,
    "tags": [
     "remove-cell"
    ],
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "> **IMPORTANT**: If you are using Colab and you run the cell above for the first time, please note that you need to restart the runtime before proceeding through (menu) *\"RuntimeÃ¢â€ â€™Restart runtime\"* . This is needed to load the latest versions of prerequisite packages installed with TIAToolbox. Doing so, you should be able to run all the remaining cells altogether (*\"RuntimeÃ¢â€ â€™Run after\"* from the next cell) or one by one.\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### GPU or CPU runtime\n",
    "\n",
    "Processes in this notebook can be accelerated by using a GPU. Therefore, whether you are running this notebook on your system or Colab, you need to check and specify if you are using GPU or CPU hardware acceleration. In Colab, you need to make sure that the runtime type is set to GPU in the *\"RuntimeÃ¢â€ â€™Change runtime typeÃ¢â€ â€™Hardware accelerator\"*. If you are *not* using GPU, consider changing the `ON_GPU` flag to `Flase` value, otherwise, some errors will be raised when running the following cells.\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "ON_GPU = True  # Should be changed to False if no cuda-enabled GPU is available"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Clean-up before a run\n",
    "\n",
    "To ensure proper clean-up (for example in abnormal termination), all files downloaded or created in this run are saved in a single directory global_save_dir, which we set equal to \"./tmp/\". To simplify maintenance, the name of the directory occurs only at this one place, so that it can easily be changed, if desired.\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import warnings\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "global_save_dir = \"./tmp/\"\n",
    "\n",
    "\n",
    "def rmdir(dir_path):\n",
    "    if os.path.isdir(dir_path):\n",
    "        shutil.rmtree(dir_path)\n",
    "        print(\"removing directory \", dir_path)\n",
    "\n",
    "\n",
    "rmdir(global_save_dir)  # remove  directory if it exists from previous runs\n",
    "os.mkdir(global_save_dir)\n",
    "print(\"creating new directory \", global_save_dir)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Importing related libraries\n",
    "\n",
    "We import some standard Python modules, and also the TIAToolbox Python modules for the image registration task, written by the TIA Centre team.\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from tiatoolbox.models.engine.semantic_segmentor import SemanticSegmentor\n",
    "from tiatoolbox.tools.registration.wsi_registration import (\n",
    "    match_histograms,\n",
    "    DFBRegister,\n",
    "    Transformer,\n",
    ")\n",
    "from tiatoolbox.wsicore.wsireader import WSIReader\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from scipy import ndimage\n",
    "from matplotlib import pyplot as plt\n",
    "from skimage import color, exposure, measure, morphology\n",
    "import matplotlib as mpl\n",
    "\n",
    "mpl.rcParams[\"figure.dpi\"] = 300  # for high resolution figure in notebook\n",
    "mpl.rcParams[\"figure.facecolor\"] = \"white\"  # To make sure text is visible in dark mode"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Downloading the required files\n",
    "\n",
    "We download, over the internet, image files used for the purpose of this notebook. In particular, we use a sample image pair from the ANHIR challenge dataset, which have been downloaded from  https://anhir.grand-challenge.org/Data/. The challenge dataset is available under the following licence: [CC-BY-NC-SA](https://creativecommons.org/licenses/by-nc-sa/2.0/). The challenge paper \\[1\\] is available [here](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7584382/).\n",
    "\n",
    "\\[1\\] Borovec, JiÃ…â„¢ÃƒÂ­, et al. \"ANHIR: automatic non-rigid histological image registration challenge.\" IEEE transactions on medical imaging 39.10 (2020): 3042-3052.\n",
    "\n",
    "Downloading is needed once in each Colab session and it should take less than 1 minute.\n",
    "\n",
    "> In Colab, if you click the file's icon (see below) in the vertical toolbar on the left-hand side then you can see all the files which the code in this notebook can access. The data will appear here when it is downloaded.\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "fixed_img_file_name = \"fixed_image.tif\"\n",
    "moving_img_file_name = \"moving_image.tif\"\n",
    "\n",
    "# Downloading fixed image from COMET dataset\n",
    "r = requests.get(\n",
    "    \"https://tiatoolbox.dcs.warwick.ac.uk/testdata/registration/CRC/06-18270_5_A1MLH1_1.tif\"\n",
    ")\n",
    "with open(fixed_img_file_name, \"wb\") as f:\n",
    "    f.write(r.content)\n",
    "\n",
    "# Downloading moving image from COMET dataset\n",
    "r = requests.get(\n",
    "    \"https://tiatoolbox.dcs.warwick.ac.uk/testdata/registration/CRC/06-18270_5_A1MSH2_1.tif\"\n",
    ")\n",
    "with open(moving_img_file_name, \"wb\") as f:\n",
    "    f.write(r.content)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Reading and visualising images\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "fixed_wsi_reader = WSIReader.open(input_img=fixed_img_file_name)\n",
    "fixed_image_rgb = fixed_wsi_reader.slide_thumbnail(resolution=0.1563, units=\"power\")\n",
    "moving_wsi_reader = WSIReader.open(input_img=moving_img_file_name)\n",
    "moving_image_rgb = moving_wsi_reader.slide_thumbnail(resolution=0.1563, units=\"power\")\n",
    "\n",
    "_, axs = plt.subplots(1, 2, figsize=(15, 10))\n",
    "axs[0].imshow(fixed_image_rgb, cmap=\"gray\")\n",
    "axs[0].set_title(\"Fixed Image\")\n",
    "axs[1].imshow(moving_image_rgb, cmap=\"gray\")\n",
    "axs[1].set_title(\"Moving Image\")\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Image Pre-processing\n",
    "\n",
    "The images are converted to greyscale. The contrast of both the images is improved by linearly rescaling the values. To unify the appearance of an image pair, histogram matching is performed as a normalisation step. In histogram matching, the histogram of an image is modified to be similar to that of another image. An image with high entropy is considered as a reference image and the histogram of an image with low entropy is matched to the reference image.\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def preprocess_image(image):\n",
    "    image = color.rgb2gray(image)\n",
    "    image = exposure.rescale_intensity(\n",
    "        image, in_range=tuple(np.percentile(image, (0.5, 99.5)))\n",
    "    )\n",
    "    image = image * 255\n",
    "    return image.astype(np.uint8)\n",
    "\n",
    "\n",
    "fixed_image = preprocess_image(fixed_image_rgb)\n",
    "moving_image = preprocess_image(moving_image_rgb)\n",
    "fixed_image, moving_image = match_histograms(fixed_image, moving_image)\n",
    "\n",
    "_, axs = plt.subplots(1, 2, figsize=(15, 10))\n",
    "axs[0].imshow(fixed_image, cmap=\"gray\")\n",
    "axs[0].set_title(\"Fixed Image\")\n",
    "axs[1].imshow(moving_image, cmap=\"gray\")\n",
    "axs[1].set_title(\"Moving Image\")\n",
    "plt.show()\n",
    "\n",
    "temp = np.repeat(np.expand_dims(fixed_image, axis=2), 3, axis=2)\n",
    "cv2.imwrite(os.path.join(global_save_dir, \"fixed.png\"), temp)\n",
    "temp = np.repeat(np.expand_dims(moving_image, axis=2), 3, axis=2)\n",
    "cv2.imwrite(os.path.join(global_save_dir, \"moving.png\"), temp)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Tissue Segmentation\n",
    "\n",
    "In this section, image segmentation is performed to generate tissue masks so that registration can be performed using only the active or discriminatory tissue area. In DFBR, these are used to exclude the matching points from the non-tissue and fatty regions while in the non-rigid alignment method, the background region is excluded from images before inputting them to the SimpleITK module.\n",
    "\n",
    "We should mention that when you use a TIAToolbox pretrained model, you don't need to worry about setting the input/output shape parameters as their optimal values will be loaded by default.\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "save_dir = os.path.join(global_save_dir, \"tissue_mask\")\n",
    "if os.path.exists(save_dir):\n",
    "    shutil.rmtree(save_dir, ignore_errors=False, onerror=None)\n",
    "\n",
    "segmentor = SemanticSegmentor(\n",
    "    pretrained_model=\"unet_tissue_mask_tsef\",\n",
    "    num_loader_workers=4,\n",
    "    batch_size=4,\n",
    ")\n",
    "\n",
    "output = segmentor.predict(\n",
    "    [\n",
    "        os.path.join(global_save_dir, \"fixed.png\"),\n",
    "        os.path.join(global_save_dir, \"moving.png\"),\n",
    "    ],\n",
    "    save_dir=save_dir,\n",
    "    mode=\"tile\",\n",
    "    resolution=1.0,\n",
    "    units=\"baseline\",\n",
    "    patch_input_shape=[1024, 1024],\n",
    "    patch_output_shape=[512, 512],\n",
    "    stride_shape=[512, 512],\n",
    "    on_gpu=ON_GPU,\n",
    "    crash_on_exception=True,\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Post-processing and Visualization of Masks\n",
    "\n",
    "In the output, the prediction method returns a list of the paths to its inputs and to the processed outputs saved on the disk. This can be used to load the results for processing and visualisation. In this section, a simple processing of the raw prediction is performed to generate the tissue mask.\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def post_processing_mask(mask):\n",
    "    mask = ndimage.binary_fill_holes(mask, structure=np.ones((3, 3))).astype(int)\n",
    "\n",
    "    # remove all the objects while keep the biggest object only\n",
    "    label_img = measure.label(mask)\n",
    "    if len(np.unique(label_img)) > 2:\n",
    "        regions = measure.regionprops(label_img)\n",
    "        mask = mask.astype(bool)\n",
    "        all_area = [i.area for i in regions]\n",
    "        second_max = max([i for i in all_area if i != max(all_area)])\n",
    "        mask = morphology.remove_small_objects(mask, min_size=second_max + 1)\n",
    "    return mask.astype(np.uint8)\n",
    "\n",
    "\n",
    "fixed_mask = np.load(output[0][1] + \".raw.0.npy\")\n",
    "moving_mask = np.load(output[1][1] + \".raw.0.npy\")\n",
    "\n",
    "# Simple processing of the raw prediction to generate semantic segmentation task\n",
    "fixed_mask = np.argmax(fixed_mask, axis=-1) == 2\n",
    "moving_mask = np.argmax(moving_mask, axis=-1) == 2\n",
    "\n",
    "fixed_mask = post_processing_mask(fixed_mask)\n",
    "moving_mask = post_processing_mask(moving_mask)\n",
    "\n",
    "_, axs = plt.subplots(1, 2, figsize=(15, 10))\n",
    "axs[0].imshow(fixed_mask, cmap=\"gray\")\n",
    "axs[0].set_title(\"Fixed Mask\")\n",
    "axs[1].imshow(moving_mask, cmap=\"gray\")\n",
    "axs[1].set_title(\"Moving Mask\")\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Registration using DFBR\n",
    "\n",
    "In this section, we apply the DFBR method for aligning the moving image with respect to the fixed image. The pre-trained VGG-16 model is used as a feature extractor which accepts input images with 3 channels. Therefore, greyscale images were stacked as the colour channel.\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "dfbr_fixed_image = np.repeat(np.expand_dims(fixed_image, axis=2), 3, axis=2)\n",
    "dfbr_moving_image = np.repeat(np.expand_dims(moving_image, axis=2), 3, axis=2)\n",
    "\n",
    "df = DFBRegister()\n",
    "dfbr_transform = df.register(\n",
    "    dfbr_fixed_image, dfbr_moving_image, fixed_mask, moving_mask\n",
    ")\n",
    "\n",
    "# Visualization\n",
    "original_moving = cv2.warpAffine(\n",
    "    moving_image, np.eye(2, 3), fixed_image.shape[:2][::-1]\n",
    ")\n",
    "dfbr_registered_image = cv2.warpAffine(\n",
    "    moving_image, dfbr_transform[0:-1], fixed_image.shape[:2][::-1]\n",
    ")\n",
    "dfbr_registered_mask = cv2.warpAffine(\n",
    "    moving_mask, dfbr_transform[0:-1], fixed_image.shape[:2][::-1]\n",
    ")\n",
    "\n",
    "before_overlay = np.dstack((original_moving, fixed_image, original_moving))\n",
    "dfbr_overlay = np.dstack((dfbr_registered_image, fixed_image, dfbr_registered_image))\n",
    "\n",
    "_, axs = plt.subplots(1, 2, figsize=(15, 10))\n",
    "axs[0].imshow(before_overlay, cmap=\"gray\")\n",
    "axs[0].set_title(\"Overlay Before Registration\")\n",
    "axs[1].imshow(dfbr_overlay, cmap=\"gray\")\n",
    "axs[1].set_title(\"Overlay After DFBR\")\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Patch Extraction\n",
    "\n",
    "In this section, we show how to extract region from a fixed image and the corresponding region from a moving image\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "location = (3500, 20000)  # at base level 0\n",
    "level = 2\n",
    "size = (1000, 1000)  # (width, height)\n",
    "\n",
    "# Updating the transform w.r.t level 0\n",
    "transform_level0 = dfbr_transform * [[1, 1, 2**6], [1, 1, 2**6], [1, 1, 1]]\n",
    "\n",
    "# Extract region from the fixed whole slide image\n",
    "fixed_tile = fixed_wsi_reader.read_rect(location, size, resolution=level, units=\"level\")\n",
    "\n",
    "# Extract transformed region from the moving whole slide image\n",
    "tfm = Transformer(moving_wsi_reader, transform_level0)\n",
    "moving_tile = tfm.read_rect(location, size, level)\n",
    "\n",
    "_, axs = plt.subplots(1, 2, figsize=(15, 10))\n",
    "axs[0].imshow(fixed_tile, cmap=\"gray\")\n",
    "axs[0].set_title(\"Fixed Tile\")\n",
    "axs[1].imshow(moving_tile, cmap=\"gray\")\n",
    "axs[1].set_title(\"Moving Tile\")\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### B-Spline Transform\n",
    "\n",
    "In this section, we perform registration of fixed and moving tiles using a multi-resolution B-Spline method. The moving image here is a registered image after the application of the DFBR transform. For the given image pair, this method is shown to perform better with the inverted intensity values followed by background removal.\n",
    "The B-Spline approach involves a number of parameters that the user can experiment with. Changing these parameters would result in varying complexity and registration completion times. Feel free to play around with the non-rigid alignment parameters and to experiment with new images.\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from tiatoolbox.tools.registration.wsi_registration import (\n",
    "    apply_bspline_transform,\n",
    "    estimate_bspline_transform,\n",
    ")\n",
    "\n",
    "fixed_mask = np.ones(shape=fixed_tile.shape, dtype=int)\n",
    "moving_mask = np.ones(shape=moving_tile.shape, dtype=int)\n",
    "outTx = estimate_bspline_transform(\n",
    "    fixed_tile,\n",
    "    moving_tile,\n",
    "    fixed_mask,\n",
    "    moving_mask,\n",
    "    grid_space=200.0,\n",
    "    sampling_percent=0.1,\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "bspline_registered_image = apply_bspline_transform(fixed_tile, moving_tile, outTx)\n",
    "\n",
    "tile_overlay = np.dstack(\n",
    "    (moving_tile[:, :, 0], fixed_tile[:, :, 0], moving_tile[:, :, 0])\n",
    ")\n",
    "bspline_overlay = np.dstack(\n",
    "    (\n",
    "        bspline_registered_image[:, :, 0],\n",
    "        fixed_tile[:, :, 0],\n",
    "        bspline_registered_image[:, :, 0],\n",
    "    )\n",
    ")\n",
    "\n",
    "_, axs = plt.subplots(1, 2, figsize=(15, 10))\n",
    "axs[0].imshow(tile_overlay, cmap=\"gray\")\n",
    "axs[0].set_title(\"Before B-spline Transform\")\n",
    "axs[1].imshow(bspline_overlay, cmap=\"gray\")\n",
    "axs[1].set_title(\"After B-spline Transform\")\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU",
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "e6aeef27e4ad4fde83f03b2122824078": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_07d2030ec38b4f529cafb73adbeb9fa8",
       "IPY_MODEL_cb8b0990d6bd4764bcd4e661c809cf67",
       "IPY_MODEL_3229d6ba40644e0aa230c7a85b65108f"
      ],
      "layout": "IPY_MODEL_ba621aab0c8e438693b431b4461f6047"
     }
    },
    "07d2030ec38b4f529cafb73adbeb9fa8": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a1a3eaad63234711a8f6c36de9a75af4",
      "placeholder": "ÃƒÂ¢Ã¢â€šÂ¬Ã¢â‚¬Â¹",
      "style": "IPY_MODEL_0a96cdeda5234fea964bdb32d567ac59",
      "value": "100%"
     }
    },
    "cb8b0990d6bd4764bcd4e661c809cf67": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a63801b6804041f987a316c6bf091d3c",
      "max": 553433881,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_3c5ec71d42d042e5a0b62ccede28b453",
      "value": 553433881
     }
    },
    "3229d6ba40644e0aa230c7a85b65108f": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_617e534f7adf48e6aa16757abf0cde5f",
      "placeholder": "ÃƒÂ¢Ã¢â€šÂ¬Ã¢â‚¬Â¹",
      "style": "IPY_MODEL_24d564d25059463480ae824699bfddca",
      "value": " 528M/528M [00:03&lt;00:00, 250MB/s]"
     }
    },
    "ba621aab0c8e438693b431b4461f6047": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a1a3eaad63234711a8f6c36de9a75af4": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "0a96cdeda5234fea964bdb32d567ac59": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "a63801b6804041f987a316c6bf091d3c": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3c5ec71d42d042e5a0b62ccede28b453": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "617e534f7adf48e6aa16757abf0cde5f": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "24d564d25059463480ae824699bfddca": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}