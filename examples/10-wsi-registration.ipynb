{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Whole Slide Image Registration\n",
    "\n",
    "Click to open in: \\[[GitHub](https://github.com/TissueImageAnalytics/tiatoolbox/blob/DFBR_notenook_revised/examples/10-wsi-registration.ipynb)\\]\\[[Colab](https://colab.research.google.com/github/TissueImageAnalytics/tiatoolbox/blob/master/examples/10-wsi-registration.ipynb)\\]\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## About this notebook\n",
    "\n",
    "This Jupyter notebook can be run on any computer with a standard browser and no prior installation of any programming language is required. It can run remotely over the Internet, free of charge, thanks to Google Colaboratory. To connect with Colab, click on one of the two blue checkboxes above. Check that \"colab\" appears in the address bar. You can right-click on \"Open in Colab\" and select \"Open in new tab\" if the left click does not work for you. Familiarize yourself with the drop-down menus near the top of the window. You can edit the notebook during the session, for example substituting your own image files for the image files used in this demo. Experiment by changing the parameters of functions. It is not possible for an ordinary user to permanently change this version of the notebook on GitHub or Colab, so you cannot inadvertently mess it up. Use the notebook's File Menu if you wish to save your own (changed) notebook.\n",
    "\n",
    "To run the notebook on any platform, except for Colab, set up your Python environment, as explained in the [README](https://github.com/TIA-Lab/tiatoolbox/blob/master/README.md#install-python-package) file.\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### About this demo\n",
    "\n",
    "In this example, we will show how to use TIAToolbox for registration of a pair of histology images. Registration refers to aligning a pair of images where one of the images is referred to as the fixed image while the other image is referred as the moving image. The moving image is spatially transformed so that it aligns with the fixed image. Registration often serves as an essential pre-processing step for many medical image analysis tasks. Registration of multiple sections in a tissue block is an important pre-requisite task before any cross-slide image analysis.\n",
    "\n",
    "We perform registration of an image pair using [Deep Feature Based Registration](https://arxiv.org/pdf/2202.09971.pdf) (DFBR) \\[1\\], followed by non-rigid alignment using [SimpleITK](https://simpleitk.readthedocs.io/en/master/registrationOverview.html). The registration tool in the TIAToolbox also comprises a pre-alignment step, pre-requisite to DFBR. In particular, we will introduce the use of our registration tool `wsi_registration`.\n",
    "\n",
    "\\[1\\] Awan, Ruqayya, et al. \"Deep Feature based Cross-slide Registration.\" arXiv preprint arXiv:2202.09971 (2022).\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Setting up the environment\n",
    "\n",
    "### TIAToolbox and dependencies installation\n",
    "\n",
    "You can skip the following cell if 1) you are not using the Colab plaform or 2) you are using Colab and this is not your first run of the notebook in the current runtime session. If you nevertheless run the cell, you may get an error message, but no harm will be done. On Colab the cell installs `tiatoolbox`, and other prerequisite software. Harmless error messages should be ignored. Outside Colab , the notebook expects `tiatoolbox` to already be installed. (See the instructions in [README](https://github.com/TIA-Lab/tiatoolbox/blob/master/README.md#install-python-package).)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "!apt-get -y install libopenjp2-7-dev libopenjp2-tools openslide-tools | tail -n 1\n",
    "!pip install git+https://github.com/TissueImageAnalytics/tiatoolbox.git@DFBR_notenook_revised | tail --line 1\n",
    "\n",
    "print(\"Installation is done.\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**IMPORTANT**: When you run the cell above for the first time, while using Colab, you need to restart the runtime before proceeding further. Click on the box *RESTART RUNTIME* that appears immediately above this cell, or use the menu *Runtime→Restart runtime*. This loads the latest versions of prerequisite packages. The notebook can then be managed normally. For example, you can run subsequent cells one by one, or you can click on *Runtime→Run all* or *Runtime→after*.\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### GPU or CPU runtime\n",
    "\n",
    "Processes in this notebook can be accelerated by using a GPU. Therefore, whether you are running this notebook on your own system or on Colab, you need to check and specify whether you are using GPU or CPU. In Colab, you need to make sure that the runtime type is set to GPU in the *Runtime→Change runtime type→Hardware accelerator*. If you are *not* using GPU, change `ON_GPU` to `False`, otherwise, some errors will be raised when running the following cells.\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "ON_GPU = True  # Should be changed to False if no cuda-enabled GPU is available"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Clean-up before a run\n",
    "\n",
    "To ensure proper clean-up (for example in abnormal termination), all files downloaded or created in this run are saved in a single directory global_save_dir, which we set equal to \"./tmp/\". To simplify maintenance, the name of the directory occurs only at this one place, so that it can easily be changed, if desired.\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import warnings\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "global_save_dir = \"./tmp/\"\n",
    "\n",
    "\n",
    "def rmdir(dir_path):\n",
    "    if os.path.isdir(dir_path):\n",
    "        shutil.rmtree(dir_path)\n",
    "        print(\"removing directory \", dir_path)\n",
    "\n",
    "\n",
    "rmdir(global_save_dir)  # remove  directory if it exists from previous runs\n",
    "os.mkdir(global_save_dir)\n",
    "print(\"creating new directory \", global_save_dir)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Importing related libraries\n",
    "\n",
    "We import some standard Python modules, and also the TIAToolbox Python modules for the image registration task, written by the TIA Centre team.\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from tiatoolbox.models.engine.semantic_segmentor import SemanticSegmentor\n",
    "from tiatoolbox.tools.registration.wsi_registration import (\n",
    "    match_histograms,\n",
    "    DFBRegister,\n",
    "    AffineWSITransformer,\n",
    "    apply_bspline_transform,\n",
    "    estimate_bspline_transform,\n",
    ")\n",
    "from tiatoolbox.wsicore.wsireader import WSIReader\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from scipy import ndimage\n",
    "from matplotlib import pyplot as plt\n",
    "from skimage import color, exposure, measure, morphology\n",
    "import matplotlib as mpl\n",
    "\n",
    "mpl.rcParams[\"figure.dpi\"] = 300  # for high resolution figure in notebook\n",
    "mpl.rcParams[\"figure.facecolor\"] = \"white\"  # To make sure text is visible in dark mode"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Downloading the required files\n",
    "\n",
    "We download, over the internet, image files used for the purpose of this notebook. In particular, we download a sample pair of images from the COMET dataset which is currently a private dataset of TIA centre. Downloading is needed once in each Colab session and it should take less than 2 minutes. Two WSIs are downloaded from the web, as seen in the code below, and saved with filenames given by the variable fixed_img_file_name and moving_img_file_name in the directory given by data_dir. Data generated by the notebook is stored under data_dir, providing rapid local access.\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "fixed_img_file_name = \"fixed_image.tif\"\n",
    "moving_img_file_name = \"moving_image.tif\"\n",
    "\n",
    "# Downloading fixed image from COMET dataset\n",
    "r = requests.get(\n",
    "    \"https://tiatoolbox.dcs.warwick.ac.uk/testdata/registration/CRC/06-18270_5_A1MLH1_1.tif\"\n",
    ")\n",
    "with open(fixed_img_file_name, \"wb\") as f:\n",
    "    f.write(r.content)\n",
    "\n",
    "# Downloading moving image from COMET dataset\n",
    "r = requests.get(\n",
    "    \"https://tiatoolbox.dcs.warwick.ac.uk/testdata/registration/CRC/06-18270_5_A1MSH2_1.tif\"\n",
    ")\n",
    "with open(moving_img_file_name, \"wb\") as f:\n",
    "    f.write(r.content)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Reading and visualising images\n",
    "\n",
    "Below, we read a downsampled version of the fixed and moving WSIs using WSIReader. WSIReader shields the user from the incompatible formats produced by different models of scanners from different vendors. The function WSIReader.open has as input a particular WSI, with a particular image format, and outputs an object fixed_wsi_reader, whose base class is WSIReader, and whose derived class depends on the image format. We load fixed and moving WSIs specified with the help of their corresponding path variables fixed_img_file_name and moving_img_file_name, respectively.\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "fixed_wsi_reader = WSIReader.open(input_img=fixed_img_file_name)\n",
    "fixed_image_rgb = fixed_wsi_reader.slide_thumbnail(resolution=0.1563, units=\"power\")\n",
    "moving_wsi_reader = WSIReader.open(input_img=moving_img_file_name)\n",
    "moving_image_rgb = moving_wsi_reader.slide_thumbnail(resolution=0.1563, units=\"power\")\n",
    "\n",
    "_, axs = plt.subplots(1, 2, figsize=(15, 10))\n",
    "axs[0].imshow(fixed_image_rgb, cmap=\"gray\")\n",
    "axs[0].set_title(\"Fixed Image\")\n",
    "axs[1].imshow(moving_image_rgb, cmap=\"gray\")\n",
    "axs[1].set_title(\"Moving Image\")\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Image Pre-processing\n",
    "\n",
    "The images are converted to greyscale. The contrast of both the images is improved by linearly rescaling the values. To unify the appearance of an image pair, histogram matching is performed as a normalisation step. In histogram matching, the histogram of an image is modified to be similar to that of another image. An image with high entropy is considered as a reference image and the histogram of an image with low entropy is matched to the reference image.\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def preprocess_image(image):\n",
    "    \"\"\"This function converts the RGB image to grayscale image and\n",
    "    improves the contrast by linearly rescaling the values.\n",
    "    \"\"\"\n",
    "    image = color.rgb2gray(image)\n",
    "    image = exposure.rescale_intensity(\n",
    "        image, in_range=tuple(np.percentile(image, (0.5, 99.5)))\n",
    "    )\n",
    "    image = image * 255\n",
    "    return image.astype(np.uint8)\n",
    "\n",
    "\n",
    "# Preprocessing fixed and moving images\n",
    "fixed_image = preprocess_image(fixed_image_rgb)\n",
    "moving_image = preprocess_image(moving_image_rgb)\n",
    "fixed_image, moving_image = match_histograms(fixed_image, moving_image)\n",
    "\n",
    "# Visualising the results\n",
    "_, axs = plt.subplots(1, 2, figsize=(15, 10))\n",
    "axs[0].imshow(fixed_image, cmap=\"gray\")\n",
    "axs[0].set_title(\"Fixed Image\")\n",
    "axs[1].imshow(moving_image, cmap=\"gray\")\n",
    "axs[1].set_title(\"Moving Image\")\n",
    "plt.show()\n",
    "\n",
    "temp = np.repeat(np.expand_dims(fixed_image, axis=2), 3, axis=2)\n",
    "_isWritten = cv2.imwrite(os.path.join(global_save_dir, \"fixed.png\"), temp)\n",
    "temp = np.repeat(np.expand_dims(moving_image, axis=2), 3, axis=2)\n",
    "_isWritten = cv2.imwrite(os.path.join(global_save_dir, \"moving.png\"), temp)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Tissue Segmentation\n",
    "\n",
    "In this section, image segmentation is performed to generate tissue masks so that registration can be performed using only the active or discriminatory tissue area. In DFBR, these are used to exclude the matching points from the non-tissue and fatty regions while in the non-rigid alignment method, the background region is excluded from images before inputting them to the SimpleITK module.\n",
    "\n",
    "Here, we use the TIAToolbox semantic segmentor and its built-in model for tissue segmentation. We should mention that when you use a TIAToolbox pretrained model, you don't need to worry about setting the input/output shape parameters as their optimal values will be loaded by default. For more detailed information on semantic segmentor, readers are referred to a [semantic segmentation example notebook.](https://github.com/TissueImageAnalytics/tiatoolbox/blob/develop/examples/06-semantic-segmentation.ipynb)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "save_dir = os.path.join(global_save_dir, \"tissue_mask\")\n",
    "if os.path.exists(save_dir):\n",
    "    shutil.rmtree(save_dir, ignore_errors=False, onerror=None)\n",
    "\n",
    "segmentor = SemanticSegmentor(\n",
    "    pretrained_model=\"unet_tissue_mask_tsef\",\n",
    "    num_loader_workers=4,\n",
    "    batch_size=4,\n",
    ")\n",
    "\n",
    "output = segmentor.predict(\n",
    "    [\n",
    "        os.path.join(global_save_dir, \"fixed.png\"),\n",
    "        os.path.join(global_save_dir, \"moving.png\"),\n",
    "    ],\n",
    "    save_dir=save_dir,\n",
    "    mode=\"tile\",\n",
    "    resolution=1.0,\n",
    "    units=\"baseline\",\n",
    "    patch_input_shape=[1024, 1024],\n",
    "    patch_output_shape=[512, 512],\n",
    "    stride_shape=[512, 512],\n",
    "    on_gpu=ON_GPU,\n",
    "    crash_on_exception=True,\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Post-processing and Visualization of Masks\n",
    "\n",
    "In the output, the prediction method returns a list of the paths to its inputs and to the processed outputs saved on the disk. This can be used to load the results for processing and visualisation. In this section, a simple processing of the raw prediction is performed to generate the tissue mask.\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def post_processing_mask(mask):\n",
    "    mask = ndimage.binary_fill_holes(mask, structure=np.ones((3, 3))).astype(int)\n",
    "\n",
    "    # remove all the objects while keep the biggest object only\n",
    "    label_img = measure.label(mask)\n",
    "    if len(np.unique(label_img)) > 2:\n",
    "        regions = measure.regionprops(label_img)\n",
    "        mask = mask.astype(bool)\n",
    "        all_area = [i.area for i in regions]\n",
    "        second_max = max([i for i in all_area if i != max(all_area)])\n",
    "        mask = morphology.remove_small_objects(mask, min_size=second_max + 1)\n",
    "    return mask.astype(np.uint8)\n",
    "\n",
    "\n",
    "fixed_mask = np.load(output[0][1] + \".raw.0.npy\")\n",
    "moving_mask = np.load(output[1][1] + \".raw.0.npy\")\n",
    "\n",
    "# Simple processing of the raw prediction to generate semantic segmentation task\n",
    "fixed_mask = np.argmax(fixed_mask, axis=-1) == 2\n",
    "moving_mask = np.argmax(moving_mask, axis=-1) == 2\n",
    "\n",
    "fixed_mask = post_processing_mask(fixed_mask)\n",
    "moving_mask = post_processing_mask(moving_mask)\n",
    "\n",
    "_, axs = plt.subplots(1, 2, figsize=(15, 10))\n",
    "axs[0].imshow(fixed_mask, cmap=\"gray\")\n",
    "axs[0].set_title(\"Fixed Mask\")\n",
    "axs[1].imshow(moving_mask, cmap=\"gray\")\n",
    "axs[1].set_title(\"Moving Mask\")\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Registration using DFBR\n",
    "\n",
    "In this section, we apply the DFBR method for aligning the moving image with respect to the fixed image. The pre-trained VGG-16 model is used as a feature extractor which accepts input images with 3 channels. Therefore, greyscale images were stacked as the colour channel. The output of \"df.register\" is the affine transform which is capable of representing translations, rotations, shearing and scaling.\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "dfbr_fixed_image = np.repeat(np.expand_dims(fixed_image, axis=2), 3, axis=2)\n",
    "dfbr_moving_image = np.repeat(np.expand_dims(moving_image, axis=2), 3, axis=2)\n",
    "\n",
    "df = DFBRegister()\n",
    "dfbr_transform = df.register(\n",
    "    dfbr_fixed_image, dfbr_moving_image, fixed_mask, moving_mask\n",
    ")\n",
    "\n",
    "# Visualization\n",
    "original_moving = cv2.warpAffine(\n",
    "    moving_image, np.eye(2, 3), fixed_image.shape[:2][::-1]\n",
    ")\n",
    "dfbr_registered_image = cv2.warpAffine(\n",
    "    moving_image, dfbr_transform[0:-1], fixed_image.shape[:2][::-1]\n",
    ")\n",
    "dfbr_registered_mask = cv2.warpAffine(\n",
    "    moving_mask, dfbr_transform[0:-1], fixed_image.shape[:2][::-1]\n",
    ")\n",
    "\n",
    "before_overlay = np.dstack((original_moving, fixed_image, original_moving))\n",
    "dfbr_overlay = np.dstack((dfbr_registered_image, fixed_image, dfbr_registered_image))\n",
    "\n",
    "_, axs = plt.subplots(1, 2, figsize=(15, 10))\n",
    "axs[0].imshow(before_overlay, cmap=\"gray\")\n",
    "axs[0].set_title(\"Overlay Before Registration\")\n",
    "axs[1].imshow(dfbr_overlay, cmap=\"gray\")\n",
    "axs[1].set_title(\"Overlay After DFBR\")\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Patch Extraction\n",
    "\n",
    "In this section, we use the AffineWSITransformer class to extract a region from a fixed image and the corresponding region from a moving image of any size at a specific location of any pyramid level. Using this class, one can extract a transformed tile from a moving image without a need to generate transformed WSI in a pyramidal format. AffineWSITransformer takes two inputs: the WSIReader object for the moving WSI and the transformation matrix at level 0. The transformation is applied for the requested tile only. The read_rect function of the AffineWSITransformer takes the same argument values for extracting region from the moving WSI as the read_rect function of the WSIReader class for extracting region from the fixed WSI.\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "location = (3500, 20000)  # at base level 0\n",
    "level = 2\n",
    "size = (1000, 1000)  # (width, height)\n",
    "\n",
    "# Extract region from the fixed whole slide image\n",
    "fixed_tile = fixed_wsi_reader.read_rect(location, size, resolution=level, units=\"level\")\n",
    "\n",
    "# DFBR transform is computed for level 6 and hence should be mapped to level 0 for AffineWSITransformer\n",
    "dfbr_transform_level = 6\n",
    "transform_level0 = dfbr_transform * [\n",
    "    [1, 1, 2**dfbr_transform_level],\n",
    "    [1, 1, 2**dfbr_transform_level],\n",
    "    [1, 1, 1],\n",
    "]\n",
    "\n",
    "# Extract transformed region from the moving whole slide image\n",
    "tfm = AffineWSITransformer(moving_wsi_reader, transform_level0)\n",
    "moving_tile = tfm.read_rect(location, size, resolution=level, units=\"level\")\n",
    "\n",
    "_, axs = plt.subplots(1, 2, figsize=(15, 10))\n",
    "axs[0].imshow(fixed_tile, cmap=\"gray\")\n",
    "axs[0].set_title(\"Fixed Tile\")\n",
    "axs[1].imshow(moving_tile, cmap=\"gray\")\n",
    "axs[1].set_title(\"Moving Tile\")\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Non-rigid registration using B-spline Transform\n",
    "\n",
    "Histology slides mostly have non-linear deformations that cannot be tackled with a global rigid transformation alone. On the other hand, non-rigid registration methods are capable of finding correspondence by locally transforming a moving image. These methods require two images to be linearly aligned before their application. In this section, we perform non-rigid [B-spline registration](https://www.sciencedirect.com/topics/computer-science/spline-registration) of linearly aligned fixed and moving tiles using a [multi-resolution approach](https://simpleitk.readthedocs.io/en/master/link_ImageRegistrationMethodBSpline3_docs.html). The moving image here is a registered tile after the application of the DFBR transform, as shown above. For any given image pair, this method is shown to perform better with the inverted intensity values.\n",
    "The B-spline approach involves a number of parameters that the user can experiment with. Changing these parameters would result in varying complexity and registration completion times. For details on the whole list of possible parameters, readers are referred to its [documentation](https://tia-toolbox.readthedocs.io/en/latest/_autosummary/tiatoolbox.tools.registration.wsi_registration.estimate_bspline_transform.html#tiatoolbox.tools.registration.wsi_registration.estimate_bspline_transform). Feel free to play around with the non-rigid alignment parameters and experiment with new images.\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "fixed_mask = np.ones(shape=fixed_tile.shape, dtype=int)\n",
    "moving_mask = np.ones(shape=moving_tile.shape, dtype=int)\n",
    "bspline_transform = estimate_bspline_transform(\n",
    "    fixed_tile,\n",
    "    moving_tile,\n",
    "    fixed_mask,\n",
    "    moving_mask,\n",
    "    grid_space=200.0,\n",
    "    sampling_percent=0.1,\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now, we apply the B-Spline transform to the linearly aligned moving tile. To apply the transform, a resampling operation is performed. For comparison purposes, overlaid images of fixed and moving tiles before and after the application of the B-spline transform are presented below.\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "bspline_registered_image = apply_bspline_transform(\n",
    "    fixed_tile, moving_tile, bspline_transform\n",
    ")\n",
    "\n",
    "tile_overlay = np.dstack(\n",
    "    (moving_tile[:, :, 0], fixed_tile[:, :, 0], moving_tile[:, :, 0])\n",
    ")\n",
    "bspline_overlay = np.dstack(\n",
    "    (\n",
    "        bspline_registered_image[:, :, 0],\n",
    "        fixed_tile[:, :, 0],\n",
    "        bspline_registered_image[:, :, 0],\n",
    "    )\n",
    ")\n",
    "\n",
    "_, axs = plt.subplots(1, 2, figsize=(15, 10))\n",
    "axs[0].imshow(tile_overlay, cmap=\"gray\")\n",
    "axs[0].set_title(\"Before B-spline Transform\")\n",
    "axs[1].imshow(bspline_overlay, cmap=\"gray\")\n",
    "axs[1].set_title(\"After B-spline Transform\")\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The overlaid image of fixed and moving images on the right side shows a better overlap of histological structures as compared to the overlaid image on the left side. This is because the B-spline approach identifies the local deformations in the tissue and hence further improves the alignment by transforming the moving image locally. The most noticeable improvement can be seen on the bottom left side and at the top of the glandular tissue area.\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU",
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "e6aeef27e4ad4fde83f03b2122824078": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_07d2030ec38b4f529cafb73adbeb9fa8",
       "IPY_MODEL_cb8b0990d6bd4764bcd4e661c809cf67",
       "IPY_MODEL_3229d6ba40644e0aa230c7a85b65108f"
      ],
      "layout": "IPY_MODEL_ba621aab0c8e438693b431b4461f6047"
     }
    },
    "07d2030ec38b4f529cafb73adbeb9fa8": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a1a3eaad63234711a8f6c36de9a75af4",
      "placeholder": "ÃƒÂ¢Ã¢â€šÂ¬Ã¢â‚¬Â¹",
      "style": "IPY_MODEL_0a96cdeda5234fea964bdb32d567ac59",
      "value": "100%"
     }
    },
    "cb8b0990d6bd4764bcd4e661c809cf67": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a63801b6804041f987a316c6bf091d3c",
      "max": 553433881,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_3c5ec71d42d042e5a0b62ccede28b453",
      "value": 553433881
     }
    },
    "3229d6ba40644e0aa230c7a85b65108f": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_617e534f7adf48e6aa16757abf0cde5f",
      "placeholder": "ÃƒÂ¢Ã¢â€šÂ¬Ã¢â‚¬Â¹",
      "style": "IPY_MODEL_24d564d25059463480ae824699bfddca",
      "value": " 528M/528M [00:03&lt;00:00, 250MB/s]"
     }
    },
    "ba621aab0c8e438693b431b4461f6047": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a1a3eaad63234711a8f6c36de9a75af4": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "0a96cdeda5234fea964bdb32d567ac59": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "a63801b6804041f987a316c6bf091d3c": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3c5ec71d42d042e5a0b62ccede28b453": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "617e534f7adf48e6aa16757abf0cde5f": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "24d564d25059463480ae824699bfddca": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}