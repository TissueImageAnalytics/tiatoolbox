{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Whole Slide Image Registration\n",
    "\n",
    "Click to open in: \\[[GitHub](https://github.com/ruqayya/Whole-Slide-Image-Registration/blob/main/WSI_Registration.ipynb)\\]\\[[Colab](https://colab.research.google.com/github/ruqayya/Whole-Slide-Image-Registration/blob/main/WSI_Registration.ipynb)\\]\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## About this notebook\n",
    "\n",
    "This jupyter notebook can be run on any computer with a standard browser and no prior installation of any programming language is required. It can run remotely over the Internet, free of charge, thanks to Google Colaboratory. To connect with Colab, click on one of the two blue checkboxes above. Check that \"colab\" appears in the address bar. You can right-click on \"Open in Colab\" and select \"Open in new tab\" if the left click does not work for you. Familiarize yourself with the drop-down menus near the top of the window. You can edit the notebook during the session, for example substituting your own image files for the image files used in this demo. Experiment by changing the parameters of functions. It is not possible for an ordinary user to permanently change this version of the notebook on GitHub or Colab, so you cannot inadvertently mess it up. Use the notebook's File Menu if you wish to save your own (changed) notebook.\n",
    "\n",
    "To run the notebook on any platform, except for Colab, set up your Python environment, as explained in the\n",
    "[README](https://github.com/TIA-Lab/tiatoolbox/blob/master/README.md#install-python-package) file.\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### About this demo\n",
    "\n",
    "In this example, we will show how to use TIAToolbox for registration of an image pair using Deep Feature Based Registration (DFBR) method, followed by non-rigid alignment using SimpleITK. The registration tool in the TIAToolbox also comprises a pre-alignment step, pre-requisit to DFBR. In particular, we will introduce the use of our registration tool\n",
    "`wsi_registration` ([Code](https://github.com/TissueImageAnalytics/tiatoolbox/tree/dev-DFBRegistration/tiatoolbox/tools/registration)).\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Setting up the environment\n",
    "\n",
    "### TIAToolbox and dependencies installation\n",
    "\n",
    "You can skip the following cell if 1) you are not using the Colab plaform or 2) you are using Colab and this is not your first run of the notebook in the current runtime session. If you nevertheless run the cell, you may get an error message, but no harm will be done. On Colab the cell installs `tiatoolbox`, and other prerequisite software. Harmless error messages should be ignored. Outside Colab , the notebook expects `tiatoolbox` to already be installed. (See the instructions in [README](https://github.com/TIA-Lab/tiatoolbox/blob/master/README.md#install-python-package).)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "!apt-get -y install libopenjp2-7-dev libopenjp2-tools openslide-tools | tail -n 1\n",
    "!pip install git+https://github.com/TissueImageAnalytics/tiatoolbox.git@dev-DFBRegistration | tail --line 1\n",
    "\n",
    "print(\"Installation is done.\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "> **IMPORTANT**: If you are using Colab and you run the cell above for the first time, please note that you need to restart the runtime before proceeding through (menu) *\"Runtime→Restart runtime\"* . This is needed to load the latest versions of prerequisite packages installed with TIAToolbox. Doing so, you should be able to run all the remaining cells altogether (*\"Runtime→Run after\"* from the next cell) or one by one.\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**\\[essential\\]** Please install the following packages as these will be required in this notebook in addition to TIAToolbox installation.\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "!pip install SimpleITK"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### GPU or CPU runtime\n",
    "\n",
    "Processes in this notebook can be accelerated by using a GPU. Therefore, whether you are running this notebook on your system or Colab, you need to check and specify if you are using GPU or CPU hardware acceleration. In Colab, you need to make sure that the runtime type is set to GPU in the *\"Runtime→Change runtime type→Hardware accelerator\"*. If you are *not* using GPU, consider changing the `ON_GPU` flag to `Flase` value, otherwise, some errors will be raised when running the following cells.\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "ON_GPU = True  # Should be changed to False if no cuda-enabled GPU is available"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Clean-up before a run\n",
    "\n",
    "To ensure proper clean-up (for example in abnormal termination), all files downloaded or created in this run are saved in a single directory global_save_dir, which we set equal to \"./tmp/\". To simplify maintenance, the name of the directory occurs only at this one place, so that it can easily be changed, if desired.\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import warnings\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "global_save_dir = \"./tmp/\"\n",
    "\n",
    "\n",
    "def rmdir(dir_path):\n",
    "    if os.path.isdir(dir_path):\n",
    "        shutil.rmtree(dir_path)\n",
    "        print(\"removing directory \", dir_path)\n",
    "\n",
    "\n",
    "rmdir(global_save_dir)  # remove  directory if it exists from previous runs\n",
    "os.mkdir(global_save_dir)\n",
    "print(\"creating new directory \", global_save_dir)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Importing related libraries\n",
    "\n",
    "We import some standard Python modules, and also the TIAToolbox Python modules for the image registration task, written by the TIA Centre team.\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from tiatoolbox.models.engine.semantic_segmentor import (\n",
    "    IOSegmentorConfig,\n",
    "    SemanticSegmentor,\n",
    ")\n",
    "from tiatoolbox.tools.registration.wsi_registration import match_histograms, DFBRegister\n",
    "from tiatoolbox.utils.misc import imread\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from scipy import ndimage\n",
    "import SimpleITK as sitk\n",
    "from matplotlib import pyplot as plt\n",
    "from skimage import color, exposure, measure, morphology"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Downloading the required files\n",
    "\n",
    "We download, over the internet, image files used for the purpose of this notebook. In particular, we use a sample image pair from the ANHIR challenge dataset, which have been downloaded from  https://anhir.grand-challenge.org/Data/. The challenge dataset is available under the following licence: [CC-BY-NC-SA](https://creativecommons.org/licenses/by-nc-sa/2.0/). The challenge paper \\[1\\] is available [here](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7584382/).\n",
    "\n",
    "\\[1\\] J. Borovec et al., \"ANHIR: Automatic Non-Rigid Histological Image Registration Challenge,\" in IEEE Transactions on Medical Imaging, vol. 39, no. 10, pp. 3042-3052, Oct. 2020, doi: 10.1109/TMI.2020.2986331.\n",
    "\n",
    "Downloading is needed once in each Colab session and it should take less than 1 minute.\n",
    "\n",
    "> In Colab, if you click the file's icon (see below) in the vertical toolbar on the left-hand side then you can see all the files which the code in this notebook can access. The data will appear here when it is downloaded.\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "fixed_img_file_name = \"fixed_image.jpg\"\n",
    "moving_img_file_name = \"moving_image.jpg\"\n",
    "\n",
    "# Downloading fixed image from ANHIR challenge\n",
    "r = requests.get(\n",
    "    \"https://tiatoolbox.dcs.warwick.ac.uk/testdata/registration/ANHIR/COAD_06_HE.jpg\"\n",
    ")\n",
    "with open(fixed_img_file_name, \"wb\") as f:\n",
    "    f.write(r.content)\n",
    "\n",
    "# Downloading moving image from ANHIR challenge\n",
    "r = requests.get(\n",
    "    \"https://tiatoolbox.dcs.warwick.ac.uk/testdata/registration/ANHIR/COAD_06_S2.jpg\"\n",
    ")\n",
    "with open(moving_img_file_name, \"wb\") as f:\n",
    "    f.write(r.content)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Reading images and visualising them\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "fixed_image_rgb = imread(fixed_img_file_name)\n",
    "moving_image_rgb = imread(moving_img_file_name)\n",
    "\n",
    "_, axs = plt.subplots(1, 2, figsize=(15, 10))\n",
    "axs[0].imshow(fixed_image_rgb, cmap=\"gray\")\n",
    "axs[0].set_title(\"Fixed Image\")\n",
    "axs[1].imshow(moving_image_rgb, cmap=\"gray\")\n",
    "axs[1].set_title(\"Moving Image\")\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Image Pre-processing\n",
    "\n",
    "The images are converted to greyscale and to unify the appearance of an image pair, histogram matching is performed as a normalisation step.\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def preprocess_image(image):\n",
    "    image = color.rgb2gray(image)\n",
    "    image = exposure.rescale_intensity(\n",
    "        image, in_range=tuple(np.percentile(image, (0.5, 99.5)))\n",
    "    )\n",
    "    image = image * 255\n",
    "    return image.astype(np.uint8)\n",
    "\n",
    "\n",
    "fixed_image = preprocess_image(fixed_image_rgb)\n",
    "moving_image = preprocess_image(moving_image_rgb)\n",
    "fixed_image, moving_image = match_histograms(fixed_image, moving_image)\n",
    "\n",
    "_, axs = plt.subplots(1, 2, figsize=(15, 10))\n",
    "axs[0].imshow(fixed_image, cmap=\"gray\")\n",
    "axs[0].set_title(\"Fixed Image\")\n",
    "axs[1].imshow(moving_image, cmap=\"gray\")\n",
    "axs[1].set_title(\"Moving Image\")\n",
    "plt.show()\n",
    "\n",
    "temp = np.repeat(np.expand_dims(fixed_image, axis=2), 3, axis=2)\n",
    "cv2.imwrite(os.path.join(global_save_dir, \"fixed.png\"), temp)\n",
    "temp = np.repeat(np.expand_dims(moving_image, axis=2), 3, axis=2)\n",
    "cv2.imwrite(os.path.join(global_save_dir, \"moving.png\"), temp)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Tissue Segmentation\n",
    "\n",
    "In this section, image segmentation is performed to generate tissue masks so that registration could be performed using the active or discriminatory tissue area only. In DFBR method, these are used to exclude the matching points from the non-tissue and fatty regions while in non-rigid alignment method, the background region is excluded from images before inputting them to the SimpleITK module.\n",
    "\n",
    "We should mention that when you are using TIAToolbox pretrained model, you don't need to worry about setting the input/output shape parameters as their optimal values will be loaded by default.\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "save_dir = os.path.join(global_save_dir, \"tissue_mask\")\n",
    "if os.path.exists(save_dir):\n",
    "    shutil.rmtree(save_dir, ignore_errors=False, onerror=None)\n",
    "\n",
    "segmentor = SemanticSegmentor(\n",
    "    pretrained_model=\"unet_tissue_mask_tsef\",\n",
    "    num_loader_workers=4,\n",
    "    batch_size=4,\n",
    ")\n",
    "\n",
    "output = segmentor.predict(\n",
    "    [\n",
    "        os.path.join(global_save_dir, \"fixed.png\"),\n",
    "        os.path.join(global_save_dir, \"moving.png\"),\n",
    "    ],\n",
    "    save_dir=save_dir,\n",
    "    mode=\"tile\",\n",
    "    resolution=1.0,\n",
    "    units=\"baseline\",\n",
    "    patch_input_shape=[1024, 1024],\n",
    "    patch_output_shape=[512, 512],\n",
    "    stride_shape=[512, 512],\n",
    "    on_gpu=ON_GPU,\n",
    "    crash_on_exception=True,\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Post-processing and Visualization of Masks\n",
    "\n",
    "In the output, the prediction method returns a list of the paths to its inputs and to the processed outputs saved on the disk. This can be used for loading the results for processing and visualisation. In this section, a simple processing of the raw prediction is performed to generate tissue mask.\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def post_processing_mask(mask):\n",
    "    mask = ndimage.binary_fill_holes(mask, structure=np.ones((3, 3))).astype(int)\n",
    "\n",
    "    # remove all the objects while keep the biggest object only\n",
    "    label_img = measure.label(mask)\n",
    "    if len(np.unique(label_img)) > 2:\n",
    "        regions = measure.regionprops(label_img)\n",
    "        mask = mask.astype(bool)\n",
    "        all_area = [i.area for i in regions]\n",
    "        second_max = max([i for i in all_area if i != max(all_area)])\n",
    "        mask = morphology.remove_small_objects(mask, min_size=second_max + 1)\n",
    "    return mask.astype(np.uint8)\n",
    "\n",
    "\n",
    "fixed_mask = np.load(output[0][1] + \".raw.0.npy\")\n",
    "moving_mask = np.load(output[1][1] + \".raw.0.npy\")\n",
    "\n",
    "# Simple processing of the raw prediction to generate semantic segmentation task\n",
    "fixed_mask = np.argmax(fixed_mask, axis=-1) == 2\n",
    "moving_mask = np.argmax(moving_mask, axis=-1) == 2\n",
    "\n",
    "fixed_mask = post_processing_mask(fixed_mask)\n",
    "moving_mask = post_processing_mask(moving_mask)\n",
    "\n",
    "_, axs = plt.subplots(1, 2, figsize=(15, 10))\n",
    "axs[0].imshow(fixed_mask, cmap=\"gray\")\n",
    "axs[0].set_title(\"Fixed Mask\")\n",
    "axs[1].imshow(moving_mask, cmap=\"gray\")\n",
    "axs[1].set_title(\"Moving Mask\")\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Registration using DFBR Method\n",
    "\n",
    "In this section, we apply DFBR method for aligning moving image with respect to the fixed image.\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "dfbr_fixed_image = np.repeat(np.expand_dims(fixed_image, axis=2), 3, axis=2)\n",
    "dfbr_moving_image = np.repeat(np.expand_dims(moving_image, axis=2), 3, axis=2)\n",
    "\n",
    "df = DFBRegister()\n",
    "dfbr_transform = df.register(\n",
    "    dfbr_fixed_image, dfbr_moving_image, fixed_mask, moving_mask\n",
    ")\n",
    "print(dfbr_transform)\n",
    "\n",
    "# Visualization\n",
    "original_moving = cv2.warpAffine(\n",
    "    moving_image, np.eye(2, 3), fixed_image.shape[:2][::-1]\n",
    ")\n",
    "dfbr_registered_image = cv2.warpAffine(\n",
    "    moving_image, dfbr_transform[0:-1], fixed_image.shape[:2][::-1]\n",
    ")\n",
    "dfbr_registered_mask = cv2.warpAffine(\n",
    "    moving_mask, dfbr_transform[0:-1], fixed_image.shape[:2][::-1]\n",
    ")\n",
    "\n",
    "before_overlay = np.dstack((original_moving, fixed_image, original_moving))\n",
    "dfbr_overlay = np.dstack((dfbr_registered_image, fixed_image, dfbr_registered_image))\n",
    "\n",
    "_, axs = plt.subplots(1, 2, figsize=(15, 10))\n",
    "axs[0].imshow(before_overlay, cmap=\"gray\")\n",
    "axs[0].set_title(\"Overlay Before Registration\")\n",
    "axs[1].imshow(dfbr_overlay, cmap=\"gray\")\n",
    "axs[1].set_title(\"Overlay After DFBR\")\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### BSpline Transform\n",
    "\n",
    "In this section, we perform registration of fixed and moving images using a [multi-resolution BSpline method](https://simpleitk.readthedocs.io/en/master/link_ImageRegistrationMethodBSpline3_docs.html). The moving image here is a registered image after the application of DFBR transform. For the given image pair, this method is shown to perform better with the inverted intensity values followed by background removal.\n",
    "\n",
    "The BSpline approach comprises a number of parameters that user can experiment with and changing these parameters would result in varying complexity and registration completion time. Feel free to play around with the non-rigid alignment parameters and experimenting with new images.\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Inverting intensity values\n",
    "fixed_image_inv = 255 - fixed_image\n",
    "moving_image_inv = 255 - dfbr_registered_image\n",
    "\n",
    "# Background Removal\n",
    "fixed_mask = np.array(fixed_mask != 0, dtype=np.uint8)\n",
    "dfbr_registered_mask = np.array(dfbr_registered_mask != 0, dtype=np.uint8)\n",
    "fixed_image_inv = cv2.bitwise_and(fixed_image_inv, fixed_image_inv, mask=fixed_mask)\n",
    "moving_image_inv = cv2.bitwise_and(\n",
    "    moving_image_inv, moving_image_inv, mask=dfbr_registered_mask\n",
    ")\n",
    "\n",
    "# Getting SimpleITK Images from numpy arrays\n",
    "fixed_image_inv_sitk = sitk.GetImageFromArray(fixed_image_inv)\n",
    "moving_image_inv_sitk = sitk.GetImageFromArray(moving_image_inv)\n",
    "fixed_image_inv_sitk = sitk.Cast(fixed_image_inv_sitk, sitk.sitkFloat32)\n",
    "moving_image_inv_sitk = sitk.Cast(moving_image_inv_sitk, sitk.sitkFloat32)\n",
    "\n",
    "# Determine the number of BSpline control points\n",
    "mesh_size = [3] * fixed_image_inv_sitk.GetDimension()\n",
    "tx = sitk.BSplineTransformInitializer(\n",
    "    image1=fixed_image_inv_sitk, transformDomainMeshSize=mesh_size\n",
    ")\n",
    "print(\"Initial Number of Parameters: {0}\".format(tx.GetNumberOfParameters()))\n",
    "\n",
    "R = sitk.ImageRegistrationMethod()\n",
    "R.SetInitialTransformAsBSpline(tx, inPlace=True, scaleFactors=[1, 2, 5])\n",
    "R.SetMetricAsMattesMutualInformation(50)\n",
    "R.SetMetricSamplingStrategy(R.RANDOM)\n",
    "R.SetMetricSamplingPercentage(0.2)\n",
    "\n",
    "R.SetShrinkFactorsPerLevel([4, 2, 1])\n",
    "R.SetSmoothingSigmasPerLevel([4, 2, 1])\n",
    "R.SetOptimizerAsGradientDescentLineSearch(\n",
    "    0.5, 100, convergenceMinimumValue=1e-4, convergenceWindowSize=5\n",
    ")\n",
    "R.SetInterpolator(sitk.sitkLinear)\n",
    "outTx = R.Execute(fixed_image_inv_sitk, moving_image_inv_sitk)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Visualizing BSpline Results\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "resampler = sitk.ResampleImageFilter()\n",
    "resampler.SetReferenceImage(fixed_image_inv_sitk)\n",
    "resampler.SetInterpolator(sitk.sitkLinear)\n",
    "resampler.SetDefaultPixelValue(1)\n",
    "resampler.SetTransform(outTx)\n",
    "\n",
    "moving_image_sitk = sitk.GetImageFromArray(dfbr_registered_image)\n",
    "sitk_registered_moving_image_sitk = resampler.Execute(moving_image_sitk)\n",
    "sitk_registered_moving_image = sitk.GetArrayFromImage(sitk_registered_moving_image_sitk)\n",
    "\n",
    "bspline_overlay = np.dstack(\n",
    "    (sitk_registered_moving_image, fixed_image, sitk_registered_moving_image)\n",
    ")\n",
    "\n",
    "_, axs = plt.subplots(1, 2, figsize=(15, 10))\n",
    "axs[0].imshow(dfbr_overlay)\n",
    "axs[0].set_title(\"DFBR Overlay\")\n",
    "axs[1].imshow(bspline_overlay)\n",
    "axs[1].set_title(\"Bspline Overlay\")\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU",
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "e6aeef27e4ad4fde83f03b2122824078": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_07d2030ec38b4f529cafb73adbeb9fa8",
       "IPY_MODEL_cb8b0990d6bd4764bcd4e661c809cf67",
       "IPY_MODEL_3229d6ba40644e0aa230c7a85b65108f"
      ],
      "layout": "IPY_MODEL_ba621aab0c8e438693b431b4461f6047"
     }
    },
    "07d2030ec38b4f529cafb73adbeb9fa8": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a1a3eaad63234711a8f6c36de9a75af4",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_0a96cdeda5234fea964bdb32d567ac59",
      "value": "100%"
     }
    },
    "cb8b0990d6bd4764bcd4e661c809cf67": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a63801b6804041f987a316c6bf091d3c",
      "max": 553433881,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_3c5ec71d42d042e5a0b62ccede28b453",
      "value": 553433881
     }
    },
    "3229d6ba40644e0aa230c7a85b65108f": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_617e534f7adf48e6aa16757abf0cde5f",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_24d564d25059463480ae824699bfddca",
      "value": " 528M/528M [00:03&lt;00:00, 250MB/s]"
     }
    },
    "ba621aab0c8e438693b431b4461f6047": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a1a3eaad63234711a8f6c36de9a75af4": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "0a96cdeda5234fea964bdb32d567ac59": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "a63801b6804041f987a316c6bf091d3c": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3c5ec71d42d042e5a0b62ccede28b453": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "617e534f7adf48e6aa16757abf0cde5f": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "24d564d25059463480ae824699bfddca": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
